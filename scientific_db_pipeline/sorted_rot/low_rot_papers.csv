paper_id,title,abstract,publication_year,citation_count,rot_score,doi,source_url,rot_group
26,Feature Weight Tuning for Recursive Neural Networks,"This paper addresses how a recursive neural network model can automatically
leave out useless information and emphasize important evidence, in other words,
to perform ""weight tuning"" for higher-level representation acquisition. We
propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural
Network (BENN), which automatically control how much one specific unit
contributes to the higher-level representation. The proposed model can be
viewed as incorporating a more powerful compositional function for embedding
acquisition in recursive neural networks. Experimental results demonstrate the
significant improvement over standard neural models.",2014,15,1.25,,http://arxiv.org/abs/1412.3714v2,Low ROT
322,Advanced Neural Network Architectures for Deep Learning - Study 220,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2015,17,1.5454545454545454,synthetic_220,https://synthetic.paper/220,Low ROT
72,"A Hierarchical Latent Variable Encoder-Decoder Model for Generating
  Dialogues","Sequential data often possesses a hierarchical structure with complex
dependencies between subsequences, such as found between the utterances in a
dialogue. In an effort to model this kind of generative process, we propose a
neural network-based generative architecture, with latent stochastic variables
that span a variable number of time steps. We apply the proposed model to the
task of dialogue response generation and compare it with recent neural network
architectures. We evaluate the model performance through automatic evaluation
metrics and by carrying out a human evaluation. The experiments demonstrate
that our model improves upon recently proposed models and that the latent
variables facilitate the generation of long outputs and maintain the context.",2016,16,1.6,,http://arxiv.org/abs/1605.06069v3,Low ROT
202,Quantum Machine Learning Algorithms - Study 100,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2016,16,1.6,synthetic_100,https://synthetic.paper/100,Low ROT
220,Deep Learning for Medical Image Analysis - Study 118,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2015,18,1.6363636363636365,synthetic_118,https://synthetic.paper/118,Low ROT
49,Towards Neural Network-based Reasoning,"We propose Neural Reasoner, a framework for neural network-based reasoning
over natural language sentences. Given a question, Neural Reasoner can infer
over multiple supporting facts and find an answer to the question in specific
forms. Neural Reasoner has 1) a specific interaction-pooling mechanism,
allowing it to examine multiple facts, and 2) a deep architecture, allowing it
to model the complicated logical relations in reasoning tasks. Assuming no
particular structure exists in the question and facts, Neural Reasoner is able
to accommodate different types of reasoning and different forms of language
expressions. Despite the model complexity, Neural Reasoner can still be trained
effectively in an end-to-end manner. Our empirical studies show that Neural
Reasoner can outperform existing neural reasoning systems with remarkable
margins on two difficult artificial tasks (Positional Reasoning and Path
Finding) proposed in [8]. For example, it improves the accuracy on Path
Finding(10K) from 33.4% [6] to over 98%.",2015,19,1.7272727272727273,,http://arxiv.org/abs/1508.05508v1,Low ROT
227,Knowledge Distillation in Deep Learning - Study 125,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,19,1.7272727272727273,synthetic_125,https://synthetic.paper/125,Low ROT
103,Graph Neural Networks for Social Network Analysis - Study 1,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2016,18,1.8,synthetic_1,https://synthetic.paper/1,Low ROT
154,Advanced Neural Network Architectures for Deep Learning - Study 52,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2015,21,1.9090909090909092,synthetic_52,https://synthetic.paper/52,Low ROT
45,"A Neural Network Approach to Context-Sensitive Generation of
  Conversational Responses","We present a novel response generation system that can be trained end to end
on large quantities of unstructured Twitter conversations. A neural network
architecture is used to address sparsity issues that arise when integrating
contextual information into classic statistical models, allowing the system to
take into account previous dialog utterances. Our dynamic-context generative
models show consistent gains over both context-sensitive and
non-context-sensitive Machine Translation and Information Retrieval baselines.",2015,22,2.0,,http://arxiv.org/abs/1506.06714v1,Low ROT
391,Quantum Machine Learning Algorithms - Study 289,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2017,18,2.0,synthetic_289,https://synthetic.paper/289,Low ROT
312,Generative Adversarial Networks for Image Synthesis - Study 210,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,22,2.0,synthetic_210,https://synthetic.paper/210,Low ROT
166,Generative Adversarial Networks for Image Synthesis - Study 64,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2016,20,2.0,synthetic_64,https://synthetic.paper/64,Low ROT
333,Continual Learning in Neural Networks - Study 231,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2015,23,2.090909090909091,synthetic_231,https://synthetic.paper/231,Low ROT
29,Quantifying Uncertainties in Natural Language Processing Tasks,"Reliable uncertainty quantification is a first step towards building
explainable, transparent, and accountable artificial intelligent systems.
Recent progress in Bayesian deep learning has made such quantification
realizable. In this paper, we propose novel methods to study the benefits of
characterizing model and data uncertainties for natural language processing
(NLP) tasks. With empirical experiments on sentiment analysis, named entity
recognition, and language modeling using convolutional and recurrent neural
network models, we show that explicitly modeling uncertainties is not only
necessary to measure output confidence levels, but also useful at enhancing
model performances in various NLP tasks.",2018,17,2.125,,http://arxiv.org/abs/1811.07253v1,Low ROT
46,"The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured
  Multi-Turn Dialogue Systems","This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost
1 million multi-turn dialogues, with a total of over 7 million utterances and
100 million words. This provides a unique resource for research into building
dialogue managers based on neural language models that can make use of large
amounts of unlabeled data. The dataset has both the multi-turn property of
conversations in the Dialog State Tracking Challenge datasets, and the
unstructured nature of interactions from microblog services such as Twitter. We
also describe two neural learning architectures suitable for analyzing this
dataset, and provide benchmark performance on the task of selecting the best
next response.",2015,24,2.1818181818181817,,http://arxiv.org/abs/1506.08909v3,Low ROT
60,"How NOT To Evaluate Your Dialogue System: An Empirical Study of
  Unsupervised Evaluation Metrics for Dialogue Response Generation","We investigate evaluation metrics for dialogue response generation systems
where supervised labels, such as task completion, are not available. Recent
works in response generation have adopted metrics from machine translation to
compare a model's generated response to a single target response. We show that
these metrics correlate very weakly with human judgements in the non-technical
Twitter domain, and not at all in the technical Ubuntu domain. We provide
quantitative and qualitative results highlighting specific weaknesses in
existing metrics, and provide recommendations for future development of better
automatic evaluation metrics for dialogue systems.",2016,22,2.2,,http://arxiv.org/abs/1603.08023v2,Low ROT
303,Machine Learning Applications in Computer Vision - Study 201,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2018,18,2.25,synthetic_201,https://synthetic.paper/201,Low ROT
245,Federated Learning for Privacy-Preserving AI - Study 143,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2018,18,2.25,synthetic_143,https://synthetic.paper/143,Low ROT
191,Optimization Algorithms in Machine Learning - Study 89,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,25,2.272727272727273,synthetic_89,https://synthetic.paper/89,Low ROT
50,"What to talk about and how? Selective Generation using LSTMs with
  Coarse-to-Fine Alignment","We propose an end-to-end, domain-independent neural encoder-aligner-decoder
model for selective generation, i.e., the joint task of content selection and
surface realization. Our model first encodes a full set of over-determined
database event records via an LSTM-based recurrent neural network, then
utilizes a novel coarse-to-fine aligner to identify the small subset of salient
records to talk about, and finally employs a decoder to generate free-form
descriptions of the aligned, selected records. Our model achieves the best
selection and generation results reported to-date (with 59% relative
improvement in generation) on the benchmark WeatherGov dataset, despite using
no specialized features or linguistic resources. Using an improved k-nearest
neighbor beam filter helps further. We also perform a series of ablations and
visualizations to elucidate the contributions of our key model components.
Lastly, we evaluate the generalizability of our model on the RoboCup dataset,
and get results that are competitive with or better than the state-of-the-art,
despite being severely data-starved.",2015,25,2.272727272727273,,http://arxiv.org/abs/1509.00838v2,Low ROT
147,Adversarial Training for Robust Neural Networks - Study 45,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2015,25,2.272727272727273,synthetic_45,https://synthetic.paper/45,Low ROT
359,Adversarial Training for Robust Neural Networks - Study 257,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,26,2.3636363636363638,synthetic_257,https://synthetic.paper/257,Low ROT
250,Attention Mechanisms in Neural Networks - Study 148,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2015,26,2.3636363636363638,synthetic_148,https://synthetic.paper/148,Low ROT
123,Adversarial Training for Robust Neural Networks - Study 21,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2018,19,2.375,synthetic_21,https://synthetic.paper/21,Low ROT
119,Natural Language Processing with Transformer Models - Study 17,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2016,24,2.4,synthetic_17,https://synthetic.paper/17,Low ROT
213,Neural Architecture Search for Automated ML - Study 111,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2016,24,2.4,synthetic_111,https://synthetic.paper/111,Low ROT
336,Federated Learning for Privacy-Preserving AI - Study 234,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2015,27,2.4545454545454546,synthetic_234,https://synthetic.paper/234,Low ROT
121,Graph Neural Networks for Social Network Analysis - Study 19,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2015,27,2.4545454545454546,synthetic_19,https://synthetic.paper/19,Low ROT
63,Piecewise Latent Variables for Neural Variational Text Processing,"Advances in neural variational inference have facilitated the learning of
powerful directed graphical models with continuous latent variables, such as
variational autoencoders. The hope is that such models will learn to represent
rich, multi-modal latent factors in real-world data, such as natural language
text. However, current models often assume simplistic priors on the latent
variables - such as the uni-modal Gaussian distribution - which are incapable
of representing complex latent factors efficiently. To overcome this
restriction, we propose the simple, but highly flexible, piecewise constant
distribution. This distribution has the capacity to represent an exponential
number of modes of a latent target distribution, while remaining mathematically
tractable. Our results demonstrate that incorporating this new latent
distribution into different models yields substantial improvements in natural
language processing tasks such as document modeling and natural language
generation for dialogue.",2016,25,2.5,,http://arxiv.org/abs/1612.00377v4,Low ROT
195,Natural Language Processing with Transformer Models - Study 93,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2016,25,2.5,synthetic_93,https://synthetic.paper/93,Low ROT
57,Compositional Obverter Communication Learning From Raw Visual Input,"One of the distinguishing aspects of human language is its compositionality,
which allows us to describe complex environments with limited vocabulary.
Previously, it has been shown that neural network agents can learn to
communicate in a highly structured, possibly compositional language based on
disentangled input (e.g. hand- engineered features). Humans, however, do not
learn to communicate based on well-summarized features. In this work, we train
neural agents to simultaneously develop visual perception from raw image
pixels, and learn to communicate with a sequence of discrete symbols. The
agents play an image description game where the image contains factors such as
colors and shapes. We train the agents using the obverter technique where an
agent introspects to generate messages that maximize its own understanding.
Through qualitative analysis, visualization and a zero-shot test, we show that
the agents can develop, out of raw image pixels, a language with compositional
properties, given a proper pressure from the environment.",2018,20,2.5,,http://arxiv.org/abs/1804.02341v1,Low ROT
51,Incorporating Copying Mechanism in Sequence-to-Sequence Learning,"We address an important problem in sequence-to-sequence (Seq2Seq) learning
referred to as copying, in which certain segments in the input sequence are
selectively replicated in the output sequence. A similar phenomenon is
observable in human language communication. For example, humans tend to repeat
entity names or even long phrases in conversation. The challenge with regard to
copying in Seq2Seq is that new machinery is needed to decide when to perform
the operation. In this paper, we incorporate copying into neural network-based
Seq2Seq learning and propose a new model called CopyNet with encoder-decoder
structure. CopyNet can nicely integrate the regular way of word generation in
the decoder with the new copying mechanism which can choose sub-sequences in
the input sequence and put them at proper places in the output sequence. Our
empirical study on both synthetic data sets and real world data sets
demonstrates the efficacy of CopyNet. For example, CopyNet can outperform
regular RNN-based model with remarkable margins on text summarization tasks.",2016,26,2.6,,http://arxiv.org/abs/1603.06393v3,Low ROT
47,"Building End-To-End Dialogue Systems Using Generative Hierarchical
  Neural Network Models","We investigate the task of building open domain, conversational dialogue
systems based on large dialogue corpora using generative models. Generative
models produce system responses that are autonomously generated word-by-word,
opening up the possibility for realistic, flexible interactions. In support of
this goal, we extend the recently proposed hierarchical recurrent
encoder-decoder neural network to the dialogue domain, and demonstrate that
this model is competitive with state-of-the-art neural language models and
back-off n-gram models. We investigate the limitations of this and similar
approaches, and show how its performance can be improved by bootstrapping the
learning from a larger question-answer pair corpus and from pretrained word
embeddings.",2015,30,2.727272727272727,,http://arxiv.org/abs/1507.04808v3,Low ROT
27,"A Recurrent Neural Model with Attention for the Recognition of Chinese
  Implicit Discourse Relations","We introduce an attention-based Bi-LSTM for Chinese implicit discourse
relations and demonstrate that modeling argument pairs as a joint sequence can
outperform word order-agnostic approaches. Our model benefits from a partial
sampling scheme and is conceptually simple, yet achieves state-of-the-art
performance on the Chinese Discourse Treebank. We also visualize its attention
activity to illustrate the model's ability to selectively focus on the relevant
parts of an input sequence.",2017,25,2.7777777777777777,,http://arxiv.org/abs/1704.08092v1,Low ROT
292,Neural Network Compression Techniques - Study 190,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2017,25,2.7777777777777777,synthetic_190,https://synthetic.paper/190,Low ROT
381,Neural Network Compression Techniques - Study 279,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,31,2.8181818181818183,synthetic_279,https://synthetic.paper/279,Low ROT
196,Generative Adversarial Networks for Image Synthesis - Study 94,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2015,31,2.8181818181818183,synthetic_94,https://synthetic.paper/94,Low ROT
238,Graph Neural Networks for Social Network Analysis - Study 136,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2015,31,2.8181818181818183,synthetic_136,https://synthetic.paper/136,Low ROT
137,Neural Network Compression Techniques - Study 35,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2016,29,2.9,synthetic_35,https://synthetic.paper/35,Low ROT
193,Optimization Algorithms in Machine Learning - Study 91,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2016,30,3.0,synthetic_91,https://synthetic.paper/91,Low ROT
399,Neural Architecture Search for Automated ML - Study 297,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2019,21,3.0,synthetic_297,https://synthetic.paper/297,Low ROT
257,Quantum Machine Learning Algorithms - Study 155,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2016,30,3.0,synthetic_155,https://synthetic.paper/155,Low ROT
55,Improving speech recognition by revising gated recurrent units,"Speech recognition is largely taking advantage of deep learning, showing that
substantial benefits can be obtained by modern Recurrent Neural Networks
(RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which
typically reach state-of-the-art performance in many tasks thanks to their
ability to learn long-term dependencies and robustness to vanishing gradients.
Nevertheless, LSTMs have a rather complex design with three multiplicative
gates, that might impair their efficient implementation. An attempt to simplify
LSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just
two multiplicative gates.
  This paper builds on these efforts by further revising GRUs and proposing a
simplified architecture potentially more suitable for speech recognition. The
contribution of this work is two-fold. First, we suggest to remove the reset
gate in the GRU design, resulting in a more efficient single-gate architecture.
Second, we propose to replace tanh with ReLU activations in the state update
equations. Results show that, in our implementation, the revised architecture
reduces the per-epoch training time with more than 30% and consistently
improves recognition performance across different tasks, input features, and
noisy conditions when compared to a standard GRU.",2017,27,3.0,,http://arxiv.org/abs/1710.00641v1,Low ROT
384,Deep Learning for Medical Image Analysis - Study 282,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,33,3.0,synthetic_282,https://synthetic.paper/282,Low ROT
197,Graph Neural Networks for Social Network Analysis - Study 95,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2016,31,3.1,synthetic_95,https://synthetic.paper/95,Low ROT
140,Graph Neural Networks for Social Network Analysis - Study 38,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2018,25,3.125,synthetic_38,https://synthetic.paper/38,Low ROT
262,Attention Mechanisms in Neural Networks - Study 160,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2018,25,3.125,synthetic_160,https://synthetic.paper/160,Low ROT
396,Explainable AI for Trustworthy Machine Learning - Study 294,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2019,22,3.142857142857143,synthetic_294,https://synthetic.paper/294,Low ROT
371,Generative Adversarial Networks for Image Synthesis - Study 269,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2020,19,3.1666666666666665,synthetic_269,https://synthetic.paper/269,Low ROT
54,Bridging LSTM Architecture and the Neural Dynamics during Reading,"Recently, the long short-term memory neural network (LSTM) has attracted wide
interest due to its success in many tasks. LSTM architecture consists of a
memory cell and three gates, which looks similar to the neuronal networks in
the brain. However, there still lacks the evidence of the cognitive
plausibility of LSTM architecture as well as its working mechanism. In this
paper, we study the cognitive plausibility of LSTM by aligning its internal
architecture with the brain activity observed via fMRI when the subjects read a
story. Experiment results show that the artificial memory vector in LSTM can
accurately predict the observed sequential brain activities, indicating the
correlation between LSTM architecture and the cognitive process of story
reading.",2016,32,3.2,,http://arxiv.org/abs/1604.06635v1,Low ROT
98,Neural Associative Memory for Dual-Sequence Modeling,"Many important NLP problems can be posed as dual-sequence or
sequence-to-sequence modeling tasks. Recent advances in building end-to-end
neural architectures have been highly successful in solving such tasks. In this
work we propose a new architecture for dual-sequence modeling that is based on
associative memory. We derive AM-RNNs, a recurrent associative memory (AM)
which augments generic recurrent neural networks (RNN). This architecture is
extended to the Dual AM-RNN which operates on two AMs at once. Our models
achieve very competitive results on textual entailment. A qualitative analysis
demonstrates that long range dependencies between source and target-sequence
can be bridged effectively using Dual AM-RNNs. However, an initial experiment
on auto-encoding reveals that these benefits are not exploited by the system
when learning to solve sequence-to-sequence tasks which indicates that
additional supervision or regularization is needed.",2016,32,3.2,,http://arxiv.org/abs/1606.03864v2,Low ROT
376,Graph Neural Networks for Social Network Analysis - Study 274,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2018,26,3.25,synthetic_274,https://synthetic.paper/274,Low ROT
249,Graph Neural Networks for Social Network Analysis - Study 147,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2016,33,3.3,synthetic_147,https://synthetic.paper/147,Low ROT
339,Deep Learning for Medical Image Analysis - Study 237,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2016,33,3.3,synthetic_237,https://synthetic.paper/237,Low ROT
48,End-to-End Attention-based Large Vocabulary Speech Recognition,"Many of the current state-of-the-art Large Vocabulary Continuous Speech
Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov
Models (HMMs). Most of these systems contain separate components that deal with
the acoustic modelling, language modelling and sequence decoding. We
investigate a more direct approach in which the HMM is replaced with a
Recurrent Neural Network (RNN) that performs sequence prediction directly at
the character level. Alignment between the input features and the desired
character sequence is learned automatically by an attention mechanism built
into the RNN. For each predicted character, the attention mechanism scans the
input sequence and chooses relevant frames. We propose two methods to speed up
this operation: limiting the scan to a subset of most promising frames and
pooling over time the information contained in neighboring frames, thereby
reducing source sequence length. Integrating an n-gram language model into the
decoding process yields recognition accuracies similar to other HMM-free
RNN-based approaches.",2015,37,3.3636363636363638,,http://arxiv.org/abs/1508.04395v2,Low ROT
44,"Recurrent Neural Networks with External Memory for Language
  Understanding","Recurrent Neural Networks (RNNs) have become increasingly popular for the
task of language understanding. In this task, a semantic tagger is deployed to
associate a semantic label to each word in an input sequence. The success of
RNN may be attributed to its ability to memorize long-term dependence that
relates the current-time semantic label prediction to the observations many
time instances away. However, the memory capacity of simple RNNs is limited
because of the gradient vanishing and exploding problem. We propose to use an
external memory to improve memorization capability of RNNs. We conducted
experiments on the ATIS dataset, and observed that the proposed model was able
to achieve the state-of-the-art results. We compare our proposed model with
alternative models and report analysis results that may provide insights for
future research.",2015,37,3.3636363636363638,,http://arxiv.org/abs/1506.00195v1,Low ROT
165,Adversarial Training for Robust Neural Networks - Study 63,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2016,34,3.4,synthetic_63,https://synthetic.paper/63,Low ROT
143,Self-Supervised Learning in Computer Vision - Study 41,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2017,31,3.4444444444444446,synthetic_41,https://synthetic.paper/41,Low ROT
294,Graph Neural Networks for Social Network Analysis - Study 192,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,38,3.4545454545454546,synthetic_192,https://synthetic.paper/192,Low ROT
247,Optimization Algorithms in Machine Learning - Study 145,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2016,35,3.5,synthetic_145,https://synthetic.paper/145,Low ROT
189,Deep Learning for Medical Image Analysis - Study 87,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2015,39,3.5454545454545454,synthetic_87,https://synthetic.paper/87,Low ROT
73,A Structured Self-attentive Sentence Embedding,"This paper proposes a new model for extracting an interpretable sentence
embedding by introducing self-attention. Instead of using a vector, we use a
2-D matrix to represent the embedding, with each row of the matrix attending on
a different part of the sentence. We also propose a self-attention mechanism
and a special regularization term for the model. As a side effect, the
embedding comes with an easy way of visualizing what specific parts of the
sentence are encoded into the embedding. We evaluate our model on 3 different
tasks: author profiling, sentiment classification, and textual entailment.
Results show that our model yields a significant performance gain compared to
other sentence embedding methods in all of the 3 tasks.",2017,32,3.5555555555555554,,http://arxiv.org/abs/1703.03130v1,Low ROT
353,Graph Neural Networks for Social Network Analysis - Study 251,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2019,25,3.5714285714285716,synthetic_251,https://synthetic.paper/251,Low ROT
331,Quantum Machine Learning Algorithms - Study 229,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2019,25,3.5714285714285716,synthetic_229,https://synthetic.paper/229,Low ROT
347,Reinforcement Learning for Autonomous Systems - Study 245,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2016,36,3.6,synthetic_245,https://synthetic.paper/245,Low ROT
261,Self-Supervised Learning in Computer Vision - Study 159,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2016,36,3.6,synthetic_159,https://synthetic.paper/159,Low ROT
215,Federated Learning for Privacy-Preserving AI - Study 113,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2016,36,3.6,synthetic_113,https://synthetic.paper/113,Low ROT
58,DOLORES: Deep Contextualized Knowledge Graph Embeddings,"We introduce a new method DOLORES for learning knowledge graph embeddings
that effectively captures contextual cues and dependencies among entities and
relations. First, we note that short paths on knowledge graphs comprising of
chains of entities and relations can encode valuable information regarding
their contextual usage. We operationalize this notion by representing knowledge
graphs not as a collection of triples but as a collection of entity-relation
chains, and learn embeddings for entities and relations using deep neural
models that capture such contextual usage. In particular, our model is based on
Bi-Directional LSTMs and learn deep representations of entities and relations
from constructed entity-relation chains. We show that these representations can
very easily be incorporated into existing models to significantly advance the
state of the art on several knowledge graph prediction tasks like link
prediction, triple classification, and missing relation type prediction (in
some cases by at least 9.5%).",2018,29,3.625,,http://arxiv.org/abs/1811.00147v1,Low ROT
320,Natural Language Processing with Transformer Models - Study 218,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2020,22,3.6666666666666665,synthetic_218,https://synthetic.paper/218,Low ROT
236,Graph Neural Networks for Social Network Analysis - Study 134,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2017,33,3.6666666666666665,synthetic_134,https://synthetic.paper/134,Low ROT
4,Tutorial on Answering Questions about Images with Deep Learning,"Together with the development of more accurate methods in Computer Vision and
Natural Language Understanding, holistic architectures that answer on questions
about the content of real-world images have emerged. In this tutorial, we build
a neural-based approach to answer questions about images. We base our tutorial
on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the
models that we present here can achieve a competitive performance on both
datasets, in fact, they are among the best methods that use a combination of
LSTM with a global, full frame CNN representation of an image. We hope that
after reading this tutorial, the reader will be able to use Deep Learning
frameworks, such as Keras and introduced Kraino, to build various architectures
that will lead to a further performance improvement on this challenging task.",2016,37,3.7,,http://arxiv.org/abs/1610.01076v1,Low ROT
114,Neural Network Compression Techniques - Study 12,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2016,37,3.7,synthetic_12,https://synthetic.paper/12,Low ROT
224,Quantum Machine Learning Algorithms - Study 122,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2019,26,3.7142857142857144,synthetic_122,https://synthetic.paper/122,Low ROT
162,Machine Learning Applications in Computer Vision - Study 60,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,41,3.727272727272727,synthetic_60,https://synthetic.paper/60,Low ROT
325,Meta-Learning Approaches for Few-Shot Learning - Study 223,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2015,41,3.727272727272727,synthetic_223,https://synthetic.paper/223,Low ROT
25,Dirichlet Variational Autoencoder for Text Modeling,"We introduce an improved variational autoencoder (VAE) for text modeling with
topic information explicitly modeled as a Dirichlet latent variable. By
providing the proposed model topic awareness, it is more superior at
reconstructing input texts. Furthermore, due to the inherent interactions
between the newly introduced Dirichlet variable and the conventional
multivariate Gaussian variable, the model is less prone to KL divergence
vanishing. We derive the variational lower bound for the new model and conduct
experiments on four different data sets. The results show that the proposed
model is superior at text reconstruction across the latent space and
classifications on learned representations have higher test accuracies.",2018,30,3.75,,http://arxiv.org/abs/1811.00135v1,Low ROT
314,Quantum Machine Learning Algorithms - Study 212,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2017,34,3.7777777777777777,synthetic_212,https://synthetic.paper/212,Low ROT
136,Neural Network Compression Techniques - Study 34,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2017,34,3.7777777777777777,synthetic_34,https://synthetic.paper/34,Low ROT
18,DopeLearning: A Computational Approach to Rap Lyrics Generation,"Writing rap lyrics requires both creativity to construct a meaningful,
interesting story and lyrical skills to produce complex rhyme patterns, which
form the cornerstone of good flow. We present a rap lyrics generation method
that captures both of these aspects. First, we develop a prediction model to
identify the next line of existing lyrics from a set of candidate next lines.
This model is based on two machine-learning techniques: the RankSVM algorithm
and a deep neural network model with a novel structure. Results show that the
prediction model can identify the true next line among 299 randomly selected
lines with an accuracy of 17%, i.e., over 50 times more likely than by random.
Second, we employ the prediction model to combine lines from existing songs,
producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics
shows that in terms of quantitative rhyme density, the method outperforms the
best human rappers by 21%. The rap lyrics generator has been deployed as an
online tool called DeepBeat, and the performance of the tool has been assessed
by analyzing its usage logs. This analysis shows that machine-learned rankings
correlate with user preferences.",2015,42,3.8181818181818183,,http://arxiv.org/abs/1505.04771v2,Low ROT
152,Natural Language Processing with Transformer Models - Study 50,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2015,42,3.8181818181818183,synthetic_50,https://synthetic.paper/50,Low ROT
70,Continual Lifelong Learning in Natural Language Processing: A Survey,"Continual learning (CL) aims to enable information systems to learn from a
continuous data stream across time. However, it is difficult for existing deep
learning architectures to learn a new task without largely forgetting
previously acquired knowledge. Furthermore, CL is particularly challenging for
language learning, as natural language is ambiguous: it is discrete,
compositional, and its meaning is context-dependent. In this work, we look at
the problem of CL through the lens of various NLP tasks. Our survey discusses
major challenges in CL and current methods applied in neural network models. We
also provide a critical review of the existing CL evaluation methods and
datasets in NLP. Finally, we present our outlook on future research directions.",2020,23,3.8333333333333335,,http://arxiv.org/abs/2012.09823v1,Low ROT
263,Self-Supervised Learning in Computer Vision - Study 161,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2020,23,3.8333333333333335,synthetic_161,https://synthetic.paper/161,Low ROT
217,Neural Network Compression Techniques - Study 115,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2019,27,3.857142857142857,synthetic_115,https://synthetic.paper/115,Low ROT
100,"Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for
  Task-Completion Dialogue Policy Learning","Training task-completion dialogue agents with reinforcement learning usually
requires a large number of real user experiences. The Dyna-Q algorithm extends
Q-learning by integrating a world model, and thus can effectively boost
training efficiency using simulated experiences generated by the world model.
The effectiveness of Dyna-Q, however, depends on the quality of the world model
- or implicitly, the pre-specified ratio of real vs. simulated experiences used
for Q-learning. To this end, we extend the recently proposed Deep Dyna-Q (DDQ)
framework by integrating a switcher that automatically determines whether to
use a real or simulated experience for Q-learning. Furthermore, we explore the
use of active learning for improving sample efficiency, by encouraging the
world model to generate simulated experiences in the state-action space where
the agent has not (fully) explored. Our results show that by combining switcher
and active learning, the new framework named as Switch-based Active Deep Dyna-Q
(Switch-DDQ), leads to significant improvement over DDQ and Q-learning
baselines in both simulation and human evaluations.",2018,31,3.875,,http://arxiv.org/abs/1811.07550v1,Low ROT
167,Attention Mechanisms in Neural Networks - Study 65,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2016,39,3.9,synthetic_65,https://synthetic.paper/65,Low ROT
131,Quantum Machine Learning Algorithms - Study 29,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2015,44,4.0,synthetic_29,https://synthetic.paper/29,Low ROT
304,Knowledge Distillation in Deep Learning - Study 202,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2017,36,4.0,synthetic_202,https://synthetic.paper/202,Low ROT
351,Meta-Learning Approaches for Few-Shot Learning - Study 249,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2019,28,4.0,synthetic_249,https://synthetic.paper/249,Low ROT
74,"Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational
  Compositional Operators for Analogy Detection","Representing the semantic relations that exist between two given words (or
entities) is an important first step in a wide-range of NLP applications such
as analogical reasoning, knowledge base completion and relational information
retrieval. A simple, yet surprisingly accurate method for representing a
relation between two words is to compute the vector offset (\PairDiff) between
their corresponding word embeddings. Despite the empirical success, it remains
unclear as to whether \PairDiff is the best operator for obtaining a relational
representation from word embeddings. We conduct a theoretical analysis of
generalised bilinear operators that can be used to measure the $\ell_{2}$
relational distance between two word-pairs. We show that, if the word
embeddings are standardised and uncorrelated, such an operator will be
independent of bilinear terms, and can be simplified to a linear form, where
\PairDiff is a special case. For numerous word embedding types, we empirically
verify the uncorrelation assumption, demonstrating the general applicability of
our theoretical result. Moreover, we experimentally discover \PairDiff from the
bilinear relation composition operator on several benchmark analogy datasets.",2017,36,4.0,,http://arxiv.org/abs/1709.06673v2,Low ROT
204,Reinforcement Learning for Autonomous Systems - Study 102,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2018,33,4.125,synthetic_102,https://synthetic.paper/102,Low ROT
234,Graph Neural Networks for Social Network Analysis - Study 132,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2019,29,4.142857142857143,synthetic_132,https://synthetic.paper/132,Low ROT
30,"NUIG-Shubhanker@Dravidian-CodeMix-FIRE2020: Sentiment Analysis of
  Code-Mixed Dravidian text using XLNet","Social media has penetrated into multilingual societies, however most of them
use English to be a preferred language for communication. So it looks natural
for them to mix their cultural language with English during conversations
resulting in abundance of multilingual data, call this code-mixed data,
available in todays' world.Downstream NLP tasks using such data is challenging
due to the semantic nature of it being spread across multiple languages.One
such Natural Language Processing task is sentiment analysis, for this we use an
auto-regressive XLNet model to perform sentiment analysis on code-mixed
Tamil-English and Malayalam-English datasets.",2020,25,4.166666666666667,,http://arxiv.org/abs/2010.07773v1,Low ROT
182,Quantum Machine Learning Algorithms - Study 80,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2015,46,4.181818181818182,synthetic_80,https://synthetic.paper/80,Low ROT
67,Czech News Dataset for Semantic Textual Similarity,"This paper describes a novel dataset consisting of sentences with semantic
similarity annotations. The data originate from the journalistic domain in the
Czech language. We describe the process of collecting and annotating the data
in detail. The dataset contains 138,556 human annotations divided into train
and test sets. In total, 485 journalism students participated in the creation
process. To increase the reliability of the test set, we compute the annotation
as an average of 9 individual annotations. We evaluate the quality of the
dataset by measuring inter and intra annotation annotators' agreements. Beside
agreement numbers, we provide detailed statistics of the collected dataset. We
conclude our paper with a baseline experiment of building a system for
predicting the semantic similarity of sentences. Due to the massive number of
training annotations (116 956), the model can perform significantly better than
an average annotator (0,92 versus 0,86 of Person's correlation coefficients).",2021,21,4.2,,http://arxiv.org/abs/2108.08708v3,Low ROT
271,Attention Mechanisms in Neural Networks - Study 169,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2016,42,4.2,synthetic_169,https://synthetic.paper/169,Low ROT
21,"Adversarial Attacks on Knowledge Graph Embeddings via Instance
  Attribution Methods","Despite the widespread use of Knowledge Graph Embeddings (KGE), little is
known about the security vulnerabilities that might disrupt their intended
behaviour. We study data poisoning attacks against KGE models for link
prediction. These attacks craft adversarial additions or deletions at training
time to cause model failure at test time. To select adversarial deletions, we
propose to use the model-agnostic instance attribution methods from
Interpretable Machine Learning, which identify the training instances that are
most influential to a neural model's predictions on test instances. We use
these influential triples as adversarial deletions. We further propose a
heuristic method to replace one of the two entities in each influential triple
to generate adversarial additions. Our experiments show that the proposed
strategies outperform the state-of-art data poisoning attacks on KGE models and
improve the MRR degradation due to the attacks by up to 62% over the baselines.",2021,21,4.2,,http://arxiv.org/abs/2111.03120v1,Low ROT
368,Reinforcement Learning for Autonomous Systems - Study 266,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2016,42,4.2,synthetic_266,https://synthetic.paper/266,Low ROT
310,Quantum Machine Learning Algorithms - Study 208,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2018,34,4.25,synthetic_208,https://synthetic.paper/208,Low ROT
394,Knowledge Distillation in Deep Learning - Study 292,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,47,4.2727272727272725,synthetic_292,https://synthetic.paper/292,Low ROT
101,Quasi-Recurrent Neural Networks,"Recurrent neural networks are a powerful tool for modeling sequential data,
but the dependence of each timestep's computation on the previous timestep's
output limits parallelism and makes RNNs unwieldy for very long sequences. We
introduce quasi-recurrent neural networks (QRNNs), an approach to neural
sequence modeling that alternates convolutional layers, which apply in parallel
across timesteps, and a minimalist recurrent pooling function that applies in
parallel across channels. Despite lacking trainable recurrent layers, stacked
QRNNs have better predictive accuracy than stacked LSTMs of the same hidden
size. Due to their increased parallelism, they are up to 16 times faster at
train and test time. Experiments on language modeling, sentiment
classification, and character-level neural machine translation demonstrate
these advantages and underline the viability of QRNNs as a basic building block
for a variety of sequence tasks.",2016,43,4.3,,http://arxiv.org/abs/1611.01576v2,Low ROT
198,Quantum Machine Learning Algorithms - Study 96,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2018,35,4.375,synthetic_96,https://synthetic.paper/96,Low ROT
382,Self-Supervised Learning in Computer Vision - Study 280,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2021,22,4.4,synthetic_280,https://synthetic.paper/280,Low ROT
142,Federated Learning for Privacy-Preserving AI - Study 40,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2016,44,4.4,synthetic_40,https://synthetic.paper/40,Low ROT
75,A Neural Comprehensive Ranker (NCR) for Open-Domain Question Answering,"This paper proposes a novel neural machine reading model for open-domain
question answering at scale. Existing machine comprehension models typically
assume that a short piece of relevant text containing answers is already
identified and given to the models, from which the models are designed to
extract answers. This assumption, however, is not realistic for building a
large-scale open-domain question answering system which requires both deep text
understanding and identifying relevant text from corpus simultaneously.
  In this paper, we introduce Neural Comprehensive Ranker (NCR) that integrates
both passage ranking and answer extraction in one single framework. A Q&A
system based on this framework allows users to issue an open-domain question
without needing to provide a piece of text that must contain the answer.
Experiments show that the unified NCR model is able to outperform the
states-of-the-art in both retrieval of relevant text and answer extraction.",2017,40,4.444444444444445,,http://arxiv.org/abs/1709.10204v2,Low ROT
212,Self-Supervised Learning in Computer Vision - Study 110,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2015,49,4.454545454545454,synthetic_110,https://synthetic.paper/110,Low ROT
266,Deep Learning for Medical Image Analysis - Study 164,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2017,41,4.555555555555555,synthetic_164,https://synthetic.paper/164,Low ROT
255,Continual Learning in Neural Networks - Study 153,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2019,32,4.571428571428571,synthetic_153,https://synthetic.paper/153,Low ROT
6,Document Image Coding and Clustering for Script Discrimination,"The paper introduces a new method for discrimination of documents given in
different scripts. The document is mapped into a uniformly coded text of
numerical values. It is derived from the position of the letters in the text
line, based on their typographical characteristics. Each code is considered as
a gray level. Accordingly, the coded text determines a 1-D image, on which
texture analysis by run-length statistics and local binary pattern is
performed. It defines feature vectors representing the script content of the
document. A modified clustering approach employed on document feature vector
groups documents written in the same script. Experimentation performed on two
custom oriented databases of historical documents in old Cyrillic, angular and
round Glagolitic as well as Antiqua and Fraktur scripts demonstrates the
superiority of the proposed method with respect to well-known methods in the
state-of-the-art.",2016,46,4.6,,http://arxiv.org/abs/1609.06492v1,Low ROT
390,Optimization Algorithms in Machine Learning - Study 288,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2015,51,4.636363636363637,synthetic_288,https://synthetic.paper/288,Low ROT
188,Optimization Algorithms in Machine Learning - Study 86,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2020,28,4.666666666666667,synthetic_86,https://synthetic.paper/86,Low ROT
59,"Deep Reinforcement Learning with Distributional Semantic Rewards for
  Abstractive Summarization","Deep reinforcement learning (RL) has been a commonly-used strategy for the
abstractive summarization task to address both the exposure bias and
non-differentiable task issues. However, the conventional reward Rouge-L simply
looks for exact n-grams matches between candidates and annotated references,
which inevitably makes the generated sentences repetitive and incoherent. In
this paper, instead of Rouge-L, we explore the practicability of utilizing the
distributional semantics to measure the matching degrees. With distributional
semantics, sentence-level evaluation can be obtained, and semantically-correct
phrases can also be generated without being limited to the surface form of the
reference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets
show that our proposed distributional semantics reward (DSR) has distinct
superiority in capturing the lexical and compositional diversity of natural
language.",2019,33,4.714285714285714,,http://arxiv.org/abs/1909.00141v2,Low ROT
392,Quantum Machine Learning Algorithms - Study 290,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2019,33,4.714285714285714,synthetic_290,https://synthetic.paper/290,Low ROT
106,Neural Architecture Search for Automated ML - Study 4,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2018,38,4.75,synthetic_4,https://synthetic.paper/4,Low ROT
241,Adversarial Training for Robust Neural Networks - Study 139,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2017,43,4.777777777777778,synthetic_139,https://synthetic.paper/139,Low ROT
222,Knowledge Distillation in Deep Learning - Study 120,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2017,43,4.777777777777778,synthetic_120,https://synthetic.paper/120,Low ROT
274,Natural Language Processing with Transformer Models - Study 172,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2021,24,4.8,synthetic_172,https://synthetic.paper/172,Low ROT
207,Self-Supervised Learning in Computer Vision - Study 105,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2015,53,4.818181818181818,synthetic_105,https://synthetic.paper/105,Low ROT
326,Optimization Algorithms in Machine Learning - Study 224,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2019,34,4.857142857142857,synthetic_224,https://synthetic.paper/224,Low ROT
192,Graph Neural Networks for Social Network Analysis - Study 90,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2019,34,4.857142857142857,synthetic_90,https://synthetic.paper/90,Low ROT
240,Generative Adversarial Networks for Image Synthesis - Study 138,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2016,49,4.9,synthetic_138,https://synthetic.paper/138,Low ROT
20,"Generating Factoid Questions With Recurrent Neural Networks: The 30M
  Factoid Question-Answer Corpus","Over the past decade, large-scale supervised learning corpora have enabled
machine learning researchers to make substantial advances. However, to this
date, there are no large-scale question-answer corpora available. In this paper
we present the 30M Factoid Question-Answer Corpus, an enormous question answer
pair corpus produced by applying a novel neural network architecture on the
knowledge base Freebase to transduce facts into natural language questions. The
produced question answer pairs are evaluated both by human evaluators and using
automatic evaluation metrics, including well-established machine translation
and sentence similarity metrics. Across all evaluation criteria the
question-generation model outperforms the competing template-based baseline.
Furthermore, when presented to human evaluators, the generated questions appear
comparable in quality to real human-generated questions.",2016,49,4.9,,http://arxiv.org/abs/1603.06807v2,Low ROT
357,Knowledge Distillation in Deep Learning - Study 255,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2015,54,4.909090909090909,synthetic_255,https://synthetic.paper/255,Low ROT
164,Federated Learning for Privacy-Preserving AI - Study 62,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2015,54,4.909090909090909,synthetic_62,https://synthetic.paper/62,Low ROT
300,Deep Learning for Medical Image Analysis - Study 198,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2016,50,5.0,synthetic_198,https://synthetic.paper/198,Low ROT
367,Attention Mechanisms in Neural Networks - Study 265,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2017,45,5.0,synthetic_265,https://synthetic.paper/265,Low ROT
345,Continual Learning in Neural Networks - Study 243,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2016,50,5.0,synthetic_243,https://synthetic.paper/243,Low ROT
276,Reinforcement Learning for Autonomous Systems - Study 174,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2018,40,5.0,synthetic_174,https://synthetic.paper/174,Low ROT
252,Continual Learning in Neural Networks - Study 150,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2018,41,5.125,synthetic_150,https://synthetic.paper/150,Low ROT
389,Quantum Machine Learning Algorithms - Study 287,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2016,52,5.2,synthetic_287,https://synthetic.paper/287,Low ROT
386,Deep Learning for Medical Image Analysis - Study 284,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2021,26,5.2,synthetic_284,https://synthetic.paper/284,Low ROT
159,Federated Learning for Privacy-Preserving AI - Study 57,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2021,26,5.2,synthetic_57,https://synthetic.paper/57,Low ROT
318,Explainable AI for Trustworthy Machine Learning - Study 216,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2021,26,5.2,synthetic_216,https://synthetic.paper/216,Low ROT
265,Deep Learning for Medical Image Analysis - Study 163,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2017,48,5.333333333333333,synthetic_163,https://synthetic.paper/163,Low ROT
2,pix2code: Generating Code from a Graphical User Interface Screenshot,"Transforming a graphical user interface screenshot created by a designer into
computer code is a typical task conducted by a developer in order to build
customized software, websites, and mobile applications. In this paper, we show
that deep learning methods can be leveraged to train a model end-to-end to
automatically generate code from a single input image with over 77% of accuracy
for three different platforms (i.e. iOS, Android and web-based technologies).",2017,48,5.333333333333333,,http://arxiv.org/abs/1705.07962v2,Low ROT
284,Multi-Modal Learning with Neural Networks - Study 182,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2017,48,5.333333333333333,synthetic_182,https://synthetic.paper/182,Low ROT
102,"Input Switched Affine Networks: An RNN Architecture Designed for
  Interpretability","There exist many problem domains where the interpretability of neural network
models is essential for deployment. Here we introduce a recurrent architecture
composed of input-switched affine transformations - in other words an RNN
without any explicit nonlinearities, but with input-dependent recurrent
weights. This simple form allows the RNN to be analyzed via straightforward
linear methods: we can exactly characterize the linear contribution of each
input to the model predictions; we can use a change-of-basis to disentangle
input, output, and computational hidden unit subspaces; we can fully
reverse-engineer the architecture's solution to a simple task. Despite this
ease of interpretation, the input switched affine network achieves reasonable
performance on a text modeling tasks, and allows greater computational
efficiency than networks with standard nonlinearities.",2016,54,5.4,,http://arxiv.org/abs/1611.09434v2,Low ROT
145,Neural Architecture Search for Automated ML - Study 43,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2019,38,5.428571428571429,synthetic_43,https://synthetic.paper/43,Low ROT
134,Multi-Modal Learning with Neural Networks - Study 32,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2017,49,5.444444444444445,synthetic_32,https://synthetic.paper/32,Low ROT
369,Adversarial Training for Robust Neural Networks - Study 267,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2020,33,5.5,synthetic_267,https://synthetic.paper/267,Low ROT
305,Optimization Algorithms in Machine Learning - Study 203,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2016,55,5.5,synthetic_203,https://synthetic.paper/203,Low ROT
259,Graph Neural Networks for Social Network Analysis - Study 157,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2017,50,5.555555555555555,synthetic_157,https://synthetic.paper/157,Low ROT
28,A Joint Model for Question Answering and Question Generation,"We propose a generative machine comprehension model that learns jointly to
ask and answer questions based on documents. The proposed model uses a
sequence-to-sequence framework that encodes the document and generates a
question (answer) given an answer (question). Significant improvement in model
performance is observed empirically on the SQuAD corpus, confirming our
hypothesis that the model benefits from jointly learning to perform both tasks.
We believe the joint model's novelty offers a new perspective on machine
comprehension beyond architectural engineering, and serves as a first step
towards autonomous information seeking.",2017,50,5.555555555555555,,http://arxiv.org/abs/1706.01450v1,Low ROT
105,Federated Learning for Privacy-Preserving AI - Study 3,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2021,28,5.6,synthetic_3,https://synthetic.paper/3,Low ROT
370,Generative Adversarial Networks for Image Synthesis - Study 268,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2021,28,5.6,synthetic_268,https://synthetic.paper/268,Low ROT
267,Optimization Algorithms in Machine Learning - Study 165,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2021,28,5.6,synthetic_165,https://synthetic.paper/165,Low ROT
329,Natural Language Processing with Transformer Models - Study 227,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2016,56,5.6,synthetic_227,https://synthetic.paper/227,Low ROT
302,Quantum Machine Learning Algorithms - Study 200,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2018,45,5.625,synthetic_200,https://synthetic.paper/200,Low ROT
116,Self-Supervised Learning in Computer Vision - Study 14,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2019,40,5.714285714285714,synthetic_14,https://synthetic.paper/14,Low ROT
309,Generative Adversarial Networks for Image Synthesis - Study 207,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2019,40,5.714285714285714,synthetic_207,https://synthetic.paper/207,Low ROT
199,Reinforcement Learning for Autonomous Systems - Study 97,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2018,46,5.75,synthetic_97,https://synthetic.paper/97,Low ROT
23,Continual Learning of Natural Language Processing Tasks: A Survey,"Continual learning (CL) is a learning paradigm that emulates the human
capability of learning and accumulating knowledge continually without
forgetting the previously learned knowledge and also transferring the learned
knowledge to help learn new tasks better. This survey presents a comprehensive
review and analysis of the recent progress of CL in NLP, which has significant
differences from CL in computer vision and machine learning. It covers (1) all
CL settings with a taxonomy of existing techniques; (2) catastrophic forgetting
(CF) prevention, (3) knowledge transfer (KT), which is particularly important
for NLP tasks; and (4) some theory and the hidden challenge of inter-task class
separation (ICS). (1), (3) and (4) have not been included in the existing
survey. Finally, a list of future directions is discussed.",2022,23,5.75,,http://arxiv.org/abs/2211.12701v2,Low ROT
99,"MOHONE: Modeling Higher Order Network Effects in KnowledgeGraphs via
  Network Infused Embeddings","Many knowledge graph embedding methods operate on triples and are therefore
implicitly limited by a very local view of the entire knowledge graph. We
present a new framework MOHONE to effectively model higher order network
effects in knowledge-graphs, thus enabling one to capture varying degrees of
network connectivity (from the local to the global). Our framework is generic,
explicitly models the network scale, and captures two different aspects of
similarity in networks: (a) shared local neighborhood and (b) structural
role-based similarity. First, we introduce methods that learn network
representations of entities in the knowledge graph capturing these varied
aspects of similarity. We then propose a fast, efficient method to incorporate
the information captured by these network representations into existing
knowledge graph embeddings. We show that our method consistently and
significantly improves the performance on link prediction of several different
knowledge-graph embedding methods including TRANSE, TRANSD, DISTMULT, and
COMPLEX(by at least 4 points or 17% in some cases).",2018,46,5.75,,http://arxiv.org/abs/1811.00198v1,Low ROT
39,Sentence Pair Scoring: Towards Unified Framework for Text Comprehension,"We review the task of Sentence Pair Scoring, popular in the literature in
various forms - viewed as Answer Sentence Selection, Semantic Text Scoring,
Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a
component of Memory Networks.
  We argue that all such tasks are similar from the model perspective and
propose new baselines by comparing the performance of common IR metrics and
popular convolutional, recurrent and attention-based neural models across many
Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating
randomized models, propose a statistically grounded methodology, and attempt to
improve comparisons by releasing new datasets that are much harder than some of
the currently used well explored benchmarks. We introduce a unified open source
software framework with easily pluggable models and tasks, which enables us to
experiment with multi-task reusability of trained sentence model. We set a new
state-of-art in performance on the Ubuntu Dialogue dataset.",2016,58,5.8,,http://arxiv.org/abs/1603.06127v4,Low ROT
194,Machine Learning Applications in Computer Vision - Study 92,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2016,58,5.8,synthetic_92,https://synthetic.paper/92,Low ROT
251,Multi-Modal Learning with Neural Networks - Study 149,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2016,58,5.8,synthetic_149,https://synthetic.paper/149,Low ROT
301,Optimization Algorithms in Machine Learning - Study 199,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2019,41,5.857142857142857,synthetic_199,https://synthetic.paper/199,Low ROT
5,Learning Visual Question Answering by Bootstrapping Hard Attention,"Attention mechanisms in biological perception are thought to select subsets
of perceptual information for more sophisticated processing which would be
prohibitive to perform on all sensory inputs. In computer vision, however,
there has been relatively little exploration of hard attention, where some
information is selectively ignored, in spite of the success of soft attention,
where information is re-weighted and aggregated, but never filtered out. Here,
we introduce a new approach for hard attention and find it achieves very
competitive performance on a recently-released visual question answering
datasets, equalling and in some cases surpassing similar soft attention
architectures while entirely ignoring some features. Even though the hard
attention mechanism is thought to be non-differentiable, we found that the
feature magnitudes correlate with semantic relevance, and provide a useful
signal for our mechanism's attentional selection criterion. Because hard
attention selects important features of the input information, it can also be
more efficient than analogous soft attention mechanisms. This is especially
important for recent approaches that use non-local pairwise operations, whereby
computational and memory costs are quadratic in the size of the set of
features.",2018,47,5.875,,http://arxiv.org/abs/1808.00300v1,Low ROT
173,Neural Architecture Search for Automated ML - Study 71,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2017,53,5.888888888888889,synthetic_71,https://synthetic.paper/71,Low ROT
372,Machine Learning Applications in Computer Vision - Study 270,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2017,53,5.888888888888889,synthetic_270,https://synthetic.paper/270,Low ROT
155,Reinforcement Learning for Autonomous Systems - Study 53,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2019,42,6.0,synthetic_53,https://synthetic.paper/53,Low ROT
282,Quantum Machine Learning Algorithms - Study 180,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2017,54,6.0,synthetic_180,https://synthetic.paper/180,Low ROT
56,Attentive Tensor Product Learning,"This paper proposes a new architecture - Attentive Tensor Product Learning
(ATPL) - to represent grammatical structures in deep learning models. ATPL is a
new architecture to bridge this gap by exploiting Tensor Product
Representations (TPR), a structured neural-symbolic model developed in
cognitive science, aiming to integrate deep learning with explicit language
structures and rules. The key ideas of ATPL are: 1) unsupervised learning of
role-unbinding vectors of words via TPR-based deep neural network; 2) employing
attention modules to compute TPR; and 3) integration of TPR with typical deep
learning architectures including Long Short-Term Memory (LSTM) and Feedforward
Neural Network (FFNN). The novelty of our approach lies in its ability to
extract the grammatical structure of a sentence by using role-unbinding
vectors, which are obtained in an unsupervised manner. This ATPL approach is
applied to 1) image captioning, 2) part of speech (POS) tagging, and 3)
constituency parsing of a sentence. Experimental results demonstrate the
effectiveness of the proposed approach.",2018,49,6.125,,http://arxiv.org/abs/1802.07089v2,Low ROT
344,Graph Neural Networks for Social Network Analysis - Study 242,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2018,49,6.125,synthetic_242,https://synthetic.paper/242,Low ROT
104,Federated Learning for Privacy-Preserving AI - Study 2,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2018,49,6.125,synthetic_2,https://synthetic.paper/2,Low ROT
355,Neural Architecture Search for Automated ML - Study 253,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2020,37,6.166666666666667,synthetic_253,https://synthetic.paper/253,Low ROT
64,"CLASSIC: Continual and Contrastive Learning of Aspect Sentiment
  Classification Tasks","This paper studies continual learning (CL) of a sequence of aspect sentiment
classification(ASC) tasks in a particular CL setting called domain incremental
learning (DIL). Each task is from a different domain or product. The DIL
setting is particularly suited to ASC because in testing the system needs not
know the task/domain to which the test data belongs. To our knowledge, this
setting has not been studied before for ASC. This paper proposes a novel model
called CLASSIC. The key novelty is a contrastive continual learning method that
enables both knowledge transfer across tasks and knowledge distillation from
old tasks to the new task, which eliminates the need for task ids in testing.
Experimental results show the high effectiveness of CLASSIC.",2021,31,6.2,,http://arxiv.org/abs/2112.02714v1,Low ROT
248,Graph Neural Networks for Social Network Analysis - Study 146,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2021,31,6.2,synthetic_146,https://synthetic.paper/146,Low ROT
128,Optimization Algorithms in Machine Learning - Study 26,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2017,56,6.222222222222222,synthetic_26,https://synthetic.paper/26,Low ROT
323,Graph Neural Networks for Social Network Analysis - Study 221,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2022,25,6.25,synthetic_221,https://synthetic.paper/221,Low ROT
210,Neural Network Compression Techniques - Study 108,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2019,44,6.285714285714286,synthetic_108,https://synthetic.paper/108,Low ROT
307,Quantum Machine Learning Algorithms - Study 205,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2020,38,6.333333333333333,synthetic_205,https://synthetic.paper/205,Low ROT
378,Reinforcement Learning for Autonomous Systems - Study 276,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2020,38,6.333333333333333,synthetic_276,https://synthetic.paper/276,Low ROT
17,"Towards Explainable NLP: A Generative Explanation Framework for Text
  Classification","Building explainable systems is a critical problem in the field of Natural
Language Processing (NLP), since most machine learning models provide no
explanations for the predictions. Existing approaches for explainable machine
learning systems tend to focus on interpreting the outputs or the connections
between inputs and outputs. However, the fine-grained information is often
ignored, and the systems do not explicitly generate the human-readable
explanations. To better alleviate this problem, we propose a novel generative
explanation framework that learns to make classification decisions and generate
fine-grained explanations at the same time. More specifically, we introduce the
explainable factor and the minimum risk training approach that learn to
generate more reasonable explanations. We construct two new datasets that
contain summaries, rating scores, and fine-grained reasons. We conduct
experiments on both datasets, comparing with several strong neural network
baseline systems. Experimental results show that our method surpasses all
baselines on both datasets, and is able to generate concise explanations at the
same time.",2018,51,6.375,,http://arxiv.org/abs/1811.00196v2,Low ROT
340,Deep Learning for Medical Image Analysis - Study 238,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2019,45,6.428571428571429,synthetic_238,https://synthetic.paper/238,Low ROT
108,Quantum Machine Learning Algorithms - Study 6,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2017,58,6.444444444444445,synthetic_6,https://synthetic.paper/6,Low ROT
22,Transformer Quality in Linear Time,"We revisit the design choices in Transformers, and propose methods to address
their weaknesses in handling long sequences. First, we propose a simple layer
named gated attention unit, which allows the use of a weaker single-head
attention with minimal quality loss. We then propose a linear approximation
method complementary to this new layer, which is accelerator-friendly and
highly competitive in quality. The resulting model, named FLASH, matches the
perplexity of improved Transformers over both short (512) and long (8K) context
lengths, achieving training speedups of up to 4.9$\times$ on Wiki-40B and
12.1$\times$ on PG-19 for auto-regressive language modeling, and 4.8$\times$ on
C4 for masked language modeling.",2022,26,6.5,,http://arxiv.org/abs/2202.10447v2,Low ROT
281,Neural Network Compression Techniques - Study 179,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2018,52,6.5,synthetic_179,https://synthetic.paper/179,Low ROT
283,Neural Network Compression Techniques - Study 181,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2018,54,6.75,synthetic_181,https://synthetic.paper/181,Low ROT
319,Self-Supervised Learning in Computer Vision - Study 217,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2022,27,6.75,synthetic_217,https://synthetic.paper/217,Low ROT
122,Neural Network Compression Techniques - Study 20,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2017,61,6.777777777777778,synthetic_20,https://synthetic.paper/20,Low ROT
40,Poisoning Knowledge Graph Embeddings via Relation Inference Patterns,"We study the problem of generating data poisoning attacks against Knowledge
Graph Embedding (KGE) models for the task of link prediction in knowledge
graphs. To poison KGE models, we propose to exploit their inductive abilities
which are captured through the relationship patterns like symmetry, inversion
and composition in the knowledge graph. Specifically, to degrade the model's
prediction confidence on target facts, we propose to improve the model's
prediction confidence on a set of decoy facts. Thus, we craft adversarial
additions that can improve the model's prediction confidence on decoy facts
through different inference patterns. Our experiments demonstrate that the
proposed poisoning attacks outperform state-of-art baselines on four KGE models
for two publicly available datasets. We also find that the symmetry pattern
based attacks generalize across all model-dataset combinations which indicates
the sensitivity of KGE models to this pattern.",2021,34,6.8,,http://arxiv.org/abs/2111.06345v1,Low ROT
180,Advanced Neural Network Architectures for Deep Learning - Study 78,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2020,41,6.833333333333333,synthetic_78,https://synthetic.paper/78,Low ROT
209,Natural Language Processing with Transformer Models - Study 107,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2017,62,6.888888888888889,synthetic_107,https://synthetic.paper/107,Low ROT
42,Collaborative Storytelling with Large-scale Neural Language Models,"Storytelling plays a central role in human socializing and entertainment.
However, much of the research on automatic storytelling generation assumes that
stories will be generated by an agent without any human interaction. In this
paper, we introduce the task of collaborative storytelling, where an artificial
intelligence agent and a person collaborate to create a unique story by taking
turns adding to it. We present a collaborative storytelling system which works
with a human storyteller to create a story by generating new utterances based
on the story so far. We constructed the storytelling system by tuning a
publicly-available large scale language model on a dataset of writing prompts
and their accompanying fictional works. We identify generating sufficiently
human-like utterances to be an important technical issue and propose a
sample-and-rank approach to improve utterance quality. Quantitative evaluation
shows that our approach outperforms a baseline, and we present qualitative
evaluation of our system's capabilities.",2020,42,7.0,,http://arxiv.org/abs/2011.10208v1,Low ROT
168,Explainable AI for Trustworthy Machine Learning - Study 66,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2020,42,7.0,synthetic_66,https://synthetic.paper/66,Low ROT
19,"The Next Big Thing(s) in Unsupervised Machine Learning: Five Lessons
  from Infant Learning","After a surge in popularity of supervised Deep Learning, the desire to reduce
the dependence on curated, labelled data sets and to leverage the vast
quantities of unlabelled data available recently triggered renewed interest in
unsupervised learning algorithms. Despite a significantly improved performance
due to approaches such as the identification of disentangled latent
representations, contrastive learning, and clustering optimisations, the
performance of unsupervised machine learning still falls short of its
hypothesised potential. Machine learning has previously taken inspiration from
neuroscience and cognitive science with great success. However, this has mostly
been based on adult learners with access to labels and a vast amount of prior
knowledge. In order to push unsupervised machine learning forward, we argue
that developmental science of infant cognition might hold the key to unlocking
the next generation of unsupervised learning approaches. Conceptually, human
infant learning is the closest biological parallel to artificial unsupervised
learning, as infants too must learn useful representations from unlabelled
data. In contrast to machine learning, these new representations are learned
rapidly and from relatively few examples. Moreover, infants learn robust
representations that can be used flexibly and efficiently in a number of
different tasks and contexts. We identify five crucial factors enabling
infants' quality and speed of learning, assess the extent to which these have
already been exploited in machine learning, and propose how further adoption of
these factors can give rise to previously unseen performance levels in
unsupervised learning.",2020,42,7.0,,http://arxiv.org/abs/2009.08497v1,Low ROT
244,Optimization Algorithms in Machine Learning - Study 142,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2019,49,7.0,synthetic_142,https://synthetic.paper/142,Low ROT
218,Optimization Algorithms in Machine Learning - Study 116,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2018,59,7.375,synthetic_116,https://synthetic.paper/116,Low ROT
130,Continual Learning in Neural Networks - Study 28,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2021,37,7.4,synthetic_28,https://synthetic.paper/28,Low ROT
293,Generative Adversarial Networks for Image Synthesis - Study 191,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2021,37,7.4,synthetic_191,https://synthetic.paper/191,Low ROT
76,Budgeted Policy Learning for Task-Oriented Dialogue Systems,"This paper presents a new approach that extends Deep Dyna-Q (DDQ) by
incorporating a Budget-Conscious Scheduling (BCS) to best utilize a fixed,
small amount of user interactions (budget) for learning task-oriented dialogue
agents. BCS consists of (1) a Poisson-based global scheduler to allocate budget
over different stages of training; (2) a controller to decide at each training
step whether the agent is trained using real or simulated experiences; (3) a
user goal sampling module to generate the experiences that are most effective
for policy learning. Experiments on a movie-ticket booking task with simulated
and real users show that our approach leads to significant improvements in
success rate over the state-of-the-art baselines given the fixed budget.",2019,52,7.428571428571429,,http://arxiv.org/abs/1906.00499v1,Low ROT
149,Advanced Neural Network Architectures for Deep Learning - Study 47,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2019,52,7.428571428571429,synthetic_47,https://synthetic.paper/47,Low ROT
233,Natural Language Processing with Transformer Models - Study 131,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2021,38,7.6,synthetic_131,https://synthetic.paper/131,Low ROT
237,Optimization Algorithms in Machine Learning - Study 135,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2019,54,7.714285714285714,synthetic_135,https://synthetic.paper/135,Low ROT
239,Graph Neural Networks for Social Network Analysis - Study 137,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2018,62,7.75,synthetic_137,https://synthetic.paper/137,Low ROT
321,Multi-Modal Learning with Neural Networks - Study 219,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2018,62,7.75,synthetic_219,https://synthetic.paper/219,Low ROT
129,Federated Learning for Privacy-Preserving AI - Study 27,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2019,55,7.857142857142857,synthetic_27,https://synthetic.paper/27,Low ROT
201,Generative Adversarial Networks for Image Synthesis - Study 99,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2020,48,8.0,synthetic_99,https://synthetic.paper/99,Low ROT
31,Formal Algorithms for Transformers,"This document aims to be a self-contained, mathematically precise overview of
transformer architectures and algorithms (*not* results). It covers what
transformers are, how they are trained, what they are used for, their key
architectural components, and a preview of the most prominent models. The
reader is assumed to be familiar with basic ML terminology and simpler neural
network architectures such as MLPs.",2022,32,8.0,,http://arxiv.org/abs/2207.09238v1,Low ROT
