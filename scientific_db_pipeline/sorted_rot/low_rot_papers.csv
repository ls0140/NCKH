paper_id,title,abstract,publication_year,citation_count,rot_score,doi,source_url,rot_group
32,"HULAT at SemEval-2023 Task 9: Data augmentation for pre-trained
  transformers applied to Multilingual Tweet Intimacy Analysis","This paper describes our participation in SemEval-2023 Task 9, Intimacy
Analysis of Multilingual Tweets. We fine-tune some of the most popular
transformer models with the training dataset and synthetic data generated by
different data augmentation techniques. During the development phase, our best
results were obtained by using XLM-T. Data augmentation techniques provide a
very slight improvement in the results. Our system ranked in the 27th position
out of the 45 participating systems. Despite its modest results, our system
shows promising results in languages such as Portuguese, English, and Dutch.
All our code is available in the repository
\url{https://github.com/isegura/hulat_intimacy}.",2023,15,5.0,,http://arxiv.org/abs/2302.12794v1,Low ROT
24,"EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math
  Languages","Formal mathematics is the discipline of translating mathematics into a
programming language in which any statement can be unequivocally checked by a
computer. Mathematicians and computer scientists have spent decades of
painstaking formalization efforts developing languages such as Coq, HOL, and
Lean. Machine learning research has converged on these formal math corpora and
given rise to an assortment of methodologies to aid in interactive and
automated theorem proving. However, these papers have primarily focused on one
method, for one proof task, in one language. This paper introduces EvoGPT-f: a
novel evolutionary framework for the first systematic quantitative analysis of
the differential machine learnability of five formal math corpora (Lean 3, Lean
4, Coq, HOL 4, HOL Light) using four tokenization methods (character,
word-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not
put to rest the question of the ""best"" or ""easiest"" language to learn. Rather,
this framework and preliminary findings begin to illuminate the differential
machine learnability of these languages, offering a foundation to forge more
systematic quantitative and qualitative comparative research across
communities.",2024,16,8.0,,http://arxiv.org/abs/2402.16878v1,Low ROT
13,"LLM2TEA: Agentic AI Designer Finds Innovative Objects with Generative
  Evolutionary Multitasking","In this paper, we introduce LLM-driven MultiTask Evolutionary Algorithm
(LLM2TEA), the first agentic AI designer within a generative evolutionary
multitasking (GEM) framework that promotes the crossover and synergy of designs
from multiple domains, leading to innovative solutions that transcend
individual disciplines. Of particular interest is the discovery of objects that
are not only innovative but also conform to the physical specifications of the
real world in science and engineering. LLM2TEA comprises a large language model
to initialize a population of genotypes (defined by text prompts) describing
the objects of interest, a text-to-3D generative model to produce phenotypes
from these prompts, a classifier to interpret the semantic representations of
the objects, and a physics simulation model to assess their physical
properties. We propose several novel LLM-based multitask evolutionary operators
to guide the search toward the discovery of high-performing practical objects.
Experimental results in conceptual design optimization validate the
effectiveness of LLM2TEA, revealing from 97\% to 174\% improvement in the
diversity of innovative objects compared to the present text-to-3D generative
model baseline. In addition, more than 73\% of the generated designs have
better physical performance than the top 1\% percentile of the designs
generated in the baseline. Moreover, LLM2TEA generates designs that are not
only aesthetically creative but also functional in real-world applications.
Several of these designs have been successfully 3D-printed, emphasizing the
proposed approach's capacity to transform AI-generated outputs into tangible
physical objects. The designs produced by LLM2TEA meets practical requirements
while showcasing creative and innovative features, underscoring its potential
applications in complex design optimization and discovery.",2024,27,13.5,,http://arxiv.org/abs/2406.14917v2,Low ROT
31,Formal Algorithms for Transformers,"This document aims to be a self-contained, mathematically precise overview of
transformer architectures and algorithms (*not* results). It covers what
transformers are, how they are trained, what they are used for, their key
architectural components, and a preview of the most prominent models. The
reader is assumed to be familiar with basic ML terminology and simpler neural
network architectures such as MLPs.",2022,60,15.0,,http://arxiv.org/abs/2207.09238v1,Low ROT
12,Symbolic Discovery of Optimization Algorithms,"We present a method to formulate algorithm discovery as program search, and
apply it to discover optimization algorithms for deep neural network training.
We leverage efficient search techniques to explore an infinite and sparse
program space. To bridge the large generalization gap between proxy and target
tasks, we also introduce program selection and simplification strategies. Our
method discovers a simple and effective optimization algorithm, $\textbf{Lion}$
($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$).
It is more memory-efficient than Adam as it only keeps track of the momentum.
Different from adaptive optimizers, its update has the same magnitude for each
parameter calculated through the sign operation. We compare Lion with widely
used optimizers, such as Adam and Adafactor, for training a variety of models
on different tasks. On image classification, Lion boosts the accuracy of ViT by
up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On
vision-language contrastive learning, we achieve 88.3% $\textit{zero-shot}$ and
91.1% $\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best
results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms
Adam by achieving a better FID score and reducing the training compute by up to
2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion
exhibits a similar or better performance compared to Adam. Our analysis of Lion
reveals that its performance gain grows with the training batch size. It also
requires a smaller learning rate than Adam due to the larger norm of the update
produced by the sign function. Additionally, we examine the limitations of Lion
and identify scenarios where its improvements are small or not statistically
significant. Lion is also successfully deployed in production systems such as
Google search ads CTR model.",2023,47,15.666666666666666,,http://arxiv.org/abs/2302.06675v4,Low ROT
9,"From Neural Activations to Concepts: A Survey on Explaining Concepts in
  Neural Networks","In this paper, we review recent approaches for explaining concepts in neural
networks. Concepts can act as a natural link between learning and reasoning:
once the concepts are identified that a neural learning system uses, one can
integrate those concepts with a reasoning system for inference or use a
reasoning system to act upon them to improve or enhance the learning system. On
the other hand, knowledge can not only be extracted from neural networks but
concept knowledge can also be inserted into neural network architectures. Since
integrating learning and reasoning is at the core of neuro-symbolic AI, the
insights gained from this survey can serve as an important step towards
realizing neuro-symbolic AI based on explainable concepts.",2023,49,16.333333333333332,,http://arxiv.org/abs/2310.11884v2,Low ROT
15,"STL: A Signed and Truncated Logarithm Activation Function for Neural
  Networks","Activation functions play an essential role in neural networks. They provide
the non-linearity for the networks. Therefore, their properties are important
for neural networks' accuracy and running performance. In this paper, we
present a novel signed and truncated logarithm function as activation function.
The proposed activation function has significantly better mathematical
properties, such as being odd function, monotone, differentiable, having
unbounded value range, and a continuous nonzero gradient. These properties make
it an excellent choice as an activation function. We compare it with other
well-known activation functions in several well-known neural networks. The
results confirm that it is the state-of-the-art. The suggested activation
function can be applied in a large range of neural networks where activation
functions are necessary.",2023,50,16.666666666666668,,http://arxiv.org/abs/2307.16389v1,Low ROT
14,QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks,"The advent of noisy intermediate-scale quantum (NISQ) computers raises a
crucial challenge to design quantum neural networks for fully quantum learning
tasks. To bridge the gap, this work proposes an end-to-end learning framework
named QTN-VQC, by introducing a trainable quantum tensor network (QTN) for
quantum embedding on a variational quantum circuit (VQC). The architecture of
QTN is composed of a parametric tensor-train network for feature extraction and
a tensor product encoding for quantum embedding. We highlight the QTN for
quantum embedding in terms of two perspectives: (1) we theoretically
characterize QTN by analyzing its representation power of input features; (2)
QTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the
generation of quantum embedding to the output measurement. Our experiments on
the MNIST dataset demonstrate the advantages of QTN for quantum embedding over
other quantum embedding approaches.",2021,98,19.6,,http://arxiv.org/abs/2110.03861v3,Low ROT
3,"Show me your NFT and I tell you how it will perform: Multimodal
  representation learning for NFT selling price prediction","Non-Fungible Tokens (NFTs) represent deeds of ownership, based on blockchain
technologies and smart contracts, of unique crypto assets on digital art forms
(e.g., artworks or collectibles). In the spotlight after skyrocketing in 2021,
NFTs have attracted the attention of crypto enthusiasts and investors intent on
placing promising investments in this profitable market. However, the NFT
financial performance prediction has not been widely explored to date.
  In this work, we address the above problem based on the hypothesis that NFT
images and their textual descriptions are essential proxies to predict the NFT
selling prices. To this purpose, we propose MERLIN, a novel multimodal deep
learning framework designed to train Transformer-based language and visual
models, along with graph neural network models, on collections of NFTs' images
and texts. A key aspect in MERLIN is its independence on financial features, as
it exploits only the primary data a user interested in NFT trading would like
to deal with, i.e., NFT images and textual descriptions. By learning dense
representations of such data, a price-category classification task is performed
by MERLIN models, which can also be tuned according to user preferences in the
inference phase to mimic different risk-return investment profiles.
Experimental evaluation on a publicly available dataset has shown that MERLIN
models achieve significant performances according to several financial
assessment criteria, fostering profitable investments, and also beating
baseline machine-learning classifiers based on financial features.",2023,64,21.333333333333332,,http://arxiv.org/abs/2302.01676v2,Low ROT
11,Liquid Structural State-Space Models,"A proper parametrization of state transition matrices of linear state-space
models (SSMs) followed by standard nonlinearities enables them to efficiently
learn representations from sequential data, establishing the state-of-the-art
on a large series of long-range sequence modeling benchmarks. In this paper, we
show that we can improve further when the structural SSM such as S4 is given by
a linear liquid time-constant (LTC) state-space model. LTC neural networks are
causal continuous-time neural networks with an input-dependent state transition
module, which makes them learn to adapt to incoming inputs at inference. We
show that by using a diagonal plus low-rank decomposition of the state
transition matrix introduced in S4, and a few simplifications, the LTC-based
structural state-space model, dubbed Liquid-S4, achieves the new
state-of-the-art generalization across sequence modeling tasks with long-term
dependencies such as image, text, audio, and medical time-series, with an
average performance of 87.32% on the Long-Range Arena benchmark. On the full
raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with
a 30% reduction in parameter counts compared to S4. The additional gain in
performance is the direct result of the Liquid-S4's kernel structure that takes
into account the similarities of the input sequence samples during training and
inference.",2022,86,21.5,,http://arxiv.org/abs/2209.12951v1,Low ROT
35,"TextConvoNet:A Convolutional Neural Network based Architecture for Text
  Classification","In recent years, deep learning-based models have significantly improved the
Natural Language Processing (NLP) tasks. Specifically, the Convolutional Neural
Network (CNN), initially used for computer vision, has shown remarkable
performance for text data in various NLP problems. Most of the existing
CNN-based models use 1-dimensional convolving filters n-gram detectors), where
each filter specialises in extracting n-grams features of a particular input
word embedding. The input word embeddings, also called sentence matrix, is
treated as a matrix where each row is a word vector. Thus, it allows the model
to apply one-dimensional convolution and only extract n-gram based features
from a sentence matrix. These features can be termed as intra-sentence n-gram
features. To the extent of our knowledge, all the existing CNN models are based
on the aforementioned concept. In this paper, we present a CNN-based
architecture TextConvoNet that not only extracts the intra-sentence n-gram
features but also captures the inter-sentence n-gram features in input text
data. It uses an alternative approach for input matrix representation and
applies a two-dimensional multi-scale convolutional operation on the input. To
evaluate the performance of TextConvoNet, we perform an experimental study on
five text classification datasets. The results are evaluated by using various
performance metrics. The experimental results show that the presented
TextConvoNet outperforms state-of-the-art machine learning and deep learning
models for text classification purposes.",2022,87,21.75,,http://arxiv.org/abs/2203.05173v1,Low ROT
8,Large Language Model Guided Tree-of-Thought,"In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel
approach aimed at improving the problem-solving capabilities of auto-regressive
large language models (LLMs). The ToT technique is inspired by the human mind's
approach for solving complex reasoning tasks through trial and error. In this
process, the human mind explores the solution space through a tree-like thought
process, allowing for backtracking when necessary. To implement ToT as a
software system, we augment an LLM with additional modules including a prompter
agent, a checker module, a memory module, and a ToT controller. In order to
solve a given problem, these modules engage in a multi-round conversation with
the LLM. The memory module records the conversation and state history of the
problem solving process, which allows the system to backtrack to the previous
steps of the thought-process and explore other directions from there. To verify
the effectiveness of the proposed technique, we implemented a ToT-based solver
for the Sudoku Puzzle. Experimental results show that the ToT framework can
significantly increase the success rate of Sudoku puzzle solving. Our
implementation of the ToT-based Sudoku solver is available on GitHub:
\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.",2023,67,22.333333333333332,,http://arxiv.org/abs/2305.08291v1,Low ROT
43,Exponentially Faster Language Modelling,"Language models only really need to use an exponential fraction of their
neurons for individual inferences. As proof, we present UltraFastBERT, a BERT
variant that uses 0.3% of its neurons during inference while performing on par
with similar BERT models. UltraFastBERT selectively engages just 12 out of 4095
neurons for each layer inference. This is achieved by replacing feedforward
networks with fast feedforward networks (FFFs). While no truly efficient
implementation currently exists to unlock the full acceleration potential of
conditional neural execution, we provide high-level CPU code achieving 78x
speedup over the optimized baseline feedforward implementation, and a PyTorch
implementation delivering 40x speedup over the equivalent batched feedforward
inference. We publish our training code, benchmarking setup, and model weights.",2023,73,24.333333333333332,,http://arxiv.org/abs/2311.10770v2,Low ROT
23,Continual Learning of Natural Language Processing Tasks: A Survey,"Continual learning (CL) is a learning paradigm that emulates the human
capability of learning and accumulating knowledge continually without
forgetting the previously learned knowledge and also transferring the learned
knowledge to help learn new tasks better. This survey presents a comprehensive
review and analysis of the recent progress of CL in NLP, which has significant
differences from CL in computer vision and machine learning. It covers (1) all
CL settings with a taxonomy of existing techniques; (2) catastrophic forgetting
(CF) prevention, (3) knowledge transfer (KT), which is particularly important
for NLP tasks; and (4) some theory and the hidden challenge of inter-task class
separation (ICS). (1), (3) and (4) have not been included in the existing
survey. Finally, a list of future directions is discussed.",2022,100,25.0,,http://arxiv.org/abs/2211.12701v2,Low ROT
34,Large Language Models and Emergence: A Complex Systems Perspective,"Emergence is a concept in complexity science that describes how many-body
systems manifest novel higher-level properties, properties that can be
described by replacing high-dimensional mechanisms with lower-dimensional
effective variables and theories. This is captured by the idea ""more is
different"". Intelligence is a consummate emergent property manifesting
increasingly efficient -- cheaper and faster -- uses of emergent capabilities
to solve problems. This is captured by the idea ""less is more"". In this paper,
we first examine claims that Large Language Models exhibit emergent
capabilities, reviewing several approaches to quantifying emergence, and
secondly ask whether LLMs possess emergent intelligence.",2025,25,25.0,,http://arxiv.org/abs/2506.11135v1,Low ROT
10,Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI,"Large language models have proliferated across multiple domains in as short
period of time. There is however hesitation in the medical and healthcare
domain towards their adoption because of issues like factuality, coherence, and
hallucinations. Give the high stakes nature of healthcare, many researchers
have even cautioned against its usage until these issues are resolved. The key
to the implementation and deployment of LLMs in healthcare is to make these
models trustworthy, transparent (as much possible) and explainable. In this
paper we describe the key elements in creating reliable, trustworthy, and
unbiased models as a necessary condition for their adoption in healthcare.
Specifically we focus on the quantification, validation, and mitigation of
hallucinations in the context in healthcare. Lastly, we discuss how the future
of LLMs in healthcare may look like.",2023,87,29.0,,http://arxiv.org/abs/2311.01463v1,Low ROT
47,"Building End-To-End Dialogue Systems Using Generative Hierarchical
  Neural Network Models","We investigate the task of building open domain, conversational dialogue
systems based on large dialogue corpora using generative models. Generative
models produce system responses that are autonomously generated word-by-word,
opening up the possibility for realistic, flexible interactions. In support of
this goal, we extend the recently proposed hierarchical recurrent
encoder-decoder neural network to the dialogue domain, and demonstrate that
this model is competitive with state-of-the-art neural language models and
back-off n-gram models. We investigate the limitations of this and similar
approaches, and show how its performance can be improved by bootstrapping the
learning from a larger question-answer pair corpus and from pretrained word
embeddings.",2015,340,30.90909090909091,,http://arxiv.org/abs/1507.04808v3,Low ROT
21,"Adversarial Attacks on Knowledge Graph Embeddings via Instance
  Attribution Methods","Despite the widespread use of Knowledge Graph Embeddings (KGE), little is
known about the security vulnerabilities that might disrupt their intended
behaviour. We study data poisoning attacks against KGE models for link
prediction. These attacks craft adversarial additions or deletions at training
time to cause model failure at test time. To select adversarial deletions, we
propose to use the model-agnostic instance attribution methods from
Interpretable Machine Learning, which identify the training instances that are
most influential to a neural model's predictions on test instances. We use
these influential triples as adversarial deletions. We further propose a
heuristic method to replace one of the two entities in each influential triple
to generate adversarial additions. Our experiments show that the proposed
strategies outperform the state-of-art data poisoning attacks on KGE models and
improve the MRR degradation due to the attacks by up to 62% over the baselines.",2021,184,36.8,,http://arxiv.org/abs/2111.03120v1,Low ROT
46,"The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured
  Multi-Turn Dialogue Systems","This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost
1 million multi-turn dialogues, with a total of over 7 million utterances and
100 million words. This provides a unique resource for research into building
dialogue managers based on neural language models that can make use of large
amounts of unlabeled data. The dataset has both the multi-turn property of
conversations in the Dialog State Tracking Challenge datasets, and the
unstructured nature of interactions from microblog services such as Twitter. We
also describe two neural learning architectures suitable for analyzing this
dataset, and provide benchmark performance on the task of selecting the best
next response.",2015,432,39.27272727272727,,http://arxiv.org/abs/1506.08909v3,Low ROT
27,"A Recurrent Neural Model with Attention for the Recognition of Chinese
  Implicit Discourse Relations","We introduce an attention-based Bi-LSTM for Chinese implicit discourse
relations and demonstrate that modeling argument pairs as a joint sequence can
outperform word order-agnostic approaches. Our model benefits from a partial
sampling scheme and is conceptually simple, yet achieves state-of-the-art
performance on the Chinese Discourse Treebank. We also visualize its attention
activity to illustrate the model's ability to selectively focus on the relevant
parts of an input sequence.",2017,414,46.0,,http://arxiv.org/abs/1704.08092v1,Low ROT
16,Adaptive Integrated Layered Attention (AILA),"We propose Adaptive Integrated Layered Attention (AILA), a neural network
architecture that combines dense skip connections with different mechanisms for
adaptive feature reuse across network layers. We evaluate AILA on three
challenging tasks: price forecasting for various commodities and indices (S&P
500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the
CIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In
all cases, AILA matches strong deep learning baselines (LSTMs, Transformers,
and ResNets), achieving it at a fraction of the training and inference time.
Notably, we implement and test two versions of the model - AILA-Architecture 1,
which uses simple linear layers as the connection mechanism between layers, and
AILA-Architecture 2, which implements an attention mechanism to selectively
focus on outputs from previous layers. Both architectures are applied in a
single-task learning setting, with each model trained separately for individual
tasks. Results confirm that AILA's adaptive inter-layer connections yield
robust gains by flexibly reusing pertinent features at multiple network depths.
The AILA approach thus presents an extension to existing architectures,
improving long-range sequence modeling, image recognition with optimised
computational speed, and SOTA classification performance in practice.",2025,48,48.0,,http://arxiv.org/abs/2503.22742v2,Low ROT
40,Poisoning Knowledge Graph Embeddings via Relation Inference Patterns,"We study the problem of generating data poisoning attacks against Knowledge
Graph Embedding (KGE) models for the task of link prediction in knowledge
graphs. To poison KGE models, we propose to exploit their inductive abilities
which are captured through the relationship patterns like symmetry, inversion
and composition in the knowledge graph. Specifically, to degrade the model's
prediction confidence on target facts, we propose to improve the model's
prediction confidence on a set of decoy facts. Thus, we craft adversarial
additions that can improve the model's prediction confidence on decoy facts
through different inference patterns. Our experiments demonstrate that the
proposed poisoning attacks outperform state-of-art baselines on four KGE models
for two publicly available datasets. We also find that the symmetry pattern
based attacks generalize across all model-dataset combinations which indicates
the sensitivity of KGE models to this pattern.",2021,257,51.4,,http://arxiv.org/abs/2111.06345v1,Low ROT
38,Early stopping by correlating online indicators in neural networks,"In order to minimize the generalization error in neural networks, a novel
technique to identify overfitting phenomena when training the learner is
formally introduced. This enables support of a reliable and trustworthy early
stopping condition, thus improving the predictive power of that type of
modeling. Our proposal exploits the correlation over time in a collection of
online indicators, namely characteristic functions for indicating if a set of
hypotheses are met, associated with a range of independent stopping conditions
built from a canary judgment to evaluate the presence of overfitting. That way,
we provide a formal basis for decision making in terms of interrupting the
learning process.
  As opposed to previous approaches focused on a single criterion, we take
advantage of subsidiarities between independent assessments, thus seeking both
a wider operating range and greater diagnostic reliability. With a view to
illustrating the effectiveness of the halting condition described, we choose to
work in the sphere of natural language processing, an operational continuum
increasingly based on machine learning. As a case study, we focus on parser
generation, one of the most demanding and complex tasks in the domain. The
selection of cross-validation as a canary function enables an actual comparison
with the most representative early stopping conditions based on overfitting
identification, pointing to a promising start toward an optimal bias and
variance control.",2024,107,53.5,,http://arxiv.org/abs/2402.02513v1,Low ROT
36,EvoPrompting: Language Models for Code-Level Neural Architecture Search,"Given the recent impressive accomplishments of language models (LMs) for code
generation, we explore the use of LMs as adaptive mutation and crossover
operators for an evolutionary neural architecture search (NAS) algorithm. While
NAS still proves too difficult a task for LMs to succeed at solely through
prompting, we find that the combination of evolutionary prompt engineering with
soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse
and high performing models. We first demonstrate that EvoPrompting is effective
on the computationally efficient MNIST-1D dataset, where EvoPrompting produces
convolutional architecture variants that outperform both those designed by
human experts and naive few-shot prompting in terms of accuracy and model size.
We then apply our method to searching for graph neural networks on the CLRS
Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel
architectures that outperform current state-of-the-art models on 21 out of 30
algorithmic reasoning tasks while maintaining similar model size. EvoPrompting
is successful at designing accurate and efficient neural network architectures
across a variety of machine learning tasks, while also being general enough for
easy adaptation to other tasks beyond neural network design.",2023,180,60.0,,http://arxiv.org/abs/2302.14838v3,Low ROT
28,A Joint Model for Question Answering and Question Generation,"We propose a generative machine comprehension model that learns jointly to
ask and answer questions based on documents. The proposed model uses a
sequence-to-sequence framework that encodes the document and generates a
question (answer) given an answer (question). Significant improvement in model
performance is observed empirically on the SQuAD corpus, confirming our
hypothesis that the model benefits from jointly learning to perform both tasks.
We believe the joint model's novelty offers a new perspective on machine
comprehension beyond architectural engineering, and serves as a first step
towards autonomous information seeking.",2017,561,62.333333333333336,,http://arxiv.org/abs/1706.01450v1,Low ROT
