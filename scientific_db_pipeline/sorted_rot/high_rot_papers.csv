paper_id,title,abstract,publication_year,citation_count,rot_score,doi,source_url,rot_group
296,Generative Adversarial Networks for Image Synthesis - Study 194,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2025,259,259.0,synthetic_194,https://synthetic.paper/194,High ROT
219,Advanced Neural Network Architectures for Deep Learning - Study 117,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2025,240,240.0,synthetic_117,https://synthetic.paper/117,High ROT
264,Advanced Neural Network Architectures for Deep Learning - Study 162,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2025,229,229.0,synthetic_162,https://synthetic.paper/162,High ROT
401,Generative Adversarial Networks for Image Synthesis - Study 299,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2025,225,225.0,synthetic_299,https://synthetic.paper/299,High ROT
109,Explainable AI for Trustworthy Machine Learning - Study 7,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2025,221,221.0,synthetic_7,https://synthetic.paper/7,High ROT
176,Neural Network Compression Techniques - Study 74,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2025,220,220.0,synthetic_74,https://synthetic.paper/74,High ROT
362,Federated Learning for Privacy-Preserving AI - Study 260,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2025,209,209.0,synthetic_260,https://synthetic.paper/260,High ROT
226,Quantum Machine Learning Algorithms - Study 124,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2025,208,208.0,synthetic_124,https://synthetic.paper/124,High ROT
115,Explainable AI for Trustworthy Machine Learning - Study 13,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2025,203,203.0,synthetic_13,https://synthetic.paper/13,High ROT
275,Generative Adversarial Networks for Image Synthesis - Study 173,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2025,181,181.0,synthetic_173,https://synthetic.paper/173,High ROT
286,Reinforcement Learning for Autonomous Systems - Study 184,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2025,176,176.0,synthetic_184,https://synthetic.paper/184,High ROT
110,Optimization Algorithms in Machine Learning - Study 8,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2025,170,170.0,synthetic_8,https://synthetic.paper/8,High ROT
181,Meta-Learning Approaches for Few-Shot Learning - Study 79,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2025,168,168.0,synthetic_79,https://synthetic.paper/79,High ROT
256,Knowledge Distillation in Deep Learning - Study 154,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2025,164,164.0,synthetic_154,https://synthetic.paper/154,High ROT
285,Adversarial Training for Robust Neural Networks - Study 183,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2025,164,164.0,synthetic_183,https://synthetic.paper/183,High ROT
374,Quantum Machine Learning Algorithms - Study 272,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2025,152,152.0,synthetic_272,https://synthetic.paper/272,High ROT
133,Quantum Machine Learning Algorithms - Study 31,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2025,150,150.0,synthetic_31,https://synthetic.paper/31,High ROT
52,GLU Attention Improve Transformer,"Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.",2025,149,149.0,,http://arxiv.org/abs/2507.00022v1,High ROT
377,Machine Learning Applications in Computer Vision - Study 275,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2025,146,146.0,synthetic_275,https://synthetic.paper/275,High ROT
161,Reinforcement Learning for Autonomous Systems - Study 59,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2025,145,145.0,synthetic_59,https://synthetic.paper/59,High ROT
383,Graph Neural Networks for Social Network Analysis - Study 281,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2025,144,144.0,synthetic_281,https://synthetic.paper/281,High ROT
96,"Comply: Learning Sentences with Complex Weights inspired by Fruit Fly
  Olfaction","Biologically inspired neural networks offer alternative avenues to model data
distributions. FlyVec is a recent example that draws inspiration from the fruit
fly's olfactory circuit to tackle the task of learning word embeddings.
Surprisingly, this model performs competitively even against deep learning
approaches specifically designed to encode text, and it does so with the
highest degree of computational efficiency. We pose the question of whether
this performance can be improved further. For this, we introduce Comply. By
incorporating positional information through complex weights, we enable a
single-layer neural network to learn sequence representations. Our experiments
show that Comply not only supersedes FlyVec but also performs on par with
significantly larger state-of-the-art models. We achieve this without
additional parameters. Comply yields sparse contextual representations of
sentences that can be interpreted explicitly from the neuron weights.",2025,143,143.0,,http://arxiv.org/abs/2502.01706v2,High ROT
397,Continual Learning in Neural Networks - Study 295,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2025,140,140.0,synthetic_295,https://synthetic.paper/295,High ROT
358,Explainable AI for Trustworthy Machine Learning - Study 256,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2025,136,136.0,synthetic_256,https://synthetic.paper/256,High ROT
375,Neural Architecture Search for Automated ML - Study 273,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2025,133,133.0,synthetic_273,https://synthetic.paper/273,High ROT
13,"LLM2TEA: Agentic AI Designer Finds Innovative Objects with Generative
  Evolutionary Multitasking","In this paper, we introduce LLM-driven MultiTask Evolutionary Algorithm
(LLM2TEA), the first agentic AI designer within a generative evolutionary
multitasking (GEM) framework that promotes the crossover and synergy of designs
from multiple domains, leading to innovative solutions that transcend
individual disciplines. Of particular interest is the discovery of objects that
are not only innovative but also conform to the physical specifications of the
real world in science and engineering. LLM2TEA comprises a large language model
to initialize a population of genotypes (defined by text prompts) describing
the objects of interest, a text-to-3D generative model to produce phenotypes
from these prompts, a classifier to interpret the semantic representations of
the objects, and a physics simulation model to assess their physical
properties. We propose several novel LLM-based multitask evolutionary operators
to guide the search toward the discovery of high-performing practical objects.
Experimental results in conceptual design optimization validate the
effectiveness of LLM2TEA, revealing from 97\% to 174\% improvement in the
diversity of innovative objects compared to the present text-to-3D generative
model baseline. In addition, more than 73\% of the generated designs have
better physical performance than the top 1\% percentile of the designs
generated in the baseline. Moreover, LLM2TEA generates designs that are not
only aesthetically creative but also functional in real-world applications.
Several of these designs have been successfully 3D-printed, emphasizing the
proposed approach's capacity to transform AI-generated outputs into tangible
physical objects. The designs produced by LLM2TEA meets practical requirements
while showcasing creative and innovative features, underscoring its potential
applications in complex design optimization and discovery.",2024,248,124.0,,http://arxiv.org/abs/2406.14917v2,High ROT
34,Large Language Models and Emergence: A Complex Systems Perspective,"Emergence is a concept in complexity science that describes how many-body
systems manifest novel higher-level properties, properties that can be
described by replacing high-dimensional mechanisms with lower-dimensional
effective variables and theories. This is captured by the idea ""more is
different"". Intelligence is a consummate emergent property manifesting
increasingly efficient -- cheaper and faster -- uses of emergent capabilities
to solve problems. This is captured by the idea ""less is more"". In this paper,
we first examine claims that Large Language Models exhibit emergent
capabilities, reviewing several approaches to quantifying emergence, and
secondly ask whether LLMs possess emergent intelligence.",2025,121,121.0,,http://arxiv.org/abs/2506.11135v1,High ROT
221,Optimization Algorithms in Machine Learning - Study 119,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2025,114,114.0,synthetic_119,https://synthetic.paper/119,High ROT
290,Attention Mechanisms in Neural Networks - Study 188,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2025,109,109.0,synthetic_188,https://synthetic.paper/188,High ROT
341,Multi-Modal Learning with Neural Networks - Study 239,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2024,217,108.5,synthetic_239,https://synthetic.paper/239,High ROT
148,Reinforcement Learning for Autonomous Systems - Study 46,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2024,213,106.5,synthetic_46,https://synthetic.paper/46,High ROT
95,"Benchmarking Randomized Optimization Algorithms on Binary, Permutation,
  and Combinatorial Problem Landscapes","In this paper, we evaluate the performance of four randomized optimization
algorithms: Randomized Hill Climbing (RHC), Simulated Annealing (SA), Genetic
Algorithms (GA), and MIMIC (Mutual Information Maximizing Input Clustering),
across three distinct types of problems: binary, permutation, and
combinatorial. We systematically compare these algorithms using a set of
benchmark fitness functions that highlight the specific challenges and
requirements of each problem category. Our study analyzes each algorithm's
effectiveness based on key performance metrics, including solution quality,
convergence speed, computational cost, and robustness. Results show that while
MIMIC and GA excel in producing high-quality solutions for binary and
combinatorial problems, their computational demands vary significantly. RHC and
SA, while computationally less expensive, demonstrate limited performance in
complex problem landscapes. The findings offer valuable insights into the
trade-offs between different optimization strategies and provide practical
guidance for selecting the appropriate algorithm based on the type of problems,
accuracy requirements, and computational constraints.",2025,102,102.0,,http://arxiv.org/abs/2501.17170v1,High ROT
107,Neural Architecture Search for Automated ML - Study 5,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2024,198,99.0,synthetic_5,https://synthetic.paper/5,High ROT
379,Quantum Machine Learning Algorithms - Study 277,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2025,98,98.0,synthetic_277,https://synthetic.paper/277,High ROT
175,Reinforcement Learning for Autonomous Systems - Study 73,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2024,196,98.0,synthetic_73,https://synthetic.paper/73,High ROT
97,"LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as
  Evolutionary Optimizers","Automated feature engineering plays a critical role in improving predictive
model performance for tabular learning tasks. Traditional automated feature
engineering methods are limited by their reliance on pre-defined
transformations within fixed, manually designed search spaces, often neglecting
domain knowledge. Recent advances using Large Language Models (LLMs) have
enabled the integration of domain knowledge into the feature engineering
process. However, existing LLM-based approaches use direct prompting or rely
solely on validation scores for feature selection, failing to leverage insights
from prior feature discovery experiments or establish meaningful reasoning
between feature generation and data-driven performance. To address these
challenges, we propose LLM-FE, a novel framework that combines evolutionary
search with the domain knowledge and reasoning capabilities of LLMs to
automatically discover effective features for tabular learning tasks. LLM-FE
formulates feature engineering as a program search problem, where LLMs propose
new feature transformation programs iteratively, and data-driven feedback
guides the search process. Our results demonstrate that LLM-FE consistently
outperforms state-of-the-art baselines, significantly enhancing the performance
of tabular prediction models across diverse classification and regression
benchmarks.",2025,96,96.0,,http://arxiv.org/abs/2503.14434v2,High ROT
38,Early stopping by correlating online indicators in neural networks,"In order to minimize the generalization error in neural networks, a novel
technique to identify overfitting phenomena when training the learner is
formally introduced. This enables support of a reliable and trustworthy early
stopping condition, thus improving the predictive power of that type of
modeling. Our proposal exploits the correlation over time in a collection of
online indicators, namely characteristic functions for indicating if a set of
hypotheses are met, associated with a range of independent stopping conditions
built from a canary judgment to evaluate the presence of overfitting. That way,
we provide a formal basis for decision making in terms of interrupting the
learning process.
  As opposed to previous approaches focused on a single criterion, we take
advantage of subsidiarities between independent assessments, thus seeking both
a wider operating range and greater diagnostic reliability. With a view to
illustrating the effectiveness of the halting condition described, we choose to
work in the sphere of natural language processing, an operational continuum
increasingly based on machine learning. As a case study, we focus on parser
generation, one of the most demanding and complex tasks in the domain. The
selection of cross-validation as a canary function enables an actual comparison
with the most representative early stopping conditions based on overfitting
identification, pointing to a promising start toward an optimal bias and
variance control.",2024,190,95.0,,http://arxiv.org/abs/2402.02513v1,High ROT
373,Federated Learning for Privacy-Preserving AI - Study 271,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2025,95,95.0,synthetic_271,https://synthetic.paper/271,High ROT
299,Attention Mechanisms in Neural Networks - Study 197,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2025,89,89.0,synthetic_197,https://synthetic.paper/197,High ROT
94,"Interlocking-free Selective Rationalization Through Genetic-based
  Learning","A popular end-to-end architecture for selective rationalization is the
select-then-predict pipeline, comprising a generator to extract highlights fed
to a predictor. Such a cooperative system suffers from suboptimal equilibrium
minima due to the dominance of one of the two modules, a phenomenon known as
interlocking. While several contributions aimed at addressing interlocking,
they only mitigate its effect, often by introducing feature-based heuristics,
sampling, and ad-hoc regularizations. We present GenSPP, the first
interlocking-free architecture for selective rationalization that does not
require any learning overhead, as the above-mentioned. GenSPP avoids
interlocking by performing disjoint training of the generator and predictor via
genetic global search. Experiments on a synthetic and a real-world benchmark
show that our model outperforms several state-of-the-art competitors.",2024,174,87.0,,http://arxiv.org/abs/2412.10312v2,High ROT
360,Explainable AI for Trustworthy Machine Learning - Study 258,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2025,87,87.0,synthetic_258,https://synthetic.paper/258,High ROT
272,Adversarial Training for Robust Neural Networks - Study 170,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2025,86,86.0,synthetic_170,https://synthetic.paper/170,High ROT
343,Meta-Learning Approaches for Few-Shot Learning - Study 241,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2025,85,85.0,synthetic_241,https://synthetic.paper/241,High ROT
37,"Beyond Scale: The Diversity Coefficient as a Data Quality Metric for
  Variability in Natural Language Data","Current trends in pre-training Large Language Models (LLMs) primarily focus
on the scaling of model and dataset size. While the quality of pre-training
data is considered an important factor for training powerful LLMs, it remains a
nebulous concept that has not been rigorously characterized. To this end, we
propose a formalization of one key aspect of data quality -- measuring the
variability of natural language data -- specifically via a measure we call the
diversity coefficient. Our empirical analysis shows that the proposed diversity
coefficient aligns with the intuitive properties of diversity and variability,
e.g., it increases as the number of latent concepts increases. Then, we measure
the diversity coefficient of publicly available pre-training datasets and
demonstrate that their formal diversity is high compared to theoretical lower
and upper bounds. Finally, we conduct a comprehensive set of controlled
interventional experiments with GPT-2 and LLaMAv2 that demonstrate the
diversity coefficient of pre-training data characterizes useful aspects of
downstream model evaluation performance -- totaling 44 models of various sizes
(51M to 7B parameters). We conclude that our formal notion of diversity is an
important aspect of data quality that captures variability and causally leads
to improved evaluation performance.",2023,252,84.0,,http://arxiv.org/abs/2306.13840v3,High ROT
92,"Assessing the Emergent Symbolic Reasoning Abilities of Llama Large
  Language Models","Large Language Models (LLMs) achieve impressive performance in a wide range
of tasks, even if they are often trained with the only objective of chatting
fluently with users. Among other skills, LLMs show emergent abilities in
mathematical reasoning benchmarks, which can be elicited with appropriate
prompting methods. In this work, we systematically investigate the capabilities
and limitations of popular open-source LLMs on different symbolic reasoning
tasks. We evaluate three models of the Llama 2 family on two datasets that
require solving mathematical formulas of varying degrees of difficulty. We test
a generalist LLM (Llama 2 Chat) as well as two fine-tuned versions of Llama 2
(MAmmoTH and MetaMath) specifically designed to tackle mathematical problems.
We observe that both increasing the scale of the model and fine-tuning it on
relevant tasks lead to significant performance gains. Furthermore, using
fine-grained evaluation measures, we find that such performance gains are
mostly observed with mathematical formulas of low complexity, which
nevertheless often remain challenging even for the largest fine-tuned models.",2024,162,81.0,,http://arxiv.org/abs/2406.06588v1,High ROT
157,Federated Learning for Privacy-Preserving AI - Study 55,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2025,78,78.0,synthetic_55,https://synthetic.paper/55,High ROT
346,Graph Neural Networks for Social Network Analysis - Study 244,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2025,78,78.0,synthetic_244,https://synthetic.paper/244,High ROT
279,Generative Adversarial Networks for Image Synthesis - Study 177,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2025,76,76.0,synthetic_177,https://synthetic.paper/177,High ROT
113,Attention Mechanisms in Neural Networks - Study 11,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2023,226,75.33333333333333,synthetic_11,https://synthetic.paper/11,High ROT
160,Knowledge Distillation in Deep Learning - Study 58,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2024,150,75.0,synthetic_58,https://synthetic.paper/58,High ROT
400,Neural Architecture Search for Automated ML - Study 298,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2025,75,75.0,synthetic_298,https://synthetic.paper/298,High ROT
141,Attention Mechanisms in Neural Networks - Study 39,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2024,149,74.5,synthetic_39,https://synthetic.paper/39,High ROT
260,Deep Learning for Medical Image Analysis - Study 158,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2024,139,69.5,synthetic_158,https://synthetic.paper/158,High ROT
350,Quantum Machine Learning Algorithms - Study 248,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2024,138,69.0,synthetic_248,https://synthetic.paper/248,High ROT
230,Optimization Algorithms in Machine Learning - Study 128,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2024,136,68.0,synthetic_128,https://synthetic.paper/128,High ROT
127,Reinforcement Learning for Autonomous Systems - Study 25,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2024,135,67.5,synthetic_25,https://synthetic.paper/25,High ROT
190,Meta-Learning Approaches for Few-Shot Learning - Study 88,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2024,135,67.5,synthetic_88,https://synthetic.paper/88,High ROT
328,Natural Language Processing with Transformer Models - Study 226,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2024,132,66.0,synthetic_226,https://synthetic.paper/226,High ROT
334,Meta-Learning Approaches for Few-Shot Learning - Study 232,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2024,131,65.5,synthetic_232,https://synthetic.paper/232,High ROT
84,"HULAT at SemEval-2023 Task 10: Data augmentation for pre-trained
  transformers applied to the detection of sexism in social media","This paper describes our participation in SemEval-2023 Task 10, whose goal is
the detection of sexism in social media. We explore some of the most popular
transformer models such as BERT, DistilBERT, RoBERTa, and XLNet. We also study
different data augmentation techniques to increase the training dataset. During
the development phase, our best results were obtained by using RoBERTa and data
augmentation for tasks B and C. However, the use of synthetic data does not
improve the results for task C. We participated in the three subtasks. Our
approach still has much room for improvement, especially in the two
fine-grained classifications. All our code is available in the repository
https://github.com/isegura/hulat_edos.",2023,196,65.33333333333333,,http://arxiv.org/abs/2302.12840v2,High ROT
278,Knowledge Distillation in Deep Learning - Study 176,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2024,130,65.0,synthetic_176,https://synthetic.paper/176,High ROT
352,Natural Language Processing with Transformer Models - Study 250,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2025,64,64.0,synthetic_250,https://synthetic.paper/250,High ROT
288,Meta-Learning Approaches for Few-Shot Learning - Study 186,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2023,192,64.0,synthetic_186,https://synthetic.paper/186,High ROT
163,Continual Learning in Neural Networks - Study 61,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2023,191,63.666666666666664,synthetic_61,https://synthetic.paper/61,High ROT
36,EvoPrompting: Language Models for Code-Level Neural Architecture Search,"Given the recent impressive accomplishments of language models (LMs) for code
generation, we explore the use of LMs as adaptive mutation and crossover
operators for an evolutionary neural architecture search (NAS) algorithm. While
NAS still proves too difficult a task for LMs to succeed at solely through
prompting, we find that the combination of evolutionary prompt engineering with
soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse
and high performing models. We first demonstrate that EvoPrompting is effective
on the computationally efficient MNIST-1D dataset, where EvoPrompting produces
convolutional architecture variants that outperform both those designed by
human experts and naive few-shot prompting in terms of accuracy and model size.
We then apply our method to searching for graph neural networks on the CLRS
Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel
architectures that outperform current state-of-the-art models on 21 out of 30
algorithmic reasoning tasks while maintaining similar model size. EvoPrompting
is successful at designing accurate and efficient neural network architectures
across a variety of machine learning tasks, while also being general enough for
easy adaptation to other tasks beyond neural network design.",2023,190,63.333333333333336,,http://arxiv.org/abs/2302.14838v3,High ROT
93,"Recent Advances in Federated Learning Driven Large Language Models: A
  Survey on Architecture, Performance, and Security","Federated Learning (FL) offers a promising paradigm for training Large
Language Models (LLMs) in a decentralized manner while preserving data privacy
and minimizing communication overhead. This survey examines recent advancements
in FL-driven LLMs, with a particular emphasis on architectural designs,
performance optimization, and security concerns, including the emerging area of
machine unlearning. In this context, machine unlearning refers to the
systematic removal of specific data contributions from trained models to comply
with privacy regulations such as the Right to be Forgotten. We review a range
of strategies enabling unlearning in federated LLMs, including
perturbation-based methods, model decomposition, and incremental retraining,
while evaluating their trade-offs in terms of efficiency, privacy guarantees,
and model utility. Through selected case studies and empirical evaluations, we
analyze how these methods perform in practical FL scenarios. This survey
identifies critical research directions toward developing secure, adaptable,
and high-performing federated LLM systems for real-world deployment.",2024,126,63.0,,http://arxiv.org/abs/2406.09831v2,High ROT
16,Adaptive Integrated Layered Attention (AILA),"We propose Adaptive Integrated Layered Attention (AILA), a neural network
architecture that combines dense skip connections with different mechanisms for
adaptive feature reuse across network layers. We evaluate AILA on three
challenging tasks: price forecasting for various commodities and indices (S&P
500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the
CIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In
all cases, AILA matches strong deep learning baselines (LSTMs, Transformers,
and ResNets), achieving it at a fraction of the training and inference time.
Notably, we implement and test two versions of the model - AILA-Architecture 1,
which uses simple linear layers as the connection mechanism between layers, and
AILA-Architecture 2, which implements an attention mechanism to selectively
focus on outputs from previous layers. Both architectures are applied in a
single-task learning setting, with each model trained separately for individual
tasks. Results confirm that AILA's adaptive inter-layer connections yield
robust gains by flexibly reusing pertinent features at multiple network depths.
The AILA approach thus presents an extension to existing architectures,
improving long-range sequence modeling, image recognition with optimised
computational speed, and SOTA classification performance in practice.",2025,63,63.0,,http://arxiv.org/abs/2503.22742v2,High ROT
24,"EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math
  Languages","Formal mathematics is the discipline of translating mathematics into a
programming language in which any statement can be unequivocally checked by a
computer. Mathematicians and computer scientists have spent decades of
painstaking formalization efforts developing languages such as Coq, HOL, and
Lean. Machine learning research has converged on these formal math corpora and
given rise to an assortment of methodologies to aid in interactive and
automated theorem proving. However, these papers have primarily focused on one
method, for one proof task, in one language. This paper introduces EvoGPT-f: a
novel evolutionary framework for the first systematic quantitative analysis of
the differential machine learnability of five formal math corpora (Lean 3, Lean
4, Coq, HOL 4, HOL Light) using four tokenization methods (character,
word-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not
put to rest the question of the ""best"" or ""easiest"" language to learn. Rather,
this framework and preliminary findings begin to illuminate the differential
machine learnability of these languages, offering a foundation to forge more
systematic quantitative and qualitative comparative research across
communities.",2024,123,61.5,,http://arxiv.org/abs/2402.16878v1,High ROT
88,Class-Incremental Learning based on Label Generation,"Despite the great success of pre-trained language models, it is still a
challenge to use these models for continual learning, especially for the
class-incremental learning (CIL) setting due to catastrophic forgetting (CF).
This paper reports our finding that if we formulate CIL as a continual label
generation problem, CF is drastically reduced and the generalizable
representations of pre-trained models can be better retained. We thus propose a
new CIL method (VAG) that also leverages the sparsity of vocabulary to focus
the generation and creates pseudo-replay samples by using label semantics.
Experimental results show that VAG outperforms baselines by a large margin.",2023,179,59.666666666666664,,http://arxiv.org/abs/2306.12619v2,High ROT
280,Optimization Algorithms in Machine Learning - Study 178,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2024,119,59.5,synthetic_178,https://synthetic.paper/178,High ROT
32,"HULAT at SemEval-2023 Task 9: Data augmentation for pre-trained
  transformers applied to Multilingual Tweet Intimacy Analysis","This paper describes our participation in SemEval-2023 Task 9, Intimacy
Analysis of Multilingual Tweets. We fine-tune some of the most popular
transformer models with the training dataset and synthetic data generated by
different data augmentation techniques. During the development phase, our best
results were obtained by using XLM-T. Data augmentation techniques provide a
very slight improvement in the results. Our system ranked in the 27th position
out of the 45 participating systems. Despite its modest results, our system
shows promising results in languages such as Portuguese, English, and Dutch.
All our code is available in the repository
\url{https://github.com/isegura/hulat_intimacy}.",2023,174,58.0,,http://arxiv.org/abs/2302.12794v1,High ROT
185,Explainable AI for Trustworthy Machine Learning - Study 83,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2025,57,57.0,synthetic_83,https://synthetic.paper/83,High ROT
366,Optimization Algorithms in Machine Learning - Study 264,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2024,113,56.5,synthetic_264,https://synthetic.paper/264,High ROT
33,EvoMerge: Neuroevolution for Large Language Models,"Extensive fine-tuning on Large Language Models does not always yield better
results. Oftentimes, models tend to get better at imitating one form of data
without gaining greater reasoning ability and may even end up losing some
intelligence. Here I introduce EvoMerge, a systematic approach to large
language model training and merging. Leveraging model merging for weight
crossover and fine-tuning for weight mutation, EvoMerge establishes an
evolutionary process aimed at pushing models beyond the limits of conventional
fine-tuning.",2024,111,55.5,,http://arxiv.org/abs/2402.00070v1,High ROT
332,Multi-Modal Learning with Neural Networks - Study 230,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2024,109,54.5,synthetic_230,https://synthetic.paper/230,High ROT
311,Adversarial Training for Robust Neural Networks - Study 209,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2023,163,54.333333333333336,synthetic_209,https://synthetic.paper/209,High ROT
297,Machine Learning Applications in Computer Vision - Study 195,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2024,108,54.0,synthetic_195,https://synthetic.paper/195,High ROT
170,Generative Adversarial Networks for Image Synthesis - Study 68,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2023,159,53.0,synthetic_68,https://synthetic.paper/68,High ROT
203,Graph Neural Networks for Social Network Analysis - Study 101,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2024,103,51.5,synthetic_101,https://synthetic.paper/101,High ROT
126,Machine Learning Applications in Computer Vision - Study 24,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2023,151,50.333333333333336,synthetic_24,https://synthetic.paper/24,High ROT
291,Meta-Learning Approaches for Few-Shot Learning - Study 189,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2025,50,50.0,synthetic_189,https://synthetic.paper/189,High ROT
43,Exponentially Faster Language Modelling,"Language models only really need to use an exponential fraction of their
neurons for individual inferences. As proof, we present UltraFastBERT, a BERT
variant that uses 0.3% of its neurons during inference while performing on par
with similar BERT models. UltraFastBERT selectively engages just 12 out of 4095
neurons for each layer inference. This is achieved by replacing feedforward
networks with fast feedforward networks (FFFs). While no truly efficient
implementation currently exists to unlock the full acceleration potential of
conditional neural execution, we provide high-level CPU code achieving 78x
speedup over the optimized baseline feedforward implementation, and a PyTorch
implementation delivering 40x speedup over the equivalent batched feedforward
inference. We publish our training code, benchmarking setup, and model weights.",2023,147,49.0,,http://arxiv.org/abs/2311.10770v2,High ROT
295,Adversarial Training for Robust Neural Networks - Study 193,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2023,147,49.0,synthetic_193,https://synthetic.paper/193,High ROT
246,Natural Language Processing with Transformer Models - Study 144,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2024,97,48.5,synthetic_144,https://synthetic.paper/144,High ROT
90,"Evolutionary Multi-Objective Optimization of Large Language Model
  Prompts for Balancing Sentiments","The advent of large language models (LLMs) such as ChatGPT has attracted
considerable attention in various domains due to their remarkable performance
and versatility. As the use of these models continues to grow, the importance
of effective prompt engineering has come to the fore. Prompt optimization
emerges as a crucial challenge, as it has a direct impact on model performance
and the extraction of relevant information. Recently, evolutionary algorithms
(EAs) have shown promise in addressing this issue, paving the way for novel
optimization strategies. In this work, we propose a evolutionary
multi-objective (EMO) approach specifically tailored for prompt optimization
called EMO-Prompts, using sentiment analysis as a case study. We use sentiment
analysis capabilities as our experimental targets. Our results demonstrate that
EMO-Prompts effectively generates prompts capable of guiding the LLM to produce
texts embodying two conflicting emotions simultaneously.",2024,95,47.5,,http://arxiv.org/abs/2401.09862v1,High ROT
269,Quantum Machine Learning Algorithms - Study 167,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2024,94,47.0,synthetic_167,https://synthetic.paper/167,High ROT
86,"MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label
  Framing Detection with Contrastive Learning","This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing
Detection. We used a multi-label contrastive loss for fine-tuning large
pre-trained language models in a multi-lingual setting, achieving very
competitive results: our system was ranked first on the official test set and
on the official shared task leaderboard for five of the six languages for which
we had training data and for which we could perform fine-tuning. Here, we
describe our experimental setup, as well as various ablation studies. The code
of our system is available at https://github.com/QishengL/SemEval2023",2023,135,45.0,,http://arxiv.org/abs/2304.14339v1,High ROT
205,Continual Learning in Neural Networks - Study 103,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2023,135,45.0,synthetic_103,https://synthetic.paper/103,High ROT
178,Knowledge Distillation in Deep Learning - Study 76,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2023,131,43.666666666666664,synthetic_76,https://synthetic.paper/76,High ROT
268,Explainable AI for Trustworthy Machine Learning - Study 166,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2024,86,43.0,synthetic_166,https://synthetic.paper/166,High ROT
132,Natural Language Processing with Transformer Models - Study 30,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2023,127,42.333333333333336,synthetic_30,https://synthetic.paper/30,High ROT
124,Machine Learning Applications in Computer Vision - Study 22,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2023,123,41.0,synthetic_22,https://synthetic.paper/22,High ROT
327,Machine Learning Applications in Computer Vision - Study 225,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2023,122,40.666666666666664,synthetic_225,https://synthetic.paper/225,High ROT
89,"Sub-network Discovery and Soft-masking for Continual Learning of Mixed
  Tasks","Continual learning (CL) has two main objectives: preventing catastrophic
forgetting (CF) and encouraging knowledge transfer (KT). The existing
literature mainly focused on overcoming CF. Some work has also been done on KT
when the tasks are similar. To our knowledge, only one method has been proposed
to learn a sequence of mixed tasks. However, these techniques still suffer from
CF and/or limited KT. This paper proposes a new CL method to achieve both. It
overcomes CF by isolating the knowledge of each task via discovering a
subnetwork for it. A soft-masking mechanism is also proposed to preserve the
previous knowledge and to enable the new task to leverage the past knowledge to
achieve KT. Experiments using classification, generation, information
extraction, and their mixture (i.e., heterogeneous tasks) show that the
proposed method consistently outperforms strong baselines.",2023,121,40.333333333333336,,http://arxiv.org/abs/2310.09436v1,High ROT
364,Multi-Modal Learning with Neural Networks - Study 262,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2023,118,39.333333333333336,synthetic_262,https://synthetic.paper/262,High ROT
150,Neural Architecture Search for Automated ML - Study 48,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2023,117,39.0,synthetic_48,https://synthetic.paper/48,High ROT
125,Self-Supervised Learning in Computer Vision - Study 23,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2023,117,39.0,synthetic_23,https://synthetic.paper/23,High ROT
9,"From Neural Activations to Concepts: A Survey on Explaining Concepts in
  Neural Networks","In this paper, we review recent approaches for explaining concepts in neural
networks. Concepts can act as a natural link between learning and reasoning:
once the concepts are identified that a neural learning system uses, one can
integrate those concepts with a reasoning system for inference or use a
reasoning system to act upon them to improve or enhance the learning system. On
the other hand, knowledge can not only be extracted from neural networks but
concept knowledge can also be inserted into neural network architectures. Since
integrating learning and reasoning is at the core of neuro-symbolic AI, the
insights gained from this survey can serve as an important step towards
realizing neuro-symbolic AI based on explainable concepts.",2023,115,38.333333333333336,,http://arxiv.org/abs/2310.11884v2,High ROT
153,Reinforcement Learning for Autonomous Systems - Study 51,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2023,111,37.0,synthetic_51,https://synthetic.paper/51,High ROT
8,Large Language Model Guided Tree-of-Thought,"In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel
approach aimed at improving the problem-solving capabilities of auto-regressive
large language models (LLMs). The ToT technique is inspired by the human mind's
approach for solving complex reasoning tasks through trial and error. In this
process, the human mind explores the solution space through a tree-like thought
process, allowing for backtracking when necessary. To implement ToT as a
software system, we augment an LLM with additional modules including a prompter
agent, a checker module, a memory module, and a ToT controller. In order to
solve a given problem, these modules engage in a multi-round conversation with
the LLM. The memory module records the conversation and state history of the
problem solving process, which allows the system to backtrack to the previous
steps of the thought-process and explore other directions from there. To verify
the effectiveness of the proposed technique, we implemented a ToT-based solver
for the Sudoku Puzzle. Experimental results show that the ToT framework can
significantly increase the success rate of Sudoku puzzle solving. Our
implementation of the ToT-based Sudoku solver is available on GitHub:
\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.",2023,109,36.333333333333336,,http://arxiv.org/abs/2305.08291v1,High ROT
348,Self-Supervised Learning in Computer Vision - Study 246,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2023,109,36.333333333333336,synthetic_246,https://synthetic.paper/246,High ROT
117,Self-Supervised Learning in Computer Vision - Study 15,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2023,108,36.0,synthetic_15,https://synthetic.paper/15,High ROT
53,"Beyond Scale: The Diversity Coefficient as a Data Quality Metric for
  Variability in Natural Language Data","Current trends in pre-training Large Language Models (LLMs) primarily focus
on the scaling of model and dataset size. While the quality of pre-training
data is considered an important factor for training powerful LLMs, it remains a
nebulous concept that has not been rigorously characterized. To this end, we
propose a formalization of one key aspect of data quality -- measuring the
variability of natural language data -- specifically via a measure we call the
diversity coefficient. Our empirical analysis shows that the proposed diversity
coefficient aligns with the intuitive properties of diversity and variability,
e.g., it increases as the number of latent concepts increases. Then, we measure
the diversity coefficient of publicly available pre-training datasets and
demonstrate that their formal diversity is high compared to theoretical lower
and upper bounds. Finally, we conduct a comprehensive set of controlled
interventional experiments with GPT-2 and LLaMAv2 that demonstrate the
diversity coefficient of pre-training data characterizes useful aspects of
downstream model evaluation performance -- totaling 44 models of various sizes
(51M to 7B parameters). We conclude that our formal notion of diversity is an
important aspect of data quality that captures variability and causally leads
to improved evaluation performance.",2023,107,35.666666666666664,,http://arxiv.org/abs/2306.13840v4,High ROT
87,"Enriching language models with graph-based context information to better
  understand textual data","A considerable number of texts encountered daily are somehow connected with
each other. For example, Wikipedia articles refer to other articles via
hyperlinks, scientific papers relate to others via citations or (co)authors,
while tweets relate via users that follow each other or reshare content. Hence,
a graph-like structure can represent existing connections and be seen as
capturing the ""context"" of the texts. The question thus arises if extracting
and integrating such context information into a language model might help
facilitate a better automated understanding of the text. In this study, we
experimentally demonstrate that incorporating graph-based contextualization
into BERT model enhances its performance on an example of a classification
task. Specifically, on Pubmed dataset, we observed a reduction in error from
8.51% to 7.96%, while increasing the number of parameters just by 1.6%.
  Our source code: https://github.com/tryptofanik/gc-bert",2023,106,35.333333333333336,,http://arxiv.org/abs/2305.11070v1,High ROT
12,Symbolic Discovery of Optimization Algorithms,"We present a method to formulate algorithm discovery as program search, and
apply it to discover optimization algorithms for deep neural network training.
We leverage efficient search techniques to explore an infinite and sparse
program space. To bridge the large generalization gap between proxy and target
tasks, we also introduce program selection and simplification strategies. Our
method discovers a simple and effective optimization algorithm, $\textbf{Lion}$
($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$).
It is more memory-efficient than Adam as it only keeps track of the momentum.
Different from adaptive optimizers, its update has the same magnitude for each
parameter calculated through the sign operation. We compare Lion with widely
used optimizers, such as Adam and Adafactor, for training a variety of models
on different tasks. On image classification, Lion boosts the accuracy of ViT by
up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On
vision-language contrastive learning, we achieve 88.3% $\textit{zero-shot}$ and
91.1% $\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best
results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms
Adam by achieving a better FID score and reducing the training compute by up to
2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion
exhibits a similar or better performance compared to Adam. Our analysis of Lion
reveals that its performance gain grows with the training batch size. It also
requires a smaller learning rate than Adam due to the larger norm of the update
produced by the sign function. Additionally, we examine the limitations of Lion
and identify scenarios where its improvements are small or not statistically
significant. Lion is also successfully deployed in production systems such as
Google search ads CTR model.",2023,104,34.666666666666664,,http://arxiv.org/abs/2302.06675v4,High ROT
146,Deep Learning for Medical Image Analysis - Study 44,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2023,104,34.666666666666664,synthetic_44,https://synthetic.paper/44,High ROT
174,Explainable AI for Trustworthy Machine Learning - Study 72,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2023,104,34.666666666666664,synthetic_72,https://synthetic.paper/72,High ROT
83,Adapting a Language Model While Preserving its General Knowledge,"Domain-adaptive pre-training (or DA-training for short), also known as
post-training, aims to train a pre-trained general-purpose language model (LM)
using an unlabeled corpus of a particular domain to adapt the LM so that
end-tasks in the domain can give improved performances. However, existing
DA-training methods are in some sense blind as they do not explicitly identify
what knowledge in the LM should be preserved and what should be changed by the
domain corpus. This paper shows that the existing methods are suboptimal and
proposes a novel method to perform a more informed adaptation of the knowledge
in the LM by (1) soft-masking the attention heads based on their importance to
best preserve the general knowledge in the LM and (2) contrasting the
representations of the general and the full (both general and domain knowledge)
to learn an integrated representation with both general and domain-specific
knowledge. Experimental results will demonstrate the effectiveness of the
proposed approach.",2023,100,33.333333333333336,,http://arxiv.org/abs/2301.08986v1,High ROT
172,Attention Mechanisms in Neural Networks - Study 70,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2024,66,33.0,synthetic_70,https://synthetic.paper/70,High ROT
398,Explainable AI for Trustworthy Machine Learning - Study 296,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2023,99,33.0,synthetic_296,https://synthetic.paper/296,High ROT
135,Machine Learning Applications in Computer Vision - Study 33,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2023,97,32.333333333333336,synthetic_33,https://synthetic.paper/33,High ROT
111,Neural Architecture Search for Automated ML - Study 9,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2024,63,31.5,synthetic_9,https://synthetic.paper/9,High ROT
232,Adversarial Training for Robust Neural Networks - Study 130,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2024,63,31.5,synthetic_130,https://synthetic.paper/130,High ROT
91,"Leave No Context Behind: Efficient Infinite Context Transformers with
  Infini-attention","This work introduces an efficient method to scale Transformer-based Large
Language Models (LLMs) to infinitely long inputs with bounded memory and
computation. A key component in our proposed approach is a new attention
technique dubbed Infini-attention. The Infini-attention incorporates a
compressive memory into the vanilla attention mechanism and builds in both
masked local attention and long-term linear attention mechanisms in a single
Transformer block. We demonstrate the effectiveness of our approach on
long-context language modeling benchmarks, 1M sequence length passkey context
block retrieval and 500K length book summarization tasks with 1B and 8B LLMs.
Our approach introduces minimal bounded memory parameters and enables fast
streaming inference for LLMs.",2024,61,30.5,,http://arxiv.org/abs/2404.07143v2,High ROT
338,Natural Language Processing with Transformer Models - Study 236,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2023,91,30.333333333333332,synthetic_236,https://synthetic.paper/236,High ROT
258,Explainable AI for Trustworthy Machine Learning - Study 156,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2023,85,28.333333333333332,synthetic_156,https://synthetic.paper/156,High ROT
206,Explainable AI for Trustworthy Machine Learning - Study 104,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2023,82,27.333333333333332,synthetic_104,https://synthetic.paper/104,High ROT
138,Graph Neural Networks for Social Network Analysis - Study 36,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2023,80,26.666666666666668,synthetic_36,https://synthetic.paper/36,High ROT
10,Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI,"Large language models have proliferated across multiple domains in as short
period of time. There is however hesitation in the medical and healthcare
domain towards their adoption because of issues like factuality, coherence, and
hallucinations. Give the high stakes nature of healthcare, many researchers
have even cautioned against its usage until these issues are resolved. The key
to the implementation and deployment of LLMs in healthcare is to make these
models trustworthy, transparent (as much possible) and explainable. In this
paper we describe the key elements in creating reliable, trustworthy, and
unbiased models as a necessary condition for their adoption in healthcare.
Specifically we focus on the quantification, validation, and mitigation of
hallucinations in the context in healthcare. Lastly, we discuss how the future
of LLMs in healthcare may look like.",2023,79,26.333333333333332,,http://arxiv.org/abs/2311.01463v1,High ROT
139,Reinforcement Learning for Autonomous Systems - Study 37,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2023,78,26.0,synthetic_37,https://synthetic.paper/37,High ROT
177,Advanced Neural Network Architectures for Deep Learning - Study 75,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2023,72,24.0,synthetic_75,https://synthetic.paper/75,High ROT
85,"ACO-tagger: A Novel Method for Part-of-Speech Tagging using Ant Colony
  Optimization","Swarm Intelligence algorithms have gained significant attention in recent
years as a means of solving complex and non-deterministic problems. These
algorithms are inspired by the collective behavior of natural creatures, and
they simulate this behavior to develop intelligent agents for computational
tasks. One such algorithm is Ant Colony Optimization (ACO), which is inspired
by the foraging behavior of ants and their pheromone laying mechanism. ACO is
used for solving difficult problems that are discrete and combinatorial in
nature. Part-of-Speech (POS) tagging is a fundamental task in natural language
processing that aims to assign a part-of-speech role to each word in a
sentence. In this research paper, proposed a high-performance POS-tagging
method based on ACO called ACO-tagger. This method achieved a high accuracy
rate of 96.867%, outperforming several state-of-the-art methods. The proposed
method is fast and efficient, making it a viable option for practical
applications.",2023,71,23.666666666666668,,http://arxiv.org/abs/2303.16760v1,High ROT
15,"STL: A Signed and Truncated Logarithm Activation Function for Neural
  Networks","Activation functions play an essential role in neural networks. They provide
the non-linearity for the networks. Therefore, their properties are important
for neural networks' accuracy and running performance. In this paper, we
present a novel signed and truncated logarithm function as activation function.
The proposed activation function has significantly better mathematical
properties, such as being odd function, monotone, differentiable, having
unbounded value range, and a continuous nonzero gradient. These properties make
it an excellent choice as an activation function. We compare it with other
well-known activation functions in several well-known neural networks. The
results confirm that it is the state-of-the-art. The suggested activation
function can be applied in a large range of neural networks where activation
functions are necessary.",2023,65,21.666666666666668,,http://arxiv.org/abs/2307.16389v1,High ROT
270,Explainable AI for Trustworthy Machine Learning - Study 168,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2022,79,19.75,synthetic_168,https://synthetic.paper/168,High ROT
11,Liquid Structural State-Space Models,"A proper parametrization of state transition matrices of linear state-space
models (SSMs) followed by standard nonlinearities enables them to efficiently
learn representations from sequential data, establishing the state-of-the-art
on a large series of long-range sequence modeling benchmarks. In this paper, we
show that we can improve further when the structural SSM such as S4 is given by
a linear liquid time-constant (LTC) state-space model. LTC neural networks are
causal continuous-time neural networks with an input-dependent state transition
module, which makes them learn to adapt to incoming inputs at inference. We
show that by using a diagonal plus low-rank decomposition of the state
transition matrix introduced in S4, and a few simplifications, the LTC-based
structural state-space model, dubbed Liquid-S4, achieves the new
state-of-the-art generalization across sequence modeling tasks with long-term
dependencies such as image, text, audio, and medical time-series, with an
average performance of 87.32% on the Long-Range Arena benchmark. On the full
raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with
a 30% reduction in parameter counts compared to S4. The additional gain in
performance is the direct result of the Liquid-S4's kernel structure that takes
into account the similarities of the input sequence samples during training and
inference.",2022,78,19.5,,http://arxiv.org/abs/2209.12951v1,High ROT
393,Attention Mechanisms in Neural Networks - Study 291,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2023,58,19.333333333333332,synthetic_291,https://synthetic.paper/291,High ROT
395,Adversarial Training for Robust Neural Networks - Study 293,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2022,77,19.25,synthetic_293,https://synthetic.paper/293,High ROT
254,Quantum Machine Learning Algorithms - Study 152,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2024,38,19.0,synthetic_152,https://synthetic.paper/152,High ROT
315,Reinforcement Learning for Autonomous Systems - Study 213,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2023,56,18.666666666666668,synthetic_213,https://synthetic.paper/213,High ROT
82,"ClassActionPrediction: A Challenging Benchmark for Legal Judgment
  Prediction of Class Action Cases in the US","The research field of Legal Natural Language Processing (NLP) has been very
active recently, with Legal Judgment Prediction (LJP) becoming one of the most
extensively studied tasks. To date, most publicly released LJP datasets
originate from countries with civil law. In this work, we release, for the
first time, a challenging LJP dataset focused on class action cases in the US.
It is the first dataset in the common law system that focuses on the harder and
more realistic task involving the complaints as input instead of the often used
facts summary written by the court. Additionally, we study the difficulty of
the task by collecting expert human predictions, showing that even human
experts can only reach 53% accuracy on this dataset. Our Longformer model
clearly outperforms the human baseline (63%), despite only considering the
first 2,048 tokens. Furthermore, we perform a detailed error analysis and find
that the Longformer model is significantly better calibrated than the human
experts. Finally, we publicly release the dataset and the code used for the
experiments.",2022,73,18.25,,http://arxiv.org/abs/2211.00582v1,High ROT
385,Quantum Machine Learning Algorithms - Study 283,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2022,70,17.5,synthetic_283,https://synthetic.paper/283,High ROT
216,Self-Supervised Learning in Computer Vision - Study 114,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2022,66,16.5,synthetic_114,https://synthetic.paper/114,High ROT
3,"Show me your NFT and I tell you how it will perform: Multimodal
  representation learning for NFT selling price prediction","Non-Fungible Tokens (NFTs) represent deeds of ownership, based on blockchain
technologies and smart contracts, of unique crypto assets on digital art forms
(e.g., artworks or collectibles). In the spotlight after skyrocketing in 2021,
NFTs have attracted the attention of crypto enthusiasts and investors intent on
placing promising investments in this profitable market. However, the NFT
financial performance prediction has not been widely explored to date.
  In this work, we address the above problem based on the hypothesis that NFT
images and their textual descriptions are essential proxies to predict the NFT
selling prices. To this purpose, we propose MERLIN, a novel multimodal deep
learning framework designed to train Transformer-based language and visual
models, along with graph neural network models, on collections of NFTs' images
and texts. A key aspect in MERLIN is its independence on financial features, as
it exploits only the primary data a user interested in NFT trading would like
to deal with, i.e., NFT images and textual descriptions. By learning dense
representations of such data, a price-category classification task is performed
by MERLIN models, which can also be tuned according to user preferences in the
inference phase to mimic different risk-return investment profiles.
Experimental evaluation on a publicly available dataset has shown that MERLIN
models achieve significant performances according to several financial
assessment criteria, fostering profitable investments, and also beating
baseline machine-learning classifiers based on financial features.",2023,49,16.333333333333332,,http://arxiv.org/abs/2302.01676v2,High ROT
208,Neural Architecture Search for Automated ML - Study 106,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2022,65,16.25,synthetic_106,https://synthetic.paper/106,High ROT
179,Explainable AI for Trustworthy Machine Learning - Study 77,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2022,64,16.0,synthetic_77,https://synthetic.paper/77,High ROT
337,Self-Supervised Learning in Computer Vision - Study 235,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2022,63,15.75,synthetic_235,https://synthetic.paper/235,High ROT
354,Federated Learning for Privacy-Preserving AI - Study 252,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2021,77,15.4,synthetic_252,https://synthetic.paper/252,High ROT
277,Natural Language Processing with Transformer Models - Study 175,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2022,60,15.0,synthetic_175,https://synthetic.paper/175,High ROT
289,Natural Language Processing with Transformer Models - Study 187,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2021,74,14.8,synthetic_187,https://synthetic.paper/187,High ROT
35,"TextConvoNet:A Convolutional Neural Network based Architecture for Text
  Classification","In recent years, deep learning-based models have significantly improved the
Natural Language Processing (NLP) tasks. Specifically, the Convolutional Neural
Network (CNN), initially used for computer vision, has shown remarkable
performance for text data in various NLP problems. Most of the existing
CNN-based models use 1-dimensional convolving filters n-gram detectors), where
each filter specialises in extracting n-grams features of a particular input
word embedding. The input word embeddings, also called sentence matrix, is
treated as a matrix where each row is a word vector. Thus, it allows the model
to apply one-dimensional convolution and only extract n-gram based features
from a sentence matrix. These features can be termed as intra-sentence n-gram
features. To the extent of our knowledge, all the existing CNN models are based
on the aforementioned concept. In this paper, we present a CNN-based
architecture TextConvoNet that not only extracts the intra-sentence n-gram
features but also captures the inter-sentence n-gram features in input text
data. It uses an alternative approach for input matrix representation and
applies a two-dimensional multi-scale convolutional operation on the input. To
evaluate the performance of TextConvoNet, we perform an experimental study on
five text classification datasets. The results are evaluated by using various
performance metrics. The experimental results show that the presented
TextConvoNet outperforms state-of-the-art machine learning and deep learning
models for text classification purposes.",2022,59,14.75,,http://arxiv.org/abs/2203.05173v1,High ROT
317,Graph Neural Networks for Social Network Analysis - Study 215,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2022,57,14.25,synthetic_215,https://synthetic.paper/215,High ROT
68,"Integration of Text and Graph-based Features for Detecting Mental Health
  Disorders from Voice","With the availability of voice-enabled devices such as smart phones, mental
health disorders could be detected and treated earlier, particularly
post-pandemic. The current methods involve extracting features directly from
audio signals. In this paper, two methods are used to enrich voice analysis for
depression detection: graph transformation of voice signals, and natural
language processing of the transcript based on representational learning, fused
together to produce final class labels. The results of experiments with the
DAIC-WOZ dataset suggest that integration of text-based voice classification
and learning from low level and graph-based voice signal features can improve
the detection of mental disorders like depression.",2022,57,14.25,,http://arxiv.org/abs/2205.07006v1,High ROT
81,Continual Training of Language Models for Few-Shot Learning,"Recent work on applying large language models (LMs) achieves impressive
performance in many NLP applications. Adapting or posttraining an LM using an
unlabeled domain corpus can produce even better performance for end-tasks in
the domain. This paper proposes the problem of continually extending an LM by
incrementally post-train the LM with a sequence of unlabeled domain corpora to
expand its knowledge without forgetting its previous skills. The goal is to
improve the few-shot end-task learning in these domains. The resulting system
is called CPT (Continual PostTraining), which to our knowledge, is the first
continual post-training system. Experimental results verify its effectiveness.",2022,57,14.25,,http://arxiv.org/abs/2210.05549v1,High ROT
313,Quantum Machine Learning Algorithms - Study 211,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2021,71,14.2,synthetic_211,https://synthetic.paper/211,High ROT
66,Continual Learning with Knowledge Transfer for Sentiment Classification,"This paper studies continual learning (CL) for sentiment classification (SC).
In this setting, the CL system learns a sequence of SC tasks incrementally in a
neural network, where each task builds a classifier to classify the sentiment
of reviews of a particular product category or domain. Two natural questions
are: Can the system transfer the knowledge learned in the past from the
previous tasks to the new task to help it learn a better model for the new
task? And, can old models for previous tasks be improved in the process as
well? This paper proposes a novel technique called KAN to achieve these
objectives. KAN can markedly improve the SC accuracy of both the new task and
the old tasks via forward and backward knowledge transfer. The effectiveness of
KAN is demonstrated through extensive experiments.",2021,70,14.0,,http://arxiv.org/abs/2112.10021v1,High ROT
156,Federated Learning for Privacy-Preserving AI - Study 54,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2021,70,14.0,synthetic_54,https://synthetic.paper/54,High ROT
324,Neural Architecture Search for Automated ML - Study 222,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2022,56,14.0,synthetic_222,https://synthetic.paper/222,High ROT
62,"Contextual Sentence Classification: Detecting Sustainability Initiatives
  in Company Reports","We introduce the novel task of detecting sustainability initiatives in
company reports. Given a full report, the aim is to automatically identify
mentions of practical activities that a company has performed in order to
tackle specific societal issues. New methods for identifying continuous
sentence spans need to be developed for capturing the multi-sentence structure
of individual sustainability initiatives. We release a new dataset of company
reports in which the text has been manually annotated with sustainability
initiatives. We also evaluate different models for initiative detection,
introducing a novel aggregation and evaluation methodology. Our proposed
architecture uses sequences of consecutive sentences to account for contextual
information when making classification decisions at the individual sentence
level.",2021,68,13.6,,http://arxiv.org/abs/2110.03727v2,High ROT
330,Natural Language Processing with Transformer Models - Study 228,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2022,54,13.5,synthetic_228,https://synthetic.paper/228,High ROT
184,Machine Learning Applications in Computer Vision - Study 82,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2022,54,13.5,synthetic_82,https://synthetic.paper/82,High ROT
342,Federated Learning for Privacy-Preserving AI - Study 240,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2022,53,13.25,synthetic_240,https://synthetic.paper/240,High ROT
61,A Survey On Neural Word Embeddings,"Understanding human language has been a sub-challenge on the way of
intelligent machines. The study of meaning in natural language processing (NLP)
relies on the distributional hypothesis where language elements get meaning
from the words that co-occur within contexts. The revolutionary idea of
distributed representation for a concept is close to the working of a human
mind in that the meaning of a word is spread across several neurons, and a loss
of activation will only slightly affect the memory retrieval process.
  Neural word embeddings transformed the whole field of NLP by introducing
substantial improvements in all NLP tasks. In this survey, we provide a
comprehensive literature review on neural word embeddings. We give theoretical
foundations and describe existing work by an interplay between word embeddings
and language modelling. We provide broad coverage on neural word embeddings,
including early word embeddings, embeddings targeting specific semantic
relations, sense embeddings, morpheme embeddings, and finally, contextual
representations. Finally, we describe benchmark datasets in word embeddings'
performance evaluation and downstream tasks along with the performance results
of/due to word embeddings.",2021,65,13.0,,http://arxiv.org/abs/2110.01804v1,High ROT
120,Adversarial Training for Robust Neural Networks - Study 18,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2021,64,12.8,synthetic_18,https://synthetic.paper/18,High ROT
144,Neural Architecture Search for Automated ML - Study 42,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2022,50,12.5,synthetic_42,https://synthetic.paper/42,High ROT
112,Deep Learning for Medical Image Analysis - Study 10,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2020,75,12.5,synthetic_10,https://synthetic.paper/10,High ROT
229,Explainable AI for Trustworthy Machine Learning - Study 127,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2022,49,12.25,synthetic_127,https://synthetic.paper/127,High ROT
380,Federated Learning for Privacy-Preserving AI - Study 278,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2021,61,12.2,synthetic_278,https://synthetic.paper/278,High ROT
7,"hyper-sinh: An Accurate and Reliable Function from Shallow to Deep
  Learning in TensorFlow and Keras","This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation
function suitable for Deep Learning (DL)-based algorithms for supervised
learning, such as Convolutional Neural Networks (CNN). hyper-sinh, developed in
the open source Python libraries TensorFlow and Keras, is thus described and
validated as an accurate and reliable activation function for both shallow and
deep neural networks. Improvements in accuracy and reliability in image and
text classification tasks on five (N = 5) benchmark data sets available from
Keras are discussed. Experimental results demonstrate the overall competitive
classification performance of both shallow and deep neural networks, obtained
via this novel function. This function is evaluated with respect to gold
standard activation functions, demonstrating its overall competitive accuracy
and reliability for both image and text classification.",2020,73,12.166666666666666,,http://arxiv.org/abs/2011.07661v1,High ROT
242,Reinforcement Learning for Autonomous Systems - Study 140,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2021,60,12.0,synthetic_140,https://synthetic.paper/140,High ROT
356,Graph Neural Networks for Social Network Analysis - Study 254,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2022,48,12.0,synthetic_254,https://synthetic.paper/254,High ROT
308,Continual Learning in Neural Networks - Study 206,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2020,71,11.833333333333334,synthetic_206,https://synthetic.paper/206,High ROT
41,Robust Document Representations using Latent Topics and Metadata,"Task specific fine-tuning of a pre-trained neural language model using a
custom softmax output layer is the de facto approach of late when dealing with
document classification problems. This technique is not adequate when labeled
examples are not available at training time and when the metadata artifacts in
a document must be exploited. We address these challenges by generating
document representations that capture both text and metadata artifacts in a
task agnostic manner. Instead of traditional auto-regressive or auto-encoding
based training, our novel self-supervised approach learns a soft-partition of
the input space when generating text embeddings. Specifically, we employ a
pre-learned topic model distribution as surrogate labels and construct a loss
function based on KL divergence. Our solution also incorporates metadata
explicitly rather than just augmenting them with text. The generated document
embeddings exhibit compositional characteristics and are directly used by
downstream classification tasks to create decision boundaries from a small
number of labeled examples, thereby eschewing complicated recognition methods.
We demonstrate through extensive evaluation that our proposed cross-model
fusion solution outperforms several competitive baselines on multiple datasets.",2020,71,11.833333333333334,,http://arxiv.org/abs/2010.12681v1,High ROT
365,Reinforcement Learning for Autonomous Systems - Study 263,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2020,70,11.666666666666666,synthetic_263,https://synthetic.paper/263,High ROT
287,Self-Supervised Learning in Computer Vision - Study 185,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2020,70,11.666666666666666,synthetic_185,https://synthetic.paper/185,High ROT
158,Explainable AI for Trustworthy Machine Learning - Study 56,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2020,68,11.333333333333334,synthetic_56,https://synthetic.paper/56,High ROT
225,Machine Learning Applications in Computer Vision - Study 123,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2020,68,11.333333333333334,synthetic_123,https://synthetic.paper/123,High ROT
388,Generative Adversarial Networks for Image Synthesis - Study 286,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2020,68,11.333333333333334,synthetic_286,https://synthetic.paper/286,High ROT
65,"Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment
  Classification Tasks","This paper studies continual learning (CL) of a sequence of aspect sentiment
classification (ASC) tasks. Although some CL techniques have been proposed for
document sentiment classification, we are not aware of any CL work on ASC. A CL
system that incrementally learns a sequence of ASC tasks should address the
following two issues: (1) transfer knowledge learned from previous tasks to the
new task to help it learn a better model, and (2) maintain the performance of
the models for previous tasks so that they are not forgotten. This paper
proposes a novel capsule network based model called B-CL to address these
issues. B-CL markedly improves the ASC performance on both the new task and the
old tasks via forward and backward knowledge transfer. The effectiveness of
B-CL is demonstrated through extensive experiments.",2021,56,11.2,,http://arxiv.org/abs/2112.03271v1,High ROT
211,Explainable AI for Trustworthy Machine Learning - Study 109,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2020,66,11.0,synthetic_109,https://synthetic.paper/109,High ROT
243,Quantum Machine Learning Algorithms - Study 141,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2021,55,11.0,synthetic_141,https://synthetic.paper/141,High ROT
349,Machine Learning Applications in Computer Vision - Study 247,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2020,65,10.833333333333334,synthetic_247,https://synthetic.paper/247,High ROT
78,Template Controllable keywords-to-text Generation,"This paper proposes a novel neural model for the understudied task of
generating text from keywords. The model takes as input a set of un-ordered
keywords, and part-of-speech (POS) based template instructions. This makes it
ideal for surface realization in any NLG setup. The framework is based on the
encode-attend-decode paradigm, where keywords and templates are encoded first,
and the decoder judiciously attends over the contexts derived from the encoded
keywords and templates to generate the sentences. Training exploits weak
supervision, as the model trains on a large amount of labeled data with
keywords and POS based templates prepared through completely automatic means.
Qualitative and quantitative performance analyses on publicly available
test-data in various domains reveal our system's superiority over baselines,
built using state-of-the-art neural machine translation and controllable
transfer techniques. Our approach is indifferent to the order of input
keywords.",2020,65,10.833333333333334,,http://arxiv.org/abs/2011.03722v1,High ROT
187,Quantum Machine Learning Algorithms - Study 85,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2021,53,10.6,synthetic_85,https://synthetic.paper/85,High ROT
387,Continual Learning in Neural Networks - Study 285,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2020,63,10.5,synthetic_285,https://synthetic.paper/285,High ROT
231,Natural Language Processing with Transformer Models - Study 129,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2020,63,10.5,synthetic_129,https://synthetic.paper/129,High ROT
80,"Adaptive Perturbation-Based Gradient Estimation for Discrete Latent
  Variable Models","The integration of discrete algorithmic components in deep learning
architectures has numerous applications. Recently, Implicit Maximum Likelihood
Estimation (IMLE, Niepert, Minervini, and Franceschi 2021), a class of gradient
estimators for discrete exponential family distributions, was proposed by
combining implicit differentiation through perturbation with the path-wise
gradient estimator. However, due to the finite difference approximation of the
gradients, it is especially sensitive to the choice of the finite difference
step size, which needs to be specified by the user. In this work, we present
Adaptive IMLE (AIMLE), the first adaptive gradient estimator for complex
discrete distributions: it adaptively identifies the target distribution for
IMLE by trading off the density of gradient information with the degree of bias
in the gradient estimates. We empirically evaluate our estimator on synthetic
examples, as well as on Learning to Explain, Discrete Variational
Auto-Encoders, and Neural Relational Inference tasks. In our experiments, we
show that our adaptive gradient estimator can produce faithful estimates while
requiring orders of magnitude fewer samples than other gradient estimators.",2022,41,10.25,,http://arxiv.org/abs/2209.04862v2,High ROT
71,"Latent Topology Induction for Understanding Contextualized
  Representations","In this work, we study the representation space of contextualized embeddings
and gain insight into the hidden topology of large language models. We show
there exists a network of latent states that summarize linguistic properties of
contextualized representations. Instead of seeking alignments to existing
well-defined annotations, we infer this latent network in a fully unsupervised
way using a structured variational autoencoder. The induced states not only
serve as anchors that mark the topology (neighbors and connectivity) of the
representation manifold but also reveal the internal mechanism of encoding
sentences. With the induced network, we: (1). decompose the representation
space into a spectrum of latent states which encode fine-grained word meanings
with lexical, morphological, syntactic and semantic information; (2). show
state-state transitions encode rich phrase constructions and serve as the
backbones of the latent space. Putting the two together, we show that sentences
are represented as a traversal over the latent network where state-state
transition chains encode syntactic templates and state-word emissions fill in
the content. We demonstrate these insights with extensive experiments and
visualizations.",2022,41,10.25,,http://arxiv.org/abs/2206.01512v1,High ROT
223,Self-Supervised Learning in Computer Vision - Study 121,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2020,61,10.166666666666666,synthetic_121,https://synthetic.paper/121,High ROT
363,Knowledge Distillation in Deep Learning - Study 261,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2019,70,10.0,synthetic_261,https://synthetic.paper/261,High ROT
306,Advanced Neural Network Architectures for Deep Learning - Study 204,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2021,50,10.0,synthetic_204,https://synthetic.paper/204,High ROT
151,Machine Learning Applications in Computer Vision - Study 49,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2020,59,9.833333333333334,synthetic_49,https://synthetic.paper/49,High ROT
169,Meta-Learning Approaches for Few-Shot Learning - Study 67,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2022,39,9.75,synthetic_67,https://synthetic.paper/67,High ROT
171,Federated Learning for Privacy-Preserving AI - Study 69,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2019,68,9.714285714285714,synthetic_69,https://synthetic.paper/69,High ROT
77,"Deep Reinforcement Learning for Chatbots Using Clustered Actions and
  Human-Likeness Rewards","Training chatbots using the reinforcement learning paradigm is challenging
due to high-dimensional states, infinite action spaces and the difficulty in
specifying the reward function. We address such problems using clustered
actions instead of infinite actions, and a simple but promising reward function
based on human-likeness scores derived from human-human dialogue data. We train
Deep Reinforcement Learning (DRL) agents using chitchat data in raw
text---without any manual annotations. Experimental results using different
splits of training data report the following. First, that our agents learn
reasonable policies in the environments they get familiarised with, but their
performance drops substantially when they are exposed to a test set of unseen
dialogues. Second, that the choice of sentence embedding size between 100 and
300 dimensions is not significantly different on test data. Third, that our
proposed human-likeness rewards are reasonable for training chatbots as long as
they use lengthy dialogue histories of >=10 sentences.",2019,67,9.571428571428571,,http://arxiv.org/abs/1908.10331v1,High ROT
186,Neural Architecture Search for Automated ML - Study 84,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2022,38,9.5,synthetic_84,https://synthetic.paper/84,High ROT
69,"Mind Your Inflections! Improving NLP for Non-Standard Englishes with
  Base-Inflection Encoding","Inflectional variation is a common feature of World Englishes such as
Colloquial Singapore English and African American Vernacular English. Although
comprehension by human readers is usually unimpaired by non-standard
inflections, current NLP systems are not yet robust. We propose Base-Inflection
Encoding (BITE), a method to tokenize English text by reducing inflected words
to their base forms before reinjecting the grammatical information as special
symbols. Fine-tuning pretrained NLP models for downstream tasks using our
encoding defends against inflectional adversaries while maintaining performance
on clean data. Models using BITE generalize better to dialects with
non-standard inflections without explicit training and translation models
converge faster when trained with BITE. Finally, we show that our encoding
improves the vocabulary efficiency of popular data-driven subword tokenizers.
Since there has been no prior work on quantitatively evaluating vocabulary
efficiency, we propose metrics to do so.",2020,57,9.5,,http://arxiv.org/abs/2004.14870v4,High ROT
361,Meta-Learning Approaches for Few-Shot Learning - Study 259,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2019,66,9.428571428571429,synthetic_259,https://synthetic.paper/259,High ROT
253,Optimization Algorithms in Machine Learning - Study 151,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2021,46,9.2,synthetic_151,https://synthetic.paper/151,High ROT
118,Multi-Modal Learning with Neural Networks - Study 16,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2022,36,9.0,synthetic_16,https://synthetic.paper/16,High ROT
298,Quantum Machine Learning Algorithms - Study 196,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2019,62,8.857142857142858,synthetic_196,https://synthetic.paper/196,High ROT
200,Attention Mechanisms in Neural Networks - Study 98,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2020,53,8.833333333333334,synthetic_98,https://synthetic.paper/98,High ROT
14,QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks,"The advent of noisy intermediate-scale quantum (NISQ) computers raises a
crucial challenge to design quantum neural networks for fully quantum learning
tasks. To bridge the gap, this work proposes an end-to-end learning framework
named QTN-VQC, by introducing a trainable quantum tensor network (QTN) for
quantum embedding on a variational quantum circuit (VQC). The architecture of
QTN is composed of a parametric tensor-train network for feature extraction and
a tensor product encoding for quantum embedding. We highlight the QTN for
quantum embedding in terms of two perspectives: (1) we theoretically
characterize QTN by analyzing its representation power of input features; (2)
QTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the
generation of quantum embedding to the output measurement. Our experiments on
the MNIST dataset demonstrate the advantages of QTN for quantum embedding over
other quantum embedding approaches.",2021,44,8.8,,http://arxiv.org/abs/2110.03861v3,High ROT
214,Federated Learning for Privacy-Preserving AI - Study 112,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2021,44,8.8,synthetic_112,https://synthetic.paper/112,High ROT
183,Attention Mechanisms in Neural Networks - Study 81,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2022,35,8.75,synthetic_81,https://synthetic.paper/81,High ROT
273,Attention Mechanisms in Neural Networks - Study 171,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2020,51,8.5,synthetic_171,https://synthetic.paper/171,High ROT
335,Adversarial Training for Robust Neural Networks - Study 233,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2019,59,8.428571428571429,synthetic_233,https://synthetic.paper/233,High ROT
316,Neural Network Compression Techniques - Study 214,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2021,42,8.4,synthetic_214,https://synthetic.paper/214,High ROT
228,Continual Learning in Neural Networks - Study 126,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2018,67,8.375,synthetic_126,https://synthetic.paper/126,High ROT
235,Optimization Algorithms in Machine Learning - Study 133,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2018,65,8.125,synthetic_133,https://synthetic.paper/133,High ROT
201,Generative Adversarial Networks for Image Synthesis - Study 99,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2020,48,8.0,synthetic_99,https://synthetic.paper/99,High ROT
