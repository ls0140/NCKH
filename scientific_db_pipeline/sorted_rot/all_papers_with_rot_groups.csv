paper_id,title,abstract,publication_year,citation_count,rot_score,doi,source_url,rot_group
296,Generative Adversarial Networks for Image Synthesis - Study 194,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2025,259,259.0,synthetic_194,https://synthetic.paper/194,High ROT
219,Advanced Neural Network Architectures for Deep Learning - Study 117,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2025,240,240.0,synthetic_117,https://synthetic.paper/117,High ROT
264,Advanced Neural Network Architectures for Deep Learning - Study 162,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2025,229,229.0,synthetic_162,https://synthetic.paper/162,High ROT
401,Generative Adversarial Networks for Image Synthesis - Study 299,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2025,225,225.0,synthetic_299,https://synthetic.paper/299,High ROT
109,Explainable AI for Trustworthy Machine Learning - Study 7,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2025,221,221.0,synthetic_7,https://synthetic.paper/7,High ROT
176,Neural Network Compression Techniques - Study 74,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2025,220,220.0,synthetic_74,https://synthetic.paper/74,High ROT
362,Federated Learning for Privacy-Preserving AI - Study 260,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2025,209,209.0,synthetic_260,https://synthetic.paper/260,High ROT
226,Quantum Machine Learning Algorithms - Study 124,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2025,208,208.0,synthetic_124,https://synthetic.paper/124,High ROT
115,Explainable AI for Trustworthy Machine Learning - Study 13,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2025,203,203.0,synthetic_13,https://synthetic.paper/13,High ROT
275,Generative Adversarial Networks for Image Synthesis - Study 173,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2025,181,181.0,synthetic_173,https://synthetic.paper/173,High ROT
286,Reinforcement Learning for Autonomous Systems - Study 184,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2025,176,176.0,synthetic_184,https://synthetic.paper/184,High ROT
110,Optimization Algorithms in Machine Learning - Study 8,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2025,170,170.0,synthetic_8,https://synthetic.paper/8,High ROT
181,Meta-Learning Approaches for Few-Shot Learning - Study 79,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2025,168,168.0,synthetic_79,https://synthetic.paper/79,High ROT
256,Knowledge Distillation in Deep Learning - Study 154,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2025,164,164.0,synthetic_154,https://synthetic.paper/154,High ROT
285,Adversarial Training for Robust Neural Networks - Study 183,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2025,164,164.0,synthetic_183,https://synthetic.paper/183,High ROT
374,Quantum Machine Learning Algorithms - Study 272,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2025,152,152.0,synthetic_272,https://synthetic.paper/272,High ROT
133,Quantum Machine Learning Algorithms - Study 31,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2025,150,150.0,synthetic_31,https://synthetic.paper/31,High ROT
52,GLU Attention Improve Transformer,"Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.",2025,149,149.0,,http://arxiv.org/abs/2507.00022v1,High ROT
377,Machine Learning Applications in Computer Vision - Study 275,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2025,146,146.0,synthetic_275,https://synthetic.paper/275,High ROT
161,Reinforcement Learning for Autonomous Systems - Study 59,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2025,145,145.0,synthetic_59,https://synthetic.paper/59,High ROT
383,Graph Neural Networks for Social Network Analysis - Study 281,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2025,144,144.0,synthetic_281,https://synthetic.paper/281,High ROT
96,"Comply: Learning Sentences with Complex Weights inspired by Fruit Fly
  Olfaction","Biologically inspired neural networks offer alternative avenues to model data
distributions. FlyVec is a recent example that draws inspiration from the fruit
fly's olfactory circuit to tackle the task of learning word embeddings.
Surprisingly, this model performs competitively even against deep learning
approaches specifically designed to encode text, and it does so with the
highest degree of computational efficiency. We pose the question of whether
this performance can be improved further. For this, we introduce Comply. By
incorporating positional information through complex weights, we enable a
single-layer neural network to learn sequence representations. Our experiments
show that Comply not only supersedes FlyVec but also performs on par with
significantly larger state-of-the-art models. We achieve this without
additional parameters. Comply yields sparse contextual representations of
sentences that can be interpreted explicitly from the neuron weights.",2025,143,143.0,,http://arxiv.org/abs/2502.01706v2,High ROT
397,Continual Learning in Neural Networks - Study 295,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2025,140,140.0,synthetic_295,https://synthetic.paper/295,High ROT
358,Explainable AI for Trustworthy Machine Learning - Study 256,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2025,136,136.0,synthetic_256,https://synthetic.paper/256,High ROT
375,Neural Architecture Search for Automated ML - Study 273,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2025,133,133.0,synthetic_273,https://synthetic.paper/273,High ROT
13,"LLM2TEA: Agentic AI Designer Finds Innovative Objects with Generative
  Evolutionary Multitasking","In this paper, we introduce LLM-driven MultiTask Evolutionary Algorithm
(LLM2TEA), the first agentic AI designer within a generative evolutionary
multitasking (GEM) framework that promotes the crossover and synergy of designs
from multiple domains, leading to innovative solutions that transcend
individual disciplines. Of particular interest is the discovery of objects that
are not only innovative but also conform to the physical specifications of the
real world in science and engineering. LLM2TEA comprises a large language model
to initialize a population of genotypes (defined by text prompts) describing
the objects of interest, a text-to-3D generative model to produce phenotypes
from these prompts, a classifier to interpret the semantic representations of
the objects, and a physics simulation model to assess their physical
properties. We propose several novel LLM-based multitask evolutionary operators
to guide the search toward the discovery of high-performing practical objects.
Experimental results in conceptual design optimization validate the
effectiveness of LLM2TEA, revealing from 97\% to 174\% improvement in the
diversity of innovative objects compared to the present text-to-3D generative
model baseline. In addition, more than 73\% of the generated designs have
better physical performance than the top 1\% percentile of the designs
generated in the baseline. Moreover, LLM2TEA generates designs that are not
only aesthetically creative but also functional in real-world applications.
Several of these designs have been successfully 3D-printed, emphasizing the
proposed approach's capacity to transform AI-generated outputs into tangible
physical objects. The designs produced by LLM2TEA meets practical requirements
while showcasing creative and innovative features, underscoring its potential
applications in complex design optimization and discovery.",2024,248,124.0,,http://arxiv.org/abs/2406.14917v2,High ROT
34,Large Language Models and Emergence: A Complex Systems Perspective,"Emergence is a concept in complexity science that describes how many-body
systems manifest novel higher-level properties, properties that can be
described by replacing high-dimensional mechanisms with lower-dimensional
effective variables and theories. This is captured by the idea ""more is
different"". Intelligence is a consummate emergent property manifesting
increasingly efficient -- cheaper and faster -- uses of emergent capabilities
to solve problems. This is captured by the idea ""less is more"". In this paper,
we first examine claims that Large Language Models exhibit emergent
capabilities, reviewing several approaches to quantifying emergence, and
secondly ask whether LLMs possess emergent intelligence.",2025,121,121.0,,http://arxiv.org/abs/2506.11135v1,High ROT
221,Optimization Algorithms in Machine Learning - Study 119,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2025,114,114.0,synthetic_119,https://synthetic.paper/119,High ROT
290,Attention Mechanisms in Neural Networks - Study 188,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2025,109,109.0,synthetic_188,https://synthetic.paper/188,High ROT
341,Multi-Modal Learning with Neural Networks - Study 239,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2024,217,108.5,synthetic_239,https://synthetic.paper/239,High ROT
148,Reinforcement Learning for Autonomous Systems - Study 46,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2024,213,106.5,synthetic_46,https://synthetic.paper/46,High ROT
95,"Benchmarking Randomized Optimization Algorithms on Binary, Permutation,
  and Combinatorial Problem Landscapes","In this paper, we evaluate the performance of four randomized optimization
algorithms: Randomized Hill Climbing (RHC), Simulated Annealing (SA), Genetic
Algorithms (GA), and MIMIC (Mutual Information Maximizing Input Clustering),
across three distinct types of problems: binary, permutation, and
combinatorial. We systematically compare these algorithms using a set of
benchmark fitness functions that highlight the specific challenges and
requirements of each problem category. Our study analyzes each algorithm's
effectiveness based on key performance metrics, including solution quality,
convergence speed, computational cost, and robustness. Results show that while
MIMIC and GA excel in producing high-quality solutions for binary and
combinatorial problems, their computational demands vary significantly. RHC and
SA, while computationally less expensive, demonstrate limited performance in
complex problem landscapes. The findings offer valuable insights into the
trade-offs between different optimization strategies and provide practical
guidance for selecting the appropriate algorithm based on the type of problems,
accuracy requirements, and computational constraints.",2025,102,102.0,,http://arxiv.org/abs/2501.17170v1,High ROT
107,Neural Architecture Search for Automated ML - Study 5,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2024,198,99.0,synthetic_5,https://synthetic.paper/5,High ROT
379,Quantum Machine Learning Algorithms - Study 277,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2025,98,98.0,synthetic_277,https://synthetic.paper/277,High ROT
175,Reinforcement Learning for Autonomous Systems - Study 73,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2024,196,98.0,synthetic_73,https://synthetic.paper/73,High ROT
97,"LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as
  Evolutionary Optimizers","Automated feature engineering plays a critical role in improving predictive
model performance for tabular learning tasks. Traditional automated feature
engineering methods are limited by their reliance on pre-defined
transformations within fixed, manually designed search spaces, often neglecting
domain knowledge. Recent advances using Large Language Models (LLMs) have
enabled the integration of domain knowledge into the feature engineering
process. However, existing LLM-based approaches use direct prompting or rely
solely on validation scores for feature selection, failing to leverage insights
from prior feature discovery experiments or establish meaningful reasoning
between feature generation and data-driven performance. To address these
challenges, we propose LLM-FE, a novel framework that combines evolutionary
search with the domain knowledge and reasoning capabilities of LLMs to
automatically discover effective features for tabular learning tasks. LLM-FE
formulates feature engineering as a program search problem, where LLMs propose
new feature transformation programs iteratively, and data-driven feedback
guides the search process. Our results demonstrate that LLM-FE consistently
outperforms state-of-the-art baselines, significantly enhancing the performance
of tabular prediction models across diverse classification and regression
benchmarks.",2025,96,96.0,,http://arxiv.org/abs/2503.14434v2,High ROT
38,Early stopping by correlating online indicators in neural networks,"In order to minimize the generalization error in neural networks, a novel
technique to identify overfitting phenomena when training the learner is
formally introduced. This enables support of a reliable and trustworthy early
stopping condition, thus improving the predictive power of that type of
modeling. Our proposal exploits the correlation over time in a collection of
online indicators, namely characteristic functions for indicating if a set of
hypotheses are met, associated with a range of independent stopping conditions
built from a canary judgment to evaluate the presence of overfitting. That way,
we provide a formal basis for decision making in terms of interrupting the
learning process.
  As opposed to previous approaches focused on a single criterion, we take
advantage of subsidiarities between independent assessments, thus seeking both
a wider operating range and greater diagnostic reliability. With a view to
illustrating the effectiveness of the halting condition described, we choose to
work in the sphere of natural language processing, an operational continuum
increasingly based on machine learning. As a case study, we focus on parser
generation, one of the most demanding and complex tasks in the domain. The
selection of cross-validation as a canary function enables an actual comparison
with the most representative early stopping conditions based on overfitting
identification, pointing to a promising start toward an optimal bias and
variance control.",2024,190,95.0,,http://arxiv.org/abs/2402.02513v1,High ROT
373,Federated Learning for Privacy-Preserving AI - Study 271,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2025,95,95.0,synthetic_271,https://synthetic.paper/271,High ROT
299,Attention Mechanisms in Neural Networks - Study 197,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2025,89,89.0,synthetic_197,https://synthetic.paper/197,High ROT
94,"Interlocking-free Selective Rationalization Through Genetic-based
  Learning","A popular end-to-end architecture for selective rationalization is the
select-then-predict pipeline, comprising a generator to extract highlights fed
to a predictor. Such a cooperative system suffers from suboptimal equilibrium
minima due to the dominance of one of the two modules, a phenomenon known as
interlocking. While several contributions aimed at addressing interlocking,
they only mitigate its effect, often by introducing feature-based heuristics,
sampling, and ad-hoc regularizations. We present GenSPP, the first
interlocking-free architecture for selective rationalization that does not
require any learning overhead, as the above-mentioned. GenSPP avoids
interlocking by performing disjoint training of the generator and predictor via
genetic global search. Experiments on a synthetic and a real-world benchmark
show that our model outperforms several state-of-the-art competitors.",2024,174,87.0,,http://arxiv.org/abs/2412.10312v2,High ROT
360,Explainable AI for Trustworthy Machine Learning - Study 258,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2025,87,87.0,synthetic_258,https://synthetic.paper/258,High ROT
272,Adversarial Training for Robust Neural Networks - Study 170,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2025,86,86.0,synthetic_170,https://synthetic.paper/170,High ROT
343,Meta-Learning Approaches for Few-Shot Learning - Study 241,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2025,85,85.0,synthetic_241,https://synthetic.paper/241,High ROT
37,"Beyond Scale: The Diversity Coefficient as a Data Quality Metric for
  Variability in Natural Language Data","Current trends in pre-training Large Language Models (LLMs) primarily focus
on the scaling of model and dataset size. While the quality of pre-training
data is considered an important factor for training powerful LLMs, it remains a
nebulous concept that has not been rigorously characterized. To this end, we
propose a formalization of one key aspect of data quality -- measuring the
variability of natural language data -- specifically via a measure we call the
diversity coefficient. Our empirical analysis shows that the proposed diversity
coefficient aligns with the intuitive properties of diversity and variability,
e.g., it increases as the number of latent concepts increases. Then, we measure
the diversity coefficient of publicly available pre-training datasets and
demonstrate that their formal diversity is high compared to theoretical lower
and upper bounds. Finally, we conduct a comprehensive set of controlled
interventional experiments with GPT-2 and LLaMAv2 that demonstrate the
diversity coefficient of pre-training data characterizes useful aspects of
downstream model evaluation performance -- totaling 44 models of various sizes
(51M to 7B parameters). We conclude that our formal notion of diversity is an
important aspect of data quality that captures variability and causally leads
to improved evaluation performance.",2023,252,84.0,,http://arxiv.org/abs/2306.13840v3,High ROT
92,"Assessing the Emergent Symbolic Reasoning Abilities of Llama Large
  Language Models","Large Language Models (LLMs) achieve impressive performance in a wide range
of tasks, even if they are often trained with the only objective of chatting
fluently with users. Among other skills, LLMs show emergent abilities in
mathematical reasoning benchmarks, which can be elicited with appropriate
prompting methods. In this work, we systematically investigate the capabilities
and limitations of popular open-source LLMs on different symbolic reasoning
tasks. We evaluate three models of the Llama 2 family on two datasets that
require solving mathematical formulas of varying degrees of difficulty. We test
a generalist LLM (Llama 2 Chat) as well as two fine-tuned versions of Llama 2
(MAmmoTH and MetaMath) specifically designed to tackle mathematical problems.
We observe that both increasing the scale of the model and fine-tuning it on
relevant tasks lead to significant performance gains. Furthermore, using
fine-grained evaluation measures, we find that such performance gains are
mostly observed with mathematical formulas of low complexity, which
nevertheless often remain challenging even for the largest fine-tuned models.",2024,162,81.0,,http://arxiv.org/abs/2406.06588v1,High ROT
157,Federated Learning for Privacy-Preserving AI - Study 55,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2025,78,78.0,synthetic_55,https://synthetic.paper/55,High ROT
346,Graph Neural Networks for Social Network Analysis - Study 244,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2025,78,78.0,synthetic_244,https://synthetic.paper/244,High ROT
279,Generative Adversarial Networks for Image Synthesis - Study 177,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2025,76,76.0,synthetic_177,https://synthetic.paper/177,High ROT
113,Attention Mechanisms in Neural Networks - Study 11,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2023,226,75.33333333333333,synthetic_11,https://synthetic.paper/11,High ROT
160,Knowledge Distillation in Deep Learning - Study 58,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2024,150,75.0,synthetic_58,https://synthetic.paper/58,High ROT
400,Neural Architecture Search for Automated ML - Study 298,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2025,75,75.0,synthetic_298,https://synthetic.paper/298,High ROT
141,Attention Mechanisms in Neural Networks - Study 39,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2024,149,74.5,synthetic_39,https://synthetic.paper/39,High ROT
260,Deep Learning for Medical Image Analysis - Study 158,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2024,139,69.5,synthetic_158,https://synthetic.paper/158,High ROT
350,Quantum Machine Learning Algorithms - Study 248,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2024,138,69.0,synthetic_248,https://synthetic.paper/248,High ROT
230,Optimization Algorithms in Machine Learning - Study 128,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2024,136,68.0,synthetic_128,https://synthetic.paper/128,High ROT
127,Reinforcement Learning for Autonomous Systems - Study 25,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2024,135,67.5,synthetic_25,https://synthetic.paper/25,High ROT
190,Meta-Learning Approaches for Few-Shot Learning - Study 88,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2024,135,67.5,synthetic_88,https://synthetic.paper/88,High ROT
328,Natural Language Processing with Transformer Models - Study 226,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2024,132,66.0,synthetic_226,https://synthetic.paper/226,High ROT
334,Meta-Learning Approaches for Few-Shot Learning - Study 232,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2024,131,65.5,synthetic_232,https://synthetic.paper/232,High ROT
84,"HULAT at SemEval-2023 Task 10: Data augmentation for pre-trained
  transformers applied to the detection of sexism in social media","This paper describes our participation in SemEval-2023 Task 10, whose goal is
the detection of sexism in social media. We explore some of the most popular
transformer models such as BERT, DistilBERT, RoBERTa, and XLNet. We also study
different data augmentation techniques to increase the training dataset. During
the development phase, our best results were obtained by using RoBERTa and data
augmentation for tasks B and C. However, the use of synthetic data does not
improve the results for task C. We participated in the three subtasks. Our
approach still has much room for improvement, especially in the two
fine-grained classifications. All our code is available in the repository
https://github.com/isegura/hulat_edos.",2023,196,65.33333333333333,,http://arxiv.org/abs/2302.12840v2,High ROT
278,Knowledge Distillation in Deep Learning - Study 176,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2024,130,65.0,synthetic_176,https://synthetic.paper/176,High ROT
352,Natural Language Processing with Transformer Models - Study 250,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2025,64,64.0,synthetic_250,https://synthetic.paper/250,High ROT
288,Meta-Learning Approaches for Few-Shot Learning - Study 186,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2023,192,64.0,synthetic_186,https://synthetic.paper/186,High ROT
163,Continual Learning in Neural Networks - Study 61,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2023,191,63.666666666666664,synthetic_61,https://synthetic.paper/61,High ROT
36,EvoPrompting: Language Models for Code-Level Neural Architecture Search,"Given the recent impressive accomplishments of language models (LMs) for code
generation, we explore the use of LMs as adaptive mutation and crossover
operators for an evolutionary neural architecture search (NAS) algorithm. While
NAS still proves too difficult a task for LMs to succeed at solely through
prompting, we find that the combination of evolutionary prompt engineering with
soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse
and high performing models. We first demonstrate that EvoPrompting is effective
on the computationally efficient MNIST-1D dataset, where EvoPrompting produces
convolutional architecture variants that outperform both those designed by
human experts and naive few-shot prompting in terms of accuracy and model size.
We then apply our method to searching for graph neural networks on the CLRS
Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel
architectures that outperform current state-of-the-art models on 21 out of 30
algorithmic reasoning tasks while maintaining similar model size. EvoPrompting
is successful at designing accurate and efficient neural network architectures
across a variety of machine learning tasks, while also being general enough for
easy adaptation to other tasks beyond neural network design.",2023,190,63.333333333333336,,http://arxiv.org/abs/2302.14838v3,High ROT
93,"Recent Advances in Federated Learning Driven Large Language Models: A
  Survey on Architecture, Performance, and Security","Federated Learning (FL) offers a promising paradigm for training Large
Language Models (LLMs) in a decentralized manner while preserving data privacy
and minimizing communication overhead. This survey examines recent advancements
in FL-driven LLMs, with a particular emphasis on architectural designs,
performance optimization, and security concerns, including the emerging area of
machine unlearning. In this context, machine unlearning refers to the
systematic removal of specific data contributions from trained models to comply
with privacy regulations such as the Right to be Forgotten. We review a range
of strategies enabling unlearning in federated LLMs, including
perturbation-based methods, model decomposition, and incremental retraining,
while evaluating their trade-offs in terms of efficiency, privacy guarantees,
and model utility. Through selected case studies and empirical evaluations, we
analyze how these methods perform in practical FL scenarios. This survey
identifies critical research directions toward developing secure, adaptable,
and high-performing federated LLM systems for real-world deployment.",2024,126,63.0,,http://arxiv.org/abs/2406.09831v2,High ROT
16,Adaptive Integrated Layered Attention (AILA),"We propose Adaptive Integrated Layered Attention (AILA), a neural network
architecture that combines dense skip connections with different mechanisms for
adaptive feature reuse across network layers. We evaluate AILA on three
challenging tasks: price forecasting for various commodities and indices (S&P
500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the
CIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In
all cases, AILA matches strong deep learning baselines (LSTMs, Transformers,
and ResNets), achieving it at a fraction of the training and inference time.
Notably, we implement and test two versions of the model - AILA-Architecture 1,
which uses simple linear layers as the connection mechanism between layers, and
AILA-Architecture 2, which implements an attention mechanism to selectively
focus on outputs from previous layers. Both architectures are applied in a
single-task learning setting, with each model trained separately for individual
tasks. Results confirm that AILA's adaptive inter-layer connections yield
robust gains by flexibly reusing pertinent features at multiple network depths.
The AILA approach thus presents an extension to existing architectures,
improving long-range sequence modeling, image recognition with optimised
computational speed, and SOTA classification performance in practice.",2025,63,63.0,,http://arxiv.org/abs/2503.22742v2,High ROT
24,"EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math
  Languages","Formal mathematics is the discipline of translating mathematics into a
programming language in which any statement can be unequivocally checked by a
computer. Mathematicians and computer scientists have spent decades of
painstaking formalization efforts developing languages such as Coq, HOL, and
Lean. Machine learning research has converged on these formal math corpora and
given rise to an assortment of methodologies to aid in interactive and
automated theorem proving. However, these papers have primarily focused on one
method, for one proof task, in one language. This paper introduces EvoGPT-f: a
novel evolutionary framework for the first systematic quantitative analysis of
the differential machine learnability of five formal math corpora (Lean 3, Lean
4, Coq, HOL 4, HOL Light) using four tokenization methods (character,
word-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not
put to rest the question of the ""best"" or ""easiest"" language to learn. Rather,
this framework and preliminary findings begin to illuminate the differential
machine learnability of these languages, offering a foundation to forge more
systematic quantitative and qualitative comparative research across
communities.",2024,123,61.5,,http://arxiv.org/abs/2402.16878v1,High ROT
88,Class-Incremental Learning based on Label Generation,"Despite the great success of pre-trained language models, it is still a
challenge to use these models for continual learning, especially for the
class-incremental learning (CIL) setting due to catastrophic forgetting (CF).
This paper reports our finding that if we formulate CIL as a continual label
generation problem, CF is drastically reduced and the generalizable
representations of pre-trained models can be better retained. We thus propose a
new CIL method (VAG) that also leverages the sparsity of vocabulary to focus
the generation and creates pseudo-replay samples by using label semantics.
Experimental results show that VAG outperforms baselines by a large margin.",2023,179,59.666666666666664,,http://arxiv.org/abs/2306.12619v2,High ROT
280,Optimization Algorithms in Machine Learning - Study 178,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2024,119,59.5,synthetic_178,https://synthetic.paper/178,High ROT
32,"HULAT at SemEval-2023 Task 9: Data augmentation for pre-trained
  transformers applied to Multilingual Tweet Intimacy Analysis","This paper describes our participation in SemEval-2023 Task 9, Intimacy
Analysis of Multilingual Tweets. We fine-tune some of the most popular
transformer models with the training dataset and synthetic data generated by
different data augmentation techniques. During the development phase, our best
results were obtained by using XLM-T. Data augmentation techniques provide a
very slight improvement in the results. Our system ranked in the 27th position
out of the 45 participating systems. Despite its modest results, our system
shows promising results in languages such as Portuguese, English, and Dutch.
All our code is available in the repository
\url{https://github.com/isegura/hulat_intimacy}.",2023,174,58.0,,http://arxiv.org/abs/2302.12794v1,High ROT
185,Explainable AI for Trustworthy Machine Learning - Study 83,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2025,57,57.0,synthetic_83,https://synthetic.paper/83,High ROT
366,Optimization Algorithms in Machine Learning - Study 264,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2024,113,56.5,synthetic_264,https://synthetic.paper/264,High ROT
33,EvoMerge: Neuroevolution for Large Language Models,"Extensive fine-tuning on Large Language Models does not always yield better
results. Oftentimes, models tend to get better at imitating one form of data
without gaining greater reasoning ability and may even end up losing some
intelligence. Here I introduce EvoMerge, a systematic approach to large
language model training and merging. Leveraging model merging for weight
crossover and fine-tuning for weight mutation, EvoMerge establishes an
evolutionary process aimed at pushing models beyond the limits of conventional
fine-tuning.",2024,111,55.5,,http://arxiv.org/abs/2402.00070v1,High ROT
332,Multi-Modal Learning with Neural Networks - Study 230,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2024,109,54.5,synthetic_230,https://synthetic.paper/230,High ROT
311,Adversarial Training for Robust Neural Networks - Study 209,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2023,163,54.333333333333336,synthetic_209,https://synthetic.paper/209,High ROT
297,Machine Learning Applications in Computer Vision - Study 195,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2024,108,54.0,synthetic_195,https://synthetic.paper/195,High ROT
170,Generative Adversarial Networks for Image Synthesis - Study 68,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2023,159,53.0,synthetic_68,https://synthetic.paper/68,High ROT
203,Graph Neural Networks for Social Network Analysis - Study 101,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2024,103,51.5,synthetic_101,https://synthetic.paper/101,High ROT
126,Machine Learning Applications in Computer Vision - Study 24,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2023,151,50.333333333333336,synthetic_24,https://synthetic.paper/24,High ROT
291,Meta-Learning Approaches for Few-Shot Learning - Study 189,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2025,50,50.0,synthetic_189,https://synthetic.paper/189,High ROT
43,Exponentially Faster Language Modelling,"Language models only really need to use an exponential fraction of their
neurons for individual inferences. As proof, we present UltraFastBERT, a BERT
variant that uses 0.3% of its neurons during inference while performing on par
with similar BERT models. UltraFastBERT selectively engages just 12 out of 4095
neurons for each layer inference. This is achieved by replacing feedforward
networks with fast feedforward networks (FFFs). While no truly efficient
implementation currently exists to unlock the full acceleration potential of
conditional neural execution, we provide high-level CPU code achieving 78x
speedup over the optimized baseline feedforward implementation, and a PyTorch
implementation delivering 40x speedup over the equivalent batched feedforward
inference. We publish our training code, benchmarking setup, and model weights.",2023,147,49.0,,http://arxiv.org/abs/2311.10770v2,High ROT
295,Adversarial Training for Robust Neural Networks - Study 193,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2023,147,49.0,synthetic_193,https://synthetic.paper/193,High ROT
246,Natural Language Processing with Transformer Models - Study 144,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2024,97,48.5,synthetic_144,https://synthetic.paper/144,High ROT
90,"Evolutionary Multi-Objective Optimization of Large Language Model
  Prompts for Balancing Sentiments","The advent of large language models (LLMs) such as ChatGPT has attracted
considerable attention in various domains due to their remarkable performance
and versatility. As the use of these models continues to grow, the importance
of effective prompt engineering has come to the fore. Prompt optimization
emerges as a crucial challenge, as it has a direct impact on model performance
and the extraction of relevant information. Recently, evolutionary algorithms
(EAs) have shown promise in addressing this issue, paving the way for novel
optimization strategies. In this work, we propose a evolutionary
multi-objective (EMO) approach specifically tailored for prompt optimization
called EMO-Prompts, using sentiment analysis as a case study. We use sentiment
analysis capabilities as our experimental targets. Our results demonstrate that
EMO-Prompts effectively generates prompts capable of guiding the LLM to produce
texts embodying two conflicting emotions simultaneously.",2024,95,47.5,,http://arxiv.org/abs/2401.09862v1,High ROT
269,Quantum Machine Learning Algorithms - Study 167,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2024,94,47.0,synthetic_167,https://synthetic.paper/167,High ROT
86,"MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label
  Framing Detection with Contrastive Learning","This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing
Detection. We used a multi-label contrastive loss for fine-tuning large
pre-trained language models in a multi-lingual setting, achieving very
competitive results: our system was ranked first on the official test set and
on the official shared task leaderboard for five of the six languages for which
we had training data and for which we could perform fine-tuning. Here, we
describe our experimental setup, as well as various ablation studies. The code
of our system is available at https://github.com/QishengL/SemEval2023",2023,135,45.0,,http://arxiv.org/abs/2304.14339v1,High ROT
205,Continual Learning in Neural Networks - Study 103,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2023,135,45.0,synthetic_103,https://synthetic.paper/103,High ROT
178,Knowledge Distillation in Deep Learning - Study 76,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2023,131,43.666666666666664,synthetic_76,https://synthetic.paper/76,High ROT
268,Explainable AI for Trustworthy Machine Learning - Study 166,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2024,86,43.0,synthetic_166,https://synthetic.paper/166,High ROT
132,Natural Language Processing with Transformer Models - Study 30,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2023,127,42.333333333333336,synthetic_30,https://synthetic.paper/30,High ROT
124,Machine Learning Applications in Computer Vision - Study 22,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2023,123,41.0,synthetic_22,https://synthetic.paper/22,High ROT
327,Machine Learning Applications in Computer Vision - Study 225,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2023,122,40.666666666666664,synthetic_225,https://synthetic.paper/225,High ROT
89,"Sub-network Discovery and Soft-masking for Continual Learning of Mixed
  Tasks","Continual learning (CL) has two main objectives: preventing catastrophic
forgetting (CF) and encouraging knowledge transfer (KT). The existing
literature mainly focused on overcoming CF. Some work has also been done on KT
when the tasks are similar. To our knowledge, only one method has been proposed
to learn a sequence of mixed tasks. However, these techniques still suffer from
CF and/or limited KT. This paper proposes a new CL method to achieve both. It
overcomes CF by isolating the knowledge of each task via discovering a
subnetwork for it. A soft-masking mechanism is also proposed to preserve the
previous knowledge and to enable the new task to leverage the past knowledge to
achieve KT. Experiments using classification, generation, information
extraction, and their mixture (i.e., heterogeneous tasks) show that the
proposed method consistently outperforms strong baselines.",2023,121,40.333333333333336,,http://arxiv.org/abs/2310.09436v1,High ROT
364,Multi-Modal Learning with Neural Networks - Study 262,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2023,118,39.333333333333336,synthetic_262,https://synthetic.paper/262,High ROT
150,Neural Architecture Search for Automated ML - Study 48,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2023,117,39.0,synthetic_48,https://synthetic.paper/48,High ROT
125,Self-Supervised Learning in Computer Vision - Study 23,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2023,117,39.0,synthetic_23,https://synthetic.paper/23,High ROT
9,"From Neural Activations to Concepts: A Survey on Explaining Concepts in
  Neural Networks","In this paper, we review recent approaches for explaining concepts in neural
networks. Concepts can act as a natural link between learning and reasoning:
once the concepts are identified that a neural learning system uses, one can
integrate those concepts with a reasoning system for inference or use a
reasoning system to act upon them to improve or enhance the learning system. On
the other hand, knowledge can not only be extracted from neural networks but
concept knowledge can also be inserted into neural network architectures. Since
integrating learning and reasoning is at the core of neuro-symbolic AI, the
insights gained from this survey can serve as an important step towards
realizing neuro-symbolic AI based on explainable concepts.",2023,115,38.333333333333336,,http://arxiv.org/abs/2310.11884v2,High ROT
153,Reinforcement Learning for Autonomous Systems - Study 51,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2023,111,37.0,synthetic_51,https://synthetic.paper/51,High ROT
8,Large Language Model Guided Tree-of-Thought,"In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel
approach aimed at improving the problem-solving capabilities of auto-regressive
large language models (LLMs). The ToT technique is inspired by the human mind's
approach for solving complex reasoning tasks through trial and error. In this
process, the human mind explores the solution space through a tree-like thought
process, allowing for backtracking when necessary. To implement ToT as a
software system, we augment an LLM with additional modules including a prompter
agent, a checker module, a memory module, and a ToT controller. In order to
solve a given problem, these modules engage in a multi-round conversation with
the LLM. The memory module records the conversation and state history of the
problem solving process, which allows the system to backtrack to the previous
steps of the thought-process and explore other directions from there. To verify
the effectiveness of the proposed technique, we implemented a ToT-based solver
for the Sudoku Puzzle. Experimental results show that the ToT framework can
significantly increase the success rate of Sudoku puzzle solving. Our
implementation of the ToT-based Sudoku solver is available on GitHub:
\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.",2023,109,36.333333333333336,,http://arxiv.org/abs/2305.08291v1,High ROT
348,Self-Supervised Learning in Computer Vision - Study 246,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2023,109,36.333333333333336,synthetic_246,https://synthetic.paper/246,High ROT
117,Self-Supervised Learning in Computer Vision - Study 15,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2023,108,36.0,synthetic_15,https://synthetic.paper/15,High ROT
53,"Beyond Scale: The Diversity Coefficient as a Data Quality Metric for
  Variability in Natural Language Data","Current trends in pre-training Large Language Models (LLMs) primarily focus
on the scaling of model and dataset size. While the quality of pre-training
data is considered an important factor for training powerful LLMs, it remains a
nebulous concept that has not been rigorously characterized. To this end, we
propose a formalization of one key aspect of data quality -- measuring the
variability of natural language data -- specifically via a measure we call the
diversity coefficient. Our empirical analysis shows that the proposed diversity
coefficient aligns with the intuitive properties of diversity and variability,
e.g., it increases as the number of latent concepts increases. Then, we measure
the diversity coefficient of publicly available pre-training datasets and
demonstrate that their formal diversity is high compared to theoretical lower
and upper bounds. Finally, we conduct a comprehensive set of controlled
interventional experiments with GPT-2 and LLaMAv2 that demonstrate the
diversity coefficient of pre-training data characterizes useful aspects of
downstream model evaluation performance -- totaling 44 models of various sizes
(51M to 7B parameters). We conclude that our formal notion of diversity is an
important aspect of data quality that captures variability and causally leads
to improved evaluation performance.",2023,107,35.666666666666664,,http://arxiv.org/abs/2306.13840v4,High ROT
87,"Enriching language models with graph-based context information to better
  understand textual data","A considerable number of texts encountered daily are somehow connected with
each other. For example, Wikipedia articles refer to other articles via
hyperlinks, scientific papers relate to others via citations or (co)authors,
while tweets relate via users that follow each other or reshare content. Hence,
a graph-like structure can represent existing connections and be seen as
capturing the ""context"" of the texts. The question thus arises if extracting
and integrating such context information into a language model might help
facilitate a better automated understanding of the text. In this study, we
experimentally demonstrate that incorporating graph-based contextualization
into BERT model enhances its performance on an example of a classification
task. Specifically, on Pubmed dataset, we observed a reduction in error from
8.51% to 7.96%, while increasing the number of parameters just by 1.6%.
  Our source code: https://github.com/tryptofanik/gc-bert",2023,106,35.333333333333336,,http://arxiv.org/abs/2305.11070v1,High ROT
12,Symbolic Discovery of Optimization Algorithms,"We present a method to formulate algorithm discovery as program search, and
apply it to discover optimization algorithms for deep neural network training.
We leverage efficient search techniques to explore an infinite and sparse
program space. To bridge the large generalization gap between proxy and target
tasks, we also introduce program selection and simplification strategies. Our
method discovers a simple and effective optimization algorithm, $\textbf{Lion}$
($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$).
It is more memory-efficient than Adam as it only keeps track of the momentum.
Different from adaptive optimizers, its update has the same magnitude for each
parameter calculated through the sign operation. We compare Lion with widely
used optimizers, such as Adam and Adafactor, for training a variety of models
on different tasks. On image classification, Lion boosts the accuracy of ViT by
up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On
vision-language contrastive learning, we achieve 88.3% $\textit{zero-shot}$ and
91.1% $\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best
results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms
Adam by achieving a better FID score and reducing the training compute by up to
2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion
exhibits a similar or better performance compared to Adam. Our analysis of Lion
reveals that its performance gain grows with the training batch size. It also
requires a smaller learning rate than Adam due to the larger norm of the update
produced by the sign function. Additionally, we examine the limitations of Lion
and identify scenarios where its improvements are small or not statistically
significant. Lion is also successfully deployed in production systems such as
Google search ads CTR model.",2023,104,34.666666666666664,,http://arxiv.org/abs/2302.06675v4,High ROT
146,Deep Learning for Medical Image Analysis - Study 44,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2023,104,34.666666666666664,synthetic_44,https://synthetic.paper/44,High ROT
174,Explainable AI for Trustworthy Machine Learning - Study 72,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2023,104,34.666666666666664,synthetic_72,https://synthetic.paper/72,High ROT
83,Adapting a Language Model While Preserving its General Knowledge,"Domain-adaptive pre-training (or DA-training for short), also known as
post-training, aims to train a pre-trained general-purpose language model (LM)
using an unlabeled corpus of a particular domain to adapt the LM so that
end-tasks in the domain can give improved performances. However, existing
DA-training methods are in some sense blind as they do not explicitly identify
what knowledge in the LM should be preserved and what should be changed by the
domain corpus. This paper shows that the existing methods are suboptimal and
proposes a novel method to perform a more informed adaptation of the knowledge
in the LM by (1) soft-masking the attention heads based on their importance to
best preserve the general knowledge in the LM and (2) contrasting the
representations of the general and the full (both general and domain knowledge)
to learn an integrated representation with both general and domain-specific
knowledge. Experimental results will demonstrate the effectiveness of the
proposed approach.",2023,100,33.333333333333336,,http://arxiv.org/abs/2301.08986v1,High ROT
172,Attention Mechanisms in Neural Networks - Study 70,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2024,66,33.0,synthetic_70,https://synthetic.paper/70,High ROT
398,Explainable AI for Trustworthy Machine Learning - Study 296,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2023,99,33.0,synthetic_296,https://synthetic.paper/296,High ROT
135,Machine Learning Applications in Computer Vision - Study 33,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2023,97,32.333333333333336,synthetic_33,https://synthetic.paper/33,High ROT
111,Neural Architecture Search for Automated ML - Study 9,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2024,63,31.5,synthetic_9,https://synthetic.paper/9,High ROT
232,Adversarial Training for Robust Neural Networks - Study 130,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2024,63,31.5,synthetic_130,https://synthetic.paper/130,High ROT
91,"Leave No Context Behind: Efficient Infinite Context Transformers with
  Infini-attention","This work introduces an efficient method to scale Transformer-based Large
Language Models (LLMs) to infinitely long inputs with bounded memory and
computation. A key component in our proposed approach is a new attention
technique dubbed Infini-attention. The Infini-attention incorporates a
compressive memory into the vanilla attention mechanism and builds in both
masked local attention and long-term linear attention mechanisms in a single
Transformer block. We demonstrate the effectiveness of our approach on
long-context language modeling benchmarks, 1M sequence length passkey context
block retrieval and 500K length book summarization tasks with 1B and 8B LLMs.
Our approach introduces minimal bounded memory parameters and enables fast
streaming inference for LLMs.",2024,61,30.5,,http://arxiv.org/abs/2404.07143v2,High ROT
338,Natural Language Processing with Transformer Models - Study 236,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2023,91,30.333333333333332,synthetic_236,https://synthetic.paper/236,High ROT
258,Explainable AI for Trustworthy Machine Learning - Study 156,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2023,85,28.333333333333332,synthetic_156,https://synthetic.paper/156,High ROT
206,Explainable AI for Trustworthy Machine Learning - Study 104,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2023,82,27.333333333333332,synthetic_104,https://synthetic.paper/104,High ROT
138,Graph Neural Networks for Social Network Analysis - Study 36,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2023,80,26.666666666666668,synthetic_36,https://synthetic.paper/36,High ROT
10,Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI,"Large language models have proliferated across multiple domains in as short
period of time. There is however hesitation in the medical and healthcare
domain towards their adoption because of issues like factuality, coherence, and
hallucinations. Give the high stakes nature of healthcare, many researchers
have even cautioned against its usage until these issues are resolved. The key
to the implementation and deployment of LLMs in healthcare is to make these
models trustworthy, transparent (as much possible) and explainable. In this
paper we describe the key elements in creating reliable, trustworthy, and
unbiased models as a necessary condition for their adoption in healthcare.
Specifically we focus on the quantification, validation, and mitigation of
hallucinations in the context in healthcare. Lastly, we discuss how the future
of LLMs in healthcare may look like.",2023,79,26.333333333333332,,http://arxiv.org/abs/2311.01463v1,High ROT
139,Reinforcement Learning for Autonomous Systems - Study 37,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2023,78,26.0,synthetic_37,https://synthetic.paper/37,High ROT
177,Advanced Neural Network Architectures for Deep Learning - Study 75,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2023,72,24.0,synthetic_75,https://synthetic.paper/75,High ROT
85,"ACO-tagger: A Novel Method for Part-of-Speech Tagging using Ant Colony
  Optimization","Swarm Intelligence algorithms have gained significant attention in recent
years as a means of solving complex and non-deterministic problems. These
algorithms are inspired by the collective behavior of natural creatures, and
they simulate this behavior to develop intelligent agents for computational
tasks. One such algorithm is Ant Colony Optimization (ACO), which is inspired
by the foraging behavior of ants and their pheromone laying mechanism. ACO is
used for solving difficult problems that are discrete and combinatorial in
nature. Part-of-Speech (POS) tagging is a fundamental task in natural language
processing that aims to assign a part-of-speech role to each word in a
sentence. In this research paper, proposed a high-performance POS-tagging
method based on ACO called ACO-tagger. This method achieved a high accuracy
rate of 96.867%, outperforming several state-of-the-art methods. The proposed
method is fast and efficient, making it a viable option for practical
applications.",2023,71,23.666666666666668,,http://arxiv.org/abs/2303.16760v1,High ROT
15,"STL: A Signed and Truncated Logarithm Activation Function for Neural
  Networks","Activation functions play an essential role in neural networks. They provide
the non-linearity for the networks. Therefore, their properties are important
for neural networks' accuracy and running performance. In this paper, we
present a novel signed and truncated logarithm function as activation function.
The proposed activation function has significantly better mathematical
properties, such as being odd function, monotone, differentiable, having
unbounded value range, and a continuous nonzero gradient. These properties make
it an excellent choice as an activation function. We compare it with other
well-known activation functions in several well-known neural networks. The
results confirm that it is the state-of-the-art. The suggested activation
function can be applied in a large range of neural networks where activation
functions are necessary.",2023,65,21.666666666666668,,http://arxiv.org/abs/2307.16389v1,High ROT
270,Explainable AI for Trustworthy Machine Learning - Study 168,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2022,79,19.75,synthetic_168,https://synthetic.paper/168,High ROT
11,Liquid Structural State-Space Models,"A proper parametrization of state transition matrices of linear state-space
models (SSMs) followed by standard nonlinearities enables them to efficiently
learn representations from sequential data, establishing the state-of-the-art
on a large series of long-range sequence modeling benchmarks. In this paper, we
show that we can improve further when the structural SSM such as S4 is given by
a linear liquid time-constant (LTC) state-space model. LTC neural networks are
causal continuous-time neural networks with an input-dependent state transition
module, which makes them learn to adapt to incoming inputs at inference. We
show that by using a diagonal plus low-rank decomposition of the state
transition matrix introduced in S4, and a few simplifications, the LTC-based
structural state-space model, dubbed Liquid-S4, achieves the new
state-of-the-art generalization across sequence modeling tasks with long-term
dependencies such as image, text, audio, and medical time-series, with an
average performance of 87.32% on the Long-Range Arena benchmark. On the full
raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with
a 30% reduction in parameter counts compared to S4. The additional gain in
performance is the direct result of the Liquid-S4's kernel structure that takes
into account the similarities of the input sequence samples during training and
inference.",2022,78,19.5,,http://arxiv.org/abs/2209.12951v1,High ROT
393,Attention Mechanisms in Neural Networks - Study 291,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2023,58,19.333333333333332,synthetic_291,https://synthetic.paper/291,High ROT
395,Adversarial Training for Robust Neural Networks - Study 293,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2022,77,19.25,synthetic_293,https://synthetic.paper/293,High ROT
254,Quantum Machine Learning Algorithms - Study 152,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2024,38,19.0,synthetic_152,https://synthetic.paper/152,High ROT
315,Reinforcement Learning for Autonomous Systems - Study 213,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2023,56,18.666666666666668,synthetic_213,https://synthetic.paper/213,High ROT
82,"ClassActionPrediction: A Challenging Benchmark for Legal Judgment
  Prediction of Class Action Cases in the US","The research field of Legal Natural Language Processing (NLP) has been very
active recently, with Legal Judgment Prediction (LJP) becoming one of the most
extensively studied tasks. To date, most publicly released LJP datasets
originate from countries with civil law. In this work, we release, for the
first time, a challenging LJP dataset focused on class action cases in the US.
It is the first dataset in the common law system that focuses on the harder and
more realistic task involving the complaints as input instead of the often used
facts summary written by the court. Additionally, we study the difficulty of
the task by collecting expert human predictions, showing that even human
experts can only reach 53% accuracy on this dataset. Our Longformer model
clearly outperforms the human baseline (63%), despite only considering the
first 2,048 tokens. Furthermore, we perform a detailed error analysis and find
that the Longformer model is significantly better calibrated than the human
experts. Finally, we publicly release the dataset and the code used for the
experiments.",2022,73,18.25,,http://arxiv.org/abs/2211.00582v1,High ROT
385,Quantum Machine Learning Algorithms - Study 283,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2022,70,17.5,synthetic_283,https://synthetic.paper/283,High ROT
216,Self-Supervised Learning in Computer Vision - Study 114,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2022,66,16.5,synthetic_114,https://synthetic.paper/114,High ROT
3,"Show me your NFT and I tell you how it will perform: Multimodal
  representation learning for NFT selling price prediction","Non-Fungible Tokens (NFTs) represent deeds of ownership, based on blockchain
technologies and smart contracts, of unique crypto assets on digital art forms
(e.g., artworks or collectibles). In the spotlight after skyrocketing in 2021,
NFTs have attracted the attention of crypto enthusiasts and investors intent on
placing promising investments in this profitable market. However, the NFT
financial performance prediction has not been widely explored to date.
  In this work, we address the above problem based on the hypothesis that NFT
images and their textual descriptions are essential proxies to predict the NFT
selling prices. To this purpose, we propose MERLIN, a novel multimodal deep
learning framework designed to train Transformer-based language and visual
models, along with graph neural network models, on collections of NFTs' images
and texts. A key aspect in MERLIN is its independence on financial features, as
it exploits only the primary data a user interested in NFT trading would like
to deal with, i.e., NFT images and textual descriptions. By learning dense
representations of such data, a price-category classification task is performed
by MERLIN models, which can also be tuned according to user preferences in the
inference phase to mimic different risk-return investment profiles.
Experimental evaluation on a publicly available dataset has shown that MERLIN
models achieve significant performances according to several financial
assessment criteria, fostering profitable investments, and also beating
baseline machine-learning classifiers based on financial features.",2023,49,16.333333333333332,,http://arxiv.org/abs/2302.01676v2,High ROT
208,Neural Architecture Search for Automated ML - Study 106,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2022,65,16.25,synthetic_106,https://synthetic.paper/106,High ROT
179,Explainable AI for Trustworthy Machine Learning - Study 77,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2022,64,16.0,synthetic_77,https://synthetic.paper/77,High ROT
337,Self-Supervised Learning in Computer Vision - Study 235,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2022,63,15.75,synthetic_235,https://synthetic.paper/235,High ROT
354,Federated Learning for Privacy-Preserving AI - Study 252,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2021,77,15.4,synthetic_252,https://synthetic.paper/252,High ROT
277,Natural Language Processing with Transformer Models - Study 175,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2022,60,15.0,synthetic_175,https://synthetic.paper/175,High ROT
289,Natural Language Processing with Transformer Models - Study 187,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2021,74,14.8,synthetic_187,https://synthetic.paper/187,High ROT
35,"TextConvoNet:A Convolutional Neural Network based Architecture for Text
  Classification","In recent years, deep learning-based models have significantly improved the
Natural Language Processing (NLP) tasks. Specifically, the Convolutional Neural
Network (CNN), initially used for computer vision, has shown remarkable
performance for text data in various NLP problems. Most of the existing
CNN-based models use 1-dimensional convolving filters n-gram detectors), where
each filter specialises in extracting n-grams features of a particular input
word embedding. The input word embeddings, also called sentence matrix, is
treated as a matrix where each row is a word vector. Thus, it allows the model
to apply one-dimensional convolution and only extract n-gram based features
from a sentence matrix. These features can be termed as intra-sentence n-gram
features. To the extent of our knowledge, all the existing CNN models are based
on the aforementioned concept. In this paper, we present a CNN-based
architecture TextConvoNet that not only extracts the intra-sentence n-gram
features but also captures the inter-sentence n-gram features in input text
data. It uses an alternative approach for input matrix representation and
applies a two-dimensional multi-scale convolutional operation on the input. To
evaluate the performance of TextConvoNet, we perform an experimental study on
five text classification datasets. The results are evaluated by using various
performance metrics. The experimental results show that the presented
TextConvoNet outperforms state-of-the-art machine learning and deep learning
models for text classification purposes.",2022,59,14.75,,http://arxiv.org/abs/2203.05173v1,High ROT
317,Graph Neural Networks for Social Network Analysis - Study 215,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2022,57,14.25,synthetic_215,https://synthetic.paper/215,High ROT
68,"Integration of Text and Graph-based Features for Detecting Mental Health
  Disorders from Voice","With the availability of voice-enabled devices such as smart phones, mental
health disorders could be detected and treated earlier, particularly
post-pandemic. The current methods involve extracting features directly from
audio signals. In this paper, two methods are used to enrich voice analysis for
depression detection: graph transformation of voice signals, and natural
language processing of the transcript based on representational learning, fused
together to produce final class labels. The results of experiments with the
DAIC-WOZ dataset suggest that integration of text-based voice classification
and learning from low level and graph-based voice signal features can improve
the detection of mental disorders like depression.",2022,57,14.25,,http://arxiv.org/abs/2205.07006v1,High ROT
81,Continual Training of Language Models for Few-Shot Learning,"Recent work on applying large language models (LMs) achieves impressive
performance in many NLP applications. Adapting or posttraining an LM using an
unlabeled domain corpus can produce even better performance for end-tasks in
the domain. This paper proposes the problem of continually extending an LM by
incrementally post-train the LM with a sequence of unlabeled domain corpora to
expand its knowledge without forgetting its previous skills. The goal is to
improve the few-shot end-task learning in these domains. The resulting system
is called CPT (Continual PostTraining), which to our knowledge, is the first
continual post-training system. Experimental results verify its effectiveness.",2022,57,14.25,,http://arxiv.org/abs/2210.05549v1,High ROT
313,Quantum Machine Learning Algorithms - Study 211,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2021,71,14.2,synthetic_211,https://synthetic.paper/211,High ROT
66,Continual Learning with Knowledge Transfer for Sentiment Classification,"This paper studies continual learning (CL) for sentiment classification (SC).
In this setting, the CL system learns a sequence of SC tasks incrementally in a
neural network, where each task builds a classifier to classify the sentiment
of reviews of a particular product category or domain. Two natural questions
are: Can the system transfer the knowledge learned in the past from the
previous tasks to the new task to help it learn a better model for the new
task? And, can old models for previous tasks be improved in the process as
well? This paper proposes a novel technique called KAN to achieve these
objectives. KAN can markedly improve the SC accuracy of both the new task and
the old tasks via forward and backward knowledge transfer. The effectiveness of
KAN is demonstrated through extensive experiments.",2021,70,14.0,,http://arxiv.org/abs/2112.10021v1,High ROT
156,Federated Learning for Privacy-Preserving AI - Study 54,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2021,70,14.0,synthetic_54,https://synthetic.paper/54,High ROT
324,Neural Architecture Search for Automated ML - Study 222,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2022,56,14.0,synthetic_222,https://synthetic.paper/222,High ROT
62,"Contextual Sentence Classification: Detecting Sustainability Initiatives
  in Company Reports","We introduce the novel task of detecting sustainability initiatives in
company reports. Given a full report, the aim is to automatically identify
mentions of practical activities that a company has performed in order to
tackle specific societal issues. New methods for identifying continuous
sentence spans need to be developed for capturing the multi-sentence structure
of individual sustainability initiatives. We release a new dataset of company
reports in which the text has been manually annotated with sustainability
initiatives. We also evaluate different models for initiative detection,
introducing a novel aggregation and evaluation methodology. Our proposed
architecture uses sequences of consecutive sentences to account for contextual
information when making classification decisions at the individual sentence
level.",2021,68,13.6,,http://arxiv.org/abs/2110.03727v2,High ROT
330,Natural Language Processing with Transformer Models - Study 228,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2022,54,13.5,synthetic_228,https://synthetic.paper/228,High ROT
184,Machine Learning Applications in Computer Vision - Study 82,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2022,54,13.5,synthetic_82,https://synthetic.paper/82,High ROT
342,Federated Learning for Privacy-Preserving AI - Study 240,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2022,53,13.25,synthetic_240,https://synthetic.paper/240,High ROT
61,A Survey On Neural Word Embeddings,"Understanding human language has been a sub-challenge on the way of
intelligent machines. The study of meaning in natural language processing (NLP)
relies on the distributional hypothesis where language elements get meaning
from the words that co-occur within contexts. The revolutionary idea of
distributed representation for a concept is close to the working of a human
mind in that the meaning of a word is spread across several neurons, and a loss
of activation will only slightly affect the memory retrieval process.
  Neural word embeddings transformed the whole field of NLP by introducing
substantial improvements in all NLP tasks. In this survey, we provide a
comprehensive literature review on neural word embeddings. We give theoretical
foundations and describe existing work by an interplay between word embeddings
and language modelling. We provide broad coverage on neural word embeddings,
including early word embeddings, embeddings targeting specific semantic
relations, sense embeddings, morpheme embeddings, and finally, contextual
representations. Finally, we describe benchmark datasets in word embeddings'
performance evaluation and downstream tasks along with the performance results
of/due to word embeddings.",2021,65,13.0,,http://arxiv.org/abs/2110.01804v1,High ROT
120,Adversarial Training for Robust Neural Networks - Study 18,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2021,64,12.8,synthetic_18,https://synthetic.paper/18,High ROT
144,Neural Architecture Search for Automated ML - Study 42,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2022,50,12.5,synthetic_42,https://synthetic.paper/42,High ROT
112,Deep Learning for Medical Image Analysis - Study 10,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2020,75,12.5,synthetic_10,https://synthetic.paper/10,High ROT
229,Explainable AI for Trustworthy Machine Learning - Study 127,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2022,49,12.25,synthetic_127,https://synthetic.paper/127,High ROT
380,Federated Learning for Privacy-Preserving AI - Study 278,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2021,61,12.2,synthetic_278,https://synthetic.paper/278,High ROT
7,"hyper-sinh: An Accurate and Reliable Function from Shallow to Deep
  Learning in TensorFlow and Keras","This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation
function suitable for Deep Learning (DL)-based algorithms for supervised
learning, such as Convolutional Neural Networks (CNN). hyper-sinh, developed in
the open source Python libraries TensorFlow and Keras, is thus described and
validated as an accurate and reliable activation function for both shallow and
deep neural networks. Improvements in accuracy and reliability in image and
text classification tasks on five (N = 5) benchmark data sets available from
Keras are discussed. Experimental results demonstrate the overall competitive
classification performance of both shallow and deep neural networks, obtained
via this novel function. This function is evaluated with respect to gold
standard activation functions, demonstrating its overall competitive accuracy
and reliability for both image and text classification.",2020,73,12.166666666666666,,http://arxiv.org/abs/2011.07661v1,High ROT
242,Reinforcement Learning for Autonomous Systems - Study 140,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2021,60,12.0,synthetic_140,https://synthetic.paper/140,High ROT
356,Graph Neural Networks for Social Network Analysis - Study 254,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2022,48,12.0,synthetic_254,https://synthetic.paper/254,High ROT
308,Continual Learning in Neural Networks - Study 206,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2020,71,11.833333333333334,synthetic_206,https://synthetic.paper/206,High ROT
41,Robust Document Representations using Latent Topics and Metadata,"Task specific fine-tuning of a pre-trained neural language model using a
custom softmax output layer is the de facto approach of late when dealing with
document classification problems. This technique is not adequate when labeled
examples are not available at training time and when the metadata artifacts in
a document must be exploited. We address these challenges by generating
document representations that capture both text and metadata artifacts in a
task agnostic manner. Instead of traditional auto-regressive or auto-encoding
based training, our novel self-supervised approach learns a soft-partition of
the input space when generating text embeddings. Specifically, we employ a
pre-learned topic model distribution as surrogate labels and construct a loss
function based on KL divergence. Our solution also incorporates metadata
explicitly rather than just augmenting them with text. The generated document
embeddings exhibit compositional characteristics and are directly used by
downstream classification tasks to create decision boundaries from a small
number of labeled examples, thereby eschewing complicated recognition methods.
We demonstrate through extensive evaluation that our proposed cross-model
fusion solution outperforms several competitive baselines on multiple datasets.",2020,71,11.833333333333334,,http://arxiv.org/abs/2010.12681v1,High ROT
365,Reinforcement Learning for Autonomous Systems - Study 263,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2020,70,11.666666666666666,synthetic_263,https://synthetic.paper/263,High ROT
287,Self-Supervised Learning in Computer Vision - Study 185,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2020,70,11.666666666666666,synthetic_185,https://synthetic.paper/185,High ROT
158,Explainable AI for Trustworthy Machine Learning - Study 56,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2020,68,11.333333333333334,synthetic_56,https://synthetic.paper/56,High ROT
225,Machine Learning Applications in Computer Vision - Study 123,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2020,68,11.333333333333334,synthetic_123,https://synthetic.paper/123,High ROT
388,Generative Adversarial Networks for Image Synthesis - Study 286,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2020,68,11.333333333333334,synthetic_286,https://synthetic.paper/286,High ROT
65,"Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment
  Classification Tasks","This paper studies continual learning (CL) of a sequence of aspect sentiment
classification (ASC) tasks. Although some CL techniques have been proposed for
document sentiment classification, we are not aware of any CL work on ASC. A CL
system that incrementally learns a sequence of ASC tasks should address the
following two issues: (1) transfer knowledge learned from previous tasks to the
new task to help it learn a better model, and (2) maintain the performance of
the models for previous tasks so that they are not forgotten. This paper
proposes a novel capsule network based model called B-CL to address these
issues. B-CL markedly improves the ASC performance on both the new task and the
old tasks via forward and backward knowledge transfer. The effectiveness of
B-CL is demonstrated through extensive experiments.",2021,56,11.2,,http://arxiv.org/abs/2112.03271v1,High ROT
211,Explainable AI for Trustworthy Machine Learning - Study 109,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2020,66,11.0,synthetic_109,https://synthetic.paper/109,High ROT
243,Quantum Machine Learning Algorithms - Study 141,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2021,55,11.0,synthetic_141,https://synthetic.paper/141,High ROT
349,Machine Learning Applications in Computer Vision - Study 247,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2020,65,10.833333333333334,synthetic_247,https://synthetic.paper/247,High ROT
78,Template Controllable keywords-to-text Generation,"This paper proposes a novel neural model for the understudied task of
generating text from keywords. The model takes as input a set of un-ordered
keywords, and part-of-speech (POS) based template instructions. This makes it
ideal for surface realization in any NLG setup. The framework is based on the
encode-attend-decode paradigm, where keywords and templates are encoded first,
and the decoder judiciously attends over the contexts derived from the encoded
keywords and templates to generate the sentences. Training exploits weak
supervision, as the model trains on a large amount of labeled data with
keywords and POS based templates prepared through completely automatic means.
Qualitative and quantitative performance analyses on publicly available
test-data in various domains reveal our system's superiority over baselines,
built using state-of-the-art neural machine translation and controllable
transfer techniques. Our approach is indifferent to the order of input
keywords.",2020,65,10.833333333333334,,http://arxiv.org/abs/2011.03722v1,High ROT
187,Quantum Machine Learning Algorithms - Study 85,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2021,53,10.6,synthetic_85,https://synthetic.paper/85,High ROT
387,Continual Learning in Neural Networks - Study 285,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2020,63,10.5,synthetic_285,https://synthetic.paper/285,High ROT
231,Natural Language Processing with Transformer Models - Study 129,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2020,63,10.5,synthetic_129,https://synthetic.paper/129,High ROT
80,"Adaptive Perturbation-Based Gradient Estimation for Discrete Latent
  Variable Models","The integration of discrete algorithmic components in deep learning
architectures has numerous applications. Recently, Implicit Maximum Likelihood
Estimation (IMLE, Niepert, Minervini, and Franceschi 2021), a class of gradient
estimators for discrete exponential family distributions, was proposed by
combining implicit differentiation through perturbation with the path-wise
gradient estimator. However, due to the finite difference approximation of the
gradients, it is especially sensitive to the choice of the finite difference
step size, which needs to be specified by the user. In this work, we present
Adaptive IMLE (AIMLE), the first adaptive gradient estimator for complex
discrete distributions: it adaptively identifies the target distribution for
IMLE by trading off the density of gradient information with the degree of bias
in the gradient estimates. We empirically evaluate our estimator on synthetic
examples, as well as on Learning to Explain, Discrete Variational
Auto-Encoders, and Neural Relational Inference tasks. In our experiments, we
show that our adaptive gradient estimator can produce faithful estimates while
requiring orders of magnitude fewer samples than other gradient estimators.",2022,41,10.25,,http://arxiv.org/abs/2209.04862v2,High ROT
71,"Latent Topology Induction for Understanding Contextualized
  Representations","In this work, we study the representation space of contextualized embeddings
and gain insight into the hidden topology of large language models. We show
there exists a network of latent states that summarize linguistic properties of
contextualized representations. Instead of seeking alignments to existing
well-defined annotations, we infer this latent network in a fully unsupervised
way using a structured variational autoencoder. The induced states not only
serve as anchors that mark the topology (neighbors and connectivity) of the
representation manifold but also reveal the internal mechanism of encoding
sentences. With the induced network, we: (1). decompose the representation
space into a spectrum of latent states which encode fine-grained word meanings
with lexical, morphological, syntactic and semantic information; (2). show
state-state transitions encode rich phrase constructions and serve as the
backbones of the latent space. Putting the two together, we show that sentences
are represented as a traversal over the latent network where state-state
transition chains encode syntactic templates and state-word emissions fill in
the content. We demonstrate these insights with extensive experiments and
visualizations.",2022,41,10.25,,http://arxiv.org/abs/2206.01512v1,High ROT
223,Self-Supervised Learning in Computer Vision - Study 121,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2020,61,10.166666666666666,synthetic_121,https://synthetic.paper/121,High ROT
363,Knowledge Distillation in Deep Learning - Study 261,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2019,70,10.0,synthetic_261,https://synthetic.paper/261,High ROT
306,Advanced Neural Network Architectures for Deep Learning - Study 204,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2021,50,10.0,synthetic_204,https://synthetic.paper/204,High ROT
151,Machine Learning Applications in Computer Vision - Study 49,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2020,59,9.833333333333334,synthetic_49,https://synthetic.paper/49,High ROT
169,Meta-Learning Approaches for Few-Shot Learning - Study 67,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2022,39,9.75,synthetic_67,https://synthetic.paper/67,High ROT
171,Federated Learning for Privacy-Preserving AI - Study 69,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2019,68,9.714285714285714,synthetic_69,https://synthetic.paper/69,High ROT
77,"Deep Reinforcement Learning for Chatbots Using Clustered Actions and
  Human-Likeness Rewards","Training chatbots using the reinforcement learning paradigm is challenging
due to high-dimensional states, infinite action spaces and the difficulty in
specifying the reward function. We address such problems using clustered
actions instead of infinite actions, and a simple but promising reward function
based on human-likeness scores derived from human-human dialogue data. We train
Deep Reinforcement Learning (DRL) agents using chitchat data in raw
text---without any manual annotations. Experimental results using different
splits of training data report the following. First, that our agents learn
reasonable policies in the environments they get familiarised with, but their
performance drops substantially when they are exposed to a test set of unseen
dialogues. Second, that the choice of sentence embedding size between 100 and
300 dimensions is not significantly different on test data. Third, that our
proposed human-likeness rewards are reasonable for training chatbots as long as
they use lengthy dialogue histories of >=10 sentences.",2019,67,9.571428571428571,,http://arxiv.org/abs/1908.10331v1,High ROT
186,Neural Architecture Search for Automated ML - Study 84,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2022,38,9.5,synthetic_84,https://synthetic.paper/84,High ROT
69,"Mind Your Inflections! Improving NLP for Non-Standard Englishes with
  Base-Inflection Encoding","Inflectional variation is a common feature of World Englishes such as
Colloquial Singapore English and African American Vernacular English. Although
comprehension by human readers is usually unimpaired by non-standard
inflections, current NLP systems are not yet robust. We propose Base-Inflection
Encoding (BITE), a method to tokenize English text by reducing inflected words
to their base forms before reinjecting the grammatical information as special
symbols. Fine-tuning pretrained NLP models for downstream tasks using our
encoding defends against inflectional adversaries while maintaining performance
on clean data. Models using BITE generalize better to dialects with
non-standard inflections without explicit training and translation models
converge faster when trained with BITE. Finally, we show that our encoding
improves the vocabulary efficiency of popular data-driven subword tokenizers.
Since there has been no prior work on quantitatively evaluating vocabulary
efficiency, we propose metrics to do so.",2020,57,9.5,,http://arxiv.org/abs/2004.14870v4,High ROT
361,Meta-Learning Approaches for Few-Shot Learning - Study 259,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2019,66,9.428571428571429,synthetic_259,https://synthetic.paper/259,High ROT
253,Optimization Algorithms in Machine Learning - Study 151,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2021,46,9.2,synthetic_151,https://synthetic.paper/151,High ROT
118,Multi-Modal Learning with Neural Networks - Study 16,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2022,36,9.0,synthetic_16,https://synthetic.paper/16,High ROT
298,Quantum Machine Learning Algorithms - Study 196,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2019,62,8.857142857142858,synthetic_196,https://synthetic.paper/196,High ROT
200,Attention Mechanisms in Neural Networks - Study 98,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2020,53,8.833333333333334,synthetic_98,https://synthetic.paper/98,High ROT
14,QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks,"The advent of noisy intermediate-scale quantum (NISQ) computers raises a
crucial challenge to design quantum neural networks for fully quantum learning
tasks. To bridge the gap, this work proposes an end-to-end learning framework
named QTN-VQC, by introducing a trainable quantum tensor network (QTN) for
quantum embedding on a variational quantum circuit (VQC). The architecture of
QTN is composed of a parametric tensor-train network for feature extraction and
a tensor product encoding for quantum embedding. We highlight the QTN for
quantum embedding in terms of two perspectives: (1) we theoretically
characterize QTN by analyzing its representation power of input features; (2)
QTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the
generation of quantum embedding to the output measurement. Our experiments on
the MNIST dataset demonstrate the advantages of QTN for quantum embedding over
other quantum embedding approaches.",2021,44,8.8,,http://arxiv.org/abs/2110.03861v3,High ROT
214,Federated Learning for Privacy-Preserving AI - Study 112,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2021,44,8.8,synthetic_112,https://synthetic.paper/112,High ROT
183,Attention Mechanisms in Neural Networks - Study 81,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2022,35,8.75,synthetic_81,https://synthetic.paper/81,High ROT
273,Attention Mechanisms in Neural Networks - Study 171,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2020,51,8.5,synthetic_171,https://synthetic.paper/171,High ROT
335,Adversarial Training for Robust Neural Networks - Study 233,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2019,59,8.428571428571429,synthetic_233,https://synthetic.paper/233,High ROT
316,Neural Network Compression Techniques - Study 214,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2021,42,8.4,synthetic_214,https://synthetic.paper/214,High ROT
228,Continual Learning in Neural Networks - Study 126,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2018,67,8.375,synthetic_126,https://synthetic.paper/126,High ROT
235,Optimization Algorithms in Machine Learning - Study 133,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2018,65,8.125,synthetic_133,https://synthetic.paper/133,High ROT
201,Generative Adversarial Networks for Image Synthesis - Study 99,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2020,48,8.0,synthetic_99,https://synthetic.paper/99,High ROT
26,Feature Weight Tuning for Recursive Neural Networks,"This paper addresses how a recursive neural network model can automatically
leave out useless information and emphasize important evidence, in other words,
to perform ""weight tuning"" for higher-level representation acquisition. We
propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural
Network (BENN), which automatically control how much one specific unit
contributes to the higher-level representation. The proposed model can be
viewed as incorporating a more powerful compositional function for embedding
acquisition in recursive neural networks. Experimental results demonstrate the
significant improvement over standard neural models.",2014,15,1.25,,http://arxiv.org/abs/1412.3714v2,Low ROT
322,Advanced Neural Network Architectures for Deep Learning - Study 220,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2015,17,1.5454545454545454,synthetic_220,https://synthetic.paper/220,Low ROT
72,"A Hierarchical Latent Variable Encoder-Decoder Model for Generating
  Dialogues","Sequential data often possesses a hierarchical structure with complex
dependencies between subsequences, such as found between the utterances in a
dialogue. In an effort to model this kind of generative process, we propose a
neural network-based generative architecture, with latent stochastic variables
that span a variable number of time steps. We apply the proposed model to the
task of dialogue response generation and compare it with recent neural network
architectures. We evaluate the model performance through automatic evaluation
metrics and by carrying out a human evaluation. The experiments demonstrate
that our model improves upon recently proposed models and that the latent
variables facilitate the generation of long outputs and maintain the context.",2016,16,1.6,,http://arxiv.org/abs/1605.06069v3,Low ROT
202,Quantum Machine Learning Algorithms - Study 100,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2016,16,1.6,synthetic_100,https://synthetic.paper/100,Low ROT
220,Deep Learning for Medical Image Analysis - Study 118,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2015,18,1.6363636363636365,synthetic_118,https://synthetic.paper/118,Low ROT
49,Towards Neural Network-based Reasoning,"We propose Neural Reasoner, a framework for neural network-based reasoning
over natural language sentences. Given a question, Neural Reasoner can infer
over multiple supporting facts and find an answer to the question in specific
forms. Neural Reasoner has 1) a specific interaction-pooling mechanism,
allowing it to examine multiple facts, and 2) a deep architecture, allowing it
to model the complicated logical relations in reasoning tasks. Assuming no
particular structure exists in the question and facts, Neural Reasoner is able
to accommodate different types of reasoning and different forms of language
expressions. Despite the model complexity, Neural Reasoner can still be trained
effectively in an end-to-end manner. Our empirical studies show that Neural
Reasoner can outperform existing neural reasoning systems with remarkable
margins on two difficult artificial tasks (Positional Reasoning and Path
Finding) proposed in [8]. For example, it improves the accuracy on Path
Finding(10K) from 33.4% [6] to over 98%.",2015,19,1.7272727272727273,,http://arxiv.org/abs/1508.05508v1,Low ROT
227,Knowledge Distillation in Deep Learning - Study 125,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,19,1.7272727272727273,synthetic_125,https://synthetic.paper/125,Low ROT
103,Graph Neural Networks for Social Network Analysis - Study 1,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2016,18,1.8,synthetic_1,https://synthetic.paper/1,Low ROT
154,Advanced Neural Network Architectures for Deep Learning - Study 52,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2015,21,1.9090909090909092,synthetic_52,https://synthetic.paper/52,Low ROT
45,"A Neural Network Approach to Context-Sensitive Generation of
  Conversational Responses","We present a novel response generation system that can be trained end to end
on large quantities of unstructured Twitter conversations. A neural network
architecture is used to address sparsity issues that arise when integrating
contextual information into classic statistical models, allowing the system to
take into account previous dialog utterances. Our dynamic-context generative
models show consistent gains over both context-sensitive and
non-context-sensitive Machine Translation and Information Retrieval baselines.",2015,22,2.0,,http://arxiv.org/abs/1506.06714v1,Low ROT
391,Quantum Machine Learning Algorithms - Study 289,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2017,18,2.0,synthetic_289,https://synthetic.paper/289,Low ROT
312,Generative Adversarial Networks for Image Synthesis - Study 210,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,22,2.0,synthetic_210,https://synthetic.paper/210,Low ROT
166,Generative Adversarial Networks for Image Synthesis - Study 64,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2016,20,2.0,synthetic_64,https://synthetic.paper/64,Low ROT
333,Continual Learning in Neural Networks - Study 231,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2015,23,2.090909090909091,synthetic_231,https://synthetic.paper/231,Low ROT
29,Quantifying Uncertainties in Natural Language Processing Tasks,"Reliable uncertainty quantification is a first step towards building
explainable, transparent, and accountable artificial intelligent systems.
Recent progress in Bayesian deep learning has made such quantification
realizable. In this paper, we propose novel methods to study the benefits of
characterizing model and data uncertainties for natural language processing
(NLP) tasks. With empirical experiments on sentiment analysis, named entity
recognition, and language modeling using convolutional and recurrent neural
network models, we show that explicitly modeling uncertainties is not only
necessary to measure output confidence levels, but also useful at enhancing
model performances in various NLP tasks.",2018,17,2.125,,http://arxiv.org/abs/1811.07253v1,Low ROT
46,"The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured
  Multi-Turn Dialogue Systems","This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost
1 million multi-turn dialogues, with a total of over 7 million utterances and
100 million words. This provides a unique resource for research into building
dialogue managers based on neural language models that can make use of large
amounts of unlabeled data. The dataset has both the multi-turn property of
conversations in the Dialog State Tracking Challenge datasets, and the
unstructured nature of interactions from microblog services such as Twitter. We
also describe two neural learning architectures suitable for analyzing this
dataset, and provide benchmark performance on the task of selecting the best
next response.",2015,24,2.1818181818181817,,http://arxiv.org/abs/1506.08909v3,Low ROT
60,"How NOT To Evaluate Your Dialogue System: An Empirical Study of
  Unsupervised Evaluation Metrics for Dialogue Response Generation","We investigate evaluation metrics for dialogue response generation systems
where supervised labels, such as task completion, are not available. Recent
works in response generation have adopted metrics from machine translation to
compare a model's generated response to a single target response. We show that
these metrics correlate very weakly with human judgements in the non-technical
Twitter domain, and not at all in the technical Ubuntu domain. We provide
quantitative and qualitative results highlighting specific weaknesses in
existing metrics, and provide recommendations for future development of better
automatic evaluation metrics for dialogue systems.",2016,22,2.2,,http://arxiv.org/abs/1603.08023v2,Low ROT
303,Machine Learning Applications in Computer Vision - Study 201,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2018,18,2.25,synthetic_201,https://synthetic.paper/201,Low ROT
245,Federated Learning for Privacy-Preserving AI - Study 143,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2018,18,2.25,synthetic_143,https://synthetic.paper/143,Low ROT
191,Optimization Algorithms in Machine Learning - Study 89,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,25,2.272727272727273,synthetic_89,https://synthetic.paper/89,Low ROT
50,"What to talk about and how? Selective Generation using LSTMs with
  Coarse-to-Fine Alignment","We propose an end-to-end, domain-independent neural encoder-aligner-decoder
model for selective generation, i.e., the joint task of content selection and
surface realization. Our model first encodes a full set of over-determined
database event records via an LSTM-based recurrent neural network, then
utilizes a novel coarse-to-fine aligner to identify the small subset of salient
records to talk about, and finally employs a decoder to generate free-form
descriptions of the aligned, selected records. Our model achieves the best
selection and generation results reported to-date (with 59% relative
improvement in generation) on the benchmark WeatherGov dataset, despite using
no specialized features or linguistic resources. Using an improved k-nearest
neighbor beam filter helps further. We also perform a series of ablations and
visualizations to elucidate the contributions of our key model components.
Lastly, we evaluate the generalizability of our model on the RoboCup dataset,
and get results that are competitive with or better than the state-of-the-art,
despite being severely data-starved.",2015,25,2.272727272727273,,http://arxiv.org/abs/1509.00838v2,Low ROT
147,Adversarial Training for Robust Neural Networks - Study 45,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2015,25,2.272727272727273,synthetic_45,https://synthetic.paper/45,Low ROT
359,Adversarial Training for Robust Neural Networks - Study 257,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,26,2.3636363636363638,synthetic_257,https://synthetic.paper/257,Low ROT
250,Attention Mechanisms in Neural Networks - Study 148,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2015,26,2.3636363636363638,synthetic_148,https://synthetic.paper/148,Low ROT
123,Adversarial Training for Robust Neural Networks - Study 21,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2018,19,2.375,synthetic_21,https://synthetic.paper/21,Low ROT
119,Natural Language Processing with Transformer Models - Study 17,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2016,24,2.4,synthetic_17,https://synthetic.paper/17,Low ROT
213,Neural Architecture Search for Automated ML - Study 111,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2016,24,2.4,synthetic_111,https://synthetic.paper/111,Low ROT
336,Federated Learning for Privacy-Preserving AI - Study 234,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2015,27,2.4545454545454546,synthetic_234,https://synthetic.paper/234,Low ROT
121,Graph Neural Networks for Social Network Analysis - Study 19,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2015,27,2.4545454545454546,synthetic_19,https://synthetic.paper/19,Low ROT
63,Piecewise Latent Variables for Neural Variational Text Processing,"Advances in neural variational inference have facilitated the learning of
powerful directed graphical models with continuous latent variables, such as
variational autoencoders. The hope is that such models will learn to represent
rich, multi-modal latent factors in real-world data, such as natural language
text. However, current models often assume simplistic priors on the latent
variables - such as the uni-modal Gaussian distribution - which are incapable
of representing complex latent factors efficiently. To overcome this
restriction, we propose the simple, but highly flexible, piecewise constant
distribution. This distribution has the capacity to represent an exponential
number of modes of a latent target distribution, while remaining mathematically
tractable. Our results demonstrate that incorporating this new latent
distribution into different models yields substantial improvements in natural
language processing tasks such as document modeling and natural language
generation for dialogue.",2016,25,2.5,,http://arxiv.org/abs/1612.00377v4,Low ROT
195,Natural Language Processing with Transformer Models - Study 93,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2016,25,2.5,synthetic_93,https://synthetic.paper/93,Low ROT
57,Compositional Obverter Communication Learning From Raw Visual Input,"One of the distinguishing aspects of human language is its compositionality,
which allows us to describe complex environments with limited vocabulary.
Previously, it has been shown that neural network agents can learn to
communicate in a highly structured, possibly compositional language based on
disentangled input (e.g. hand- engineered features). Humans, however, do not
learn to communicate based on well-summarized features. In this work, we train
neural agents to simultaneously develop visual perception from raw image
pixels, and learn to communicate with a sequence of discrete symbols. The
agents play an image description game where the image contains factors such as
colors and shapes. We train the agents using the obverter technique where an
agent introspects to generate messages that maximize its own understanding.
Through qualitative analysis, visualization and a zero-shot test, we show that
the agents can develop, out of raw image pixels, a language with compositional
properties, given a proper pressure from the environment.",2018,20,2.5,,http://arxiv.org/abs/1804.02341v1,Low ROT
51,Incorporating Copying Mechanism in Sequence-to-Sequence Learning,"We address an important problem in sequence-to-sequence (Seq2Seq) learning
referred to as copying, in which certain segments in the input sequence are
selectively replicated in the output sequence. A similar phenomenon is
observable in human language communication. For example, humans tend to repeat
entity names or even long phrases in conversation. The challenge with regard to
copying in Seq2Seq is that new machinery is needed to decide when to perform
the operation. In this paper, we incorporate copying into neural network-based
Seq2Seq learning and propose a new model called CopyNet with encoder-decoder
structure. CopyNet can nicely integrate the regular way of word generation in
the decoder with the new copying mechanism which can choose sub-sequences in
the input sequence and put them at proper places in the output sequence. Our
empirical study on both synthetic data sets and real world data sets
demonstrates the efficacy of CopyNet. For example, CopyNet can outperform
regular RNN-based model with remarkable margins on text summarization tasks.",2016,26,2.6,,http://arxiv.org/abs/1603.06393v3,Low ROT
47,"Building End-To-End Dialogue Systems Using Generative Hierarchical
  Neural Network Models","We investigate the task of building open domain, conversational dialogue
systems based on large dialogue corpora using generative models. Generative
models produce system responses that are autonomously generated word-by-word,
opening up the possibility for realistic, flexible interactions. In support of
this goal, we extend the recently proposed hierarchical recurrent
encoder-decoder neural network to the dialogue domain, and demonstrate that
this model is competitive with state-of-the-art neural language models and
back-off n-gram models. We investigate the limitations of this and similar
approaches, and show how its performance can be improved by bootstrapping the
learning from a larger question-answer pair corpus and from pretrained word
embeddings.",2015,30,2.727272727272727,,http://arxiv.org/abs/1507.04808v3,Low ROT
27,"A Recurrent Neural Model with Attention for the Recognition of Chinese
  Implicit Discourse Relations","We introduce an attention-based Bi-LSTM for Chinese implicit discourse
relations and demonstrate that modeling argument pairs as a joint sequence can
outperform word order-agnostic approaches. Our model benefits from a partial
sampling scheme and is conceptually simple, yet achieves state-of-the-art
performance on the Chinese Discourse Treebank. We also visualize its attention
activity to illustrate the model's ability to selectively focus on the relevant
parts of an input sequence.",2017,25,2.7777777777777777,,http://arxiv.org/abs/1704.08092v1,Low ROT
292,Neural Network Compression Techniques - Study 190,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2017,25,2.7777777777777777,synthetic_190,https://synthetic.paper/190,Low ROT
381,Neural Network Compression Techniques - Study 279,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,31,2.8181818181818183,synthetic_279,https://synthetic.paper/279,Low ROT
196,Generative Adversarial Networks for Image Synthesis - Study 94,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2015,31,2.8181818181818183,synthetic_94,https://synthetic.paper/94,Low ROT
238,Graph Neural Networks for Social Network Analysis - Study 136,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2015,31,2.8181818181818183,synthetic_136,https://synthetic.paper/136,Low ROT
137,Neural Network Compression Techniques - Study 35,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2016,29,2.9,synthetic_35,https://synthetic.paper/35,Low ROT
193,Optimization Algorithms in Machine Learning - Study 91,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2016,30,3.0,synthetic_91,https://synthetic.paper/91,Low ROT
399,Neural Architecture Search for Automated ML - Study 297,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2019,21,3.0,synthetic_297,https://synthetic.paper/297,Low ROT
257,Quantum Machine Learning Algorithms - Study 155,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2016,30,3.0,synthetic_155,https://synthetic.paper/155,Low ROT
55,Improving speech recognition by revising gated recurrent units,"Speech recognition is largely taking advantage of deep learning, showing that
substantial benefits can be obtained by modern Recurrent Neural Networks
(RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which
typically reach state-of-the-art performance in many tasks thanks to their
ability to learn long-term dependencies and robustness to vanishing gradients.
Nevertheless, LSTMs have a rather complex design with three multiplicative
gates, that might impair their efficient implementation. An attempt to simplify
LSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just
two multiplicative gates.
  This paper builds on these efforts by further revising GRUs and proposing a
simplified architecture potentially more suitable for speech recognition. The
contribution of this work is two-fold. First, we suggest to remove the reset
gate in the GRU design, resulting in a more efficient single-gate architecture.
Second, we propose to replace tanh with ReLU activations in the state update
equations. Results show that, in our implementation, the revised architecture
reduces the per-epoch training time with more than 30% and consistently
improves recognition performance across different tasks, input features, and
noisy conditions when compared to a standard GRU.",2017,27,3.0,,http://arxiv.org/abs/1710.00641v1,Low ROT
384,Deep Learning for Medical Image Analysis - Study 282,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,33,3.0,synthetic_282,https://synthetic.paper/282,Low ROT
197,Graph Neural Networks for Social Network Analysis - Study 95,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2016,31,3.1,synthetic_95,https://synthetic.paper/95,Low ROT
140,Graph Neural Networks for Social Network Analysis - Study 38,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2018,25,3.125,synthetic_38,https://synthetic.paper/38,Low ROT
262,Attention Mechanisms in Neural Networks - Study 160,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2018,25,3.125,synthetic_160,https://synthetic.paper/160,Low ROT
396,Explainable AI for Trustworthy Machine Learning - Study 294,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2019,22,3.142857142857143,synthetic_294,https://synthetic.paper/294,Low ROT
371,Generative Adversarial Networks for Image Synthesis - Study 269,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2020,19,3.1666666666666665,synthetic_269,https://synthetic.paper/269,Low ROT
54,Bridging LSTM Architecture and the Neural Dynamics during Reading,"Recently, the long short-term memory neural network (LSTM) has attracted wide
interest due to its success in many tasks. LSTM architecture consists of a
memory cell and three gates, which looks similar to the neuronal networks in
the brain. However, there still lacks the evidence of the cognitive
plausibility of LSTM architecture as well as its working mechanism. In this
paper, we study the cognitive plausibility of LSTM by aligning its internal
architecture with the brain activity observed via fMRI when the subjects read a
story. Experiment results show that the artificial memory vector in LSTM can
accurately predict the observed sequential brain activities, indicating the
correlation between LSTM architecture and the cognitive process of story
reading.",2016,32,3.2,,http://arxiv.org/abs/1604.06635v1,Low ROT
98,Neural Associative Memory for Dual-Sequence Modeling,"Many important NLP problems can be posed as dual-sequence or
sequence-to-sequence modeling tasks. Recent advances in building end-to-end
neural architectures have been highly successful in solving such tasks. In this
work we propose a new architecture for dual-sequence modeling that is based on
associative memory. We derive AM-RNNs, a recurrent associative memory (AM)
which augments generic recurrent neural networks (RNN). This architecture is
extended to the Dual AM-RNN which operates on two AMs at once. Our models
achieve very competitive results on textual entailment. A qualitative analysis
demonstrates that long range dependencies between source and target-sequence
can be bridged effectively using Dual AM-RNNs. However, an initial experiment
on auto-encoding reveals that these benefits are not exploited by the system
when learning to solve sequence-to-sequence tasks which indicates that
additional supervision or regularization is needed.",2016,32,3.2,,http://arxiv.org/abs/1606.03864v2,Low ROT
376,Graph Neural Networks for Social Network Analysis - Study 274,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2018,26,3.25,synthetic_274,https://synthetic.paper/274,Low ROT
249,Graph Neural Networks for Social Network Analysis - Study 147,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2016,33,3.3,synthetic_147,https://synthetic.paper/147,Low ROT
339,Deep Learning for Medical Image Analysis - Study 237,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2016,33,3.3,synthetic_237,https://synthetic.paper/237,Low ROT
48,End-to-End Attention-based Large Vocabulary Speech Recognition,"Many of the current state-of-the-art Large Vocabulary Continuous Speech
Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov
Models (HMMs). Most of these systems contain separate components that deal with
the acoustic modelling, language modelling and sequence decoding. We
investigate a more direct approach in which the HMM is replaced with a
Recurrent Neural Network (RNN) that performs sequence prediction directly at
the character level. Alignment between the input features and the desired
character sequence is learned automatically by an attention mechanism built
into the RNN. For each predicted character, the attention mechanism scans the
input sequence and chooses relevant frames. We propose two methods to speed up
this operation: limiting the scan to a subset of most promising frames and
pooling over time the information contained in neighboring frames, thereby
reducing source sequence length. Integrating an n-gram language model into the
decoding process yields recognition accuracies similar to other HMM-free
RNN-based approaches.",2015,37,3.3636363636363638,,http://arxiv.org/abs/1508.04395v2,Low ROT
44,"Recurrent Neural Networks with External Memory for Language
  Understanding","Recurrent Neural Networks (RNNs) have become increasingly popular for the
task of language understanding. In this task, a semantic tagger is deployed to
associate a semantic label to each word in an input sequence. The success of
RNN may be attributed to its ability to memorize long-term dependence that
relates the current-time semantic label prediction to the observations many
time instances away. However, the memory capacity of simple RNNs is limited
because of the gradient vanishing and exploding problem. We propose to use an
external memory to improve memorization capability of RNNs. We conducted
experiments on the ATIS dataset, and observed that the proposed model was able
to achieve the state-of-the-art results. We compare our proposed model with
alternative models and report analysis results that may provide insights for
future research.",2015,37,3.3636363636363638,,http://arxiv.org/abs/1506.00195v1,Low ROT
165,Adversarial Training for Robust Neural Networks - Study 63,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2016,34,3.4,synthetic_63,https://synthetic.paper/63,Low ROT
143,Self-Supervised Learning in Computer Vision - Study 41,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2017,31,3.4444444444444446,synthetic_41,https://synthetic.paper/41,Low ROT
294,Graph Neural Networks for Social Network Analysis - Study 192,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,38,3.4545454545454546,synthetic_192,https://synthetic.paper/192,Low ROT
247,Optimization Algorithms in Machine Learning - Study 145,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2016,35,3.5,synthetic_145,https://synthetic.paper/145,Low ROT
189,Deep Learning for Medical Image Analysis - Study 87,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2015,39,3.5454545454545454,synthetic_87,https://synthetic.paper/87,Low ROT
73,A Structured Self-attentive Sentence Embedding,"This paper proposes a new model for extracting an interpretable sentence
embedding by introducing self-attention. Instead of using a vector, we use a
2-D matrix to represent the embedding, with each row of the matrix attending on
a different part of the sentence. We also propose a self-attention mechanism
and a special regularization term for the model. As a side effect, the
embedding comes with an easy way of visualizing what specific parts of the
sentence are encoded into the embedding. We evaluate our model on 3 different
tasks: author profiling, sentiment classification, and textual entailment.
Results show that our model yields a significant performance gain compared to
other sentence embedding methods in all of the 3 tasks.",2017,32,3.5555555555555554,,http://arxiv.org/abs/1703.03130v1,Low ROT
353,Graph Neural Networks for Social Network Analysis - Study 251,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2019,25,3.5714285714285716,synthetic_251,https://synthetic.paper/251,Low ROT
331,Quantum Machine Learning Algorithms - Study 229,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2019,25,3.5714285714285716,synthetic_229,https://synthetic.paper/229,Low ROT
347,Reinforcement Learning for Autonomous Systems - Study 245,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2016,36,3.6,synthetic_245,https://synthetic.paper/245,Low ROT
261,Self-Supervised Learning in Computer Vision - Study 159,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2016,36,3.6,synthetic_159,https://synthetic.paper/159,Low ROT
215,Federated Learning for Privacy-Preserving AI - Study 113,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2016,36,3.6,synthetic_113,https://synthetic.paper/113,Low ROT
58,DOLORES: Deep Contextualized Knowledge Graph Embeddings,"We introduce a new method DOLORES for learning knowledge graph embeddings
that effectively captures contextual cues and dependencies among entities and
relations. First, we note that short paths on knowledge graphs comprising of
chains of entities and relations can encode valuable information regarding
their contextual usage. We operationalize this notion by representing knowledge
graphs not as a collection of triples but as a collection of entity-relation
chains, and learn embeddings for entities and relations using deep neural
models that capture such contextual usage. In particular, our model is based on
Bi-Directional LSTMs and learn deep representations of entities and relations
from constructed entity-relation chains. We show that these representations can
very easily be incorporated into existing models to significantly advance the
state of the art on several knowledge graph prediction tasks like link
prediction, triple classification, and missing relation type prediction (in
some cases by at least 9.5%).",2018,29,3.625,,http://arxiv.org/abs/1811.00147v1,Low ROT
320,Natural Language Processing with Transformer Models - Study 218,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2020,22,3.6666666666666665,synthetic_218,https://synthetic.paper/218,Low ROT
236,Graph Neural Networks for Social Network Analysis - Study 134,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2017,33,3.6666666666666665,synthetic_134,https://synthetic.paper/134,Low ROT
4,Tutorial on Answering Questions about Images with Deep Learning,"Together with the development of more accurate methods in Computer Vision and
Natural Language Understanding, holistic architectures that answer on questions
about the content of real-world images have emerged. In this tutorial, we build
a neural-based approach to answer questions about images. We base our tutorial
on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the
models that we present here can achieve a competitive performance on both
datasets, in fact, they are among the best methods that use a combination of
LSTM with a global, full frame CNN representation of an image. We hope that
after reading this tutorial, the reader will be able to use Deep Learning
frameworks, such as Keras and introduced Kraino, to build various architectures
that will lead to a further performance improvement on this challenging task.",2016,37,3.7,,http://arxiv.org/abs/1610.01076v1,Low ROT
114,Neural Network Compression Techniques - Study 12,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2016,37,3.7,synthetic_12,https://synthetic.paper/12,Low ROT
224,Quantum Machine Learning Algorithms - Study 122,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2019,26,3.7142857142857144,synthetic_122,https://synthetic.paper/122,Low ROT
162,Machine Learning Applications in Computer Vision - Study 60,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,41,3.727272727272727,synthetic_60,https://synthetic.paper/60,Low ROT
325,Meta-Learning Approaches for Few-Shot Learning - Study 223,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2015,41,3.727272727272727,synthetic_223,https://synthetic.paper/223,Low ROT
25,Dirichlet Variational Autoencoder for Text Modeling,"We introduce an improved variational autoencoder (VAE) for text modeling with
topic information explicitly modeled as a Dirichlet latent variable. By
providing the proposed model topic awareness, it is more superior at
reconstructing input texts. Furthermore, due to the inherent interactions
between the newly introduced Dirichlet variable and the conventional
multivariate Gaussian variable, the model is less prone to KL divergence
vanishing. We derive the variational lower bound for the new model and conduct
experiments on four different data sets. The results show that the proposed
model is superior at text reconstruction across the latent space and
classifications on learned representations have higher test accuracies.",2018,30,3.75,,http://arxiv.org/abs/1811.00135v1,Low ROT
314,Quantum Machine Learning Algorithms - Study 212,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2017,34,3.7777777777777777,synthetic_212,https://synthetic.paper/212,Low ROT
136,Neural Network Compression Techniques - Study 34,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2017,34,3.7777777777777777,synthetic_34,https://synthetic.paper/34,Low ROT
18,DopeLearning: A Computational Approach to Rap Lyrics Generation,"Writing rap lyrics requires both creativity to construct a meaningful,
interesting story and lyrical skills to produce complex rhyme patterns, which
form the cornerstone of good flow. We present a rap lyrics generation method
that captures both of these aspects. First, we develop a prediction model to
identify the next line of existing lyrics from a set of candidate next lines.
This model is based on two machine-learning techniques: the RankSVM algorithm
and a deep neural network model with a novel structure. Results show that the
prediction model can identify the true next line among 299 randomly selected
lines with an accuracy of 17%, i.e., over 50 times more likely than by random.
Second, we employ the prediction model to combine lines from existing songs,
producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics
shows that in terms of quantitative rhyme density, the method outperforms the
best human rappers by 21%. The rap lyrics generator has been deployed as an
online tool called DeepBeat, and the performance of the tool has been assessed
by analyzing its usage logs. This analysis shows that machine-learned rankings
correlate with user preferences.",2015,42,3.8181818181818183,,http://arxiv.org/abs/1505.04771v2,Low ROT
152,Natural Language Processing with Transformer Models - Study 50,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2015,42,3.8181818181818183,synthetic_50,https://synthetic.paper/50,Low ROT
70,Continual Lifelong Learning in Natural Language Processing: A Survey,"Continual learning (CL) aims to enable information systems to learn from a
continuous data stream across time. However, it is difficult for existing deep
learning architectures to learn a new task without largely forgetting
previously acquired knowledge. Furthermore, CL is particularly challenging for
language learning, as natural language is ambiguous: it is discrete,
compositional, and its meaning is context-dependent. In this work, we look at
the problem of CL through the lens of various NLP tasks. Our survey discusses
major challenges in CL and current methods applied in neural network models. We
also provide a critical review of the existing CL evaluation methods and
datasets in NLP. Finally, we present our outlook on future research directions.",2020,23,3.8333333333333335,,http://arxiv.org/abs/2012.09823v1,Low ROT
263,Self-Supervised Learning in Computer Vision - Study 161,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2020,23,3.8333333333333335,synthetic_161,https://synthetic.paper/161,Low ROT
217,Neural Network Compression Techniques - Study 115,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2019,27,3.857142857142857,synthetic_115,https://synthetic.paper/115,Low ROT
100,"Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for
  Task-Completion Dialogue Policy Learning","Training task-completion dialogue agents with reinforcement learning usually
requires a large number of real user experiences. The Dyna-Q algorithm extends
Q-learning by integrating a world model, and thus can effectively boost
training efficiency using simulated experiences generated by the world model.
The effectiveness of Dyna-Q, however, depends on the quality of the world model
- or implicitly, the pre-specified ratio of real vs. simulated experiences used
for Q-learning. To this end, we extend the recently proposed Deep Dyna-Q (DDQ)
framework by integrating a switcher that automatically determines whether to
use a real or simulated experience for Q-learning. Furthermore, we explore the
use of active learning for improving sample efficiency, by encouraging the
world model to generate simulated experiences in the state-action space where
the agent has not (fully) explored. Our results show that by combining switcher
and active learning, the new framework named as Switch-based Active Deep Dyna-Q
(Switch-DDQ), leads to significant improvement over DDQ and Q-learning
baselines in both simulation and human evaluations.",2018,31,3.875,,http://arxiv.org/abs/1811.07550v1,Low ROT
167,Attention Mechanisms in Neural Networks - Study 65,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2016,39,3.9,synthetic_65,https://synthetic.paper/65,Low ROT
131,Quantum Machine Learning Algorithms - Study 29,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2015,44,4.0,synthetic_29,https://synthetic.paper/29,Low ROT
304,Knowledge Distillation in Deep Learning - Study 202,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2017,36,4.0,synthetic_202,https://synthetic.paper/202,Low ROT
351,Meta-Learning Approaches for Few-Shot Learning - Study 249,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2019,28,4.0,synthetic_249,https://synthetic.paper/249,Low ROT
74,"Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational
  Compositional Operators for Analogy Detection","Representing the semantic relations that exist between two given words (or
entities) is an important first step in a wide-range of NLP applications such
as analogical reasoning, knowledge base completion and relational information
retrieval. A simple, yet surprisingly accurate method for representing a
relation between two words is to compute the vector offset (\PairDiff) between
their corresponding word embeddings. Despite the empirical success, it remains
unclear as to whether \PairDiff is the best operator for obtaining a relational
representation from word embeddings. We conduct a theoretical analysis of
generalised bilinear operators that can be used to measure the $\ell_{2}$
relational distance between two word-pairs. We show that, if the word
embeddings are standardised and uncorrelated, such an operator will be
independent of bilinear terms, and can be simplified to a linear form, where
\PairDiff is a special case. For numerous word embedding types, we empirically
verify the uncorrelation assumption, demonstrating the general applicability of
our theoretical result. Moreover, we experimentally discover \PairDiff from the
bilinear relation composition operator on several benchmark analogy datasets.",2017,36,4.0,,http://arxiv.org/abs/1709.06673v2,Low ROT
204,Reinforcement Learning for Autonomous Systems - Study 102,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2018,33,4.125,synthetic_102,https://synthetic.paper/102,Low ROT
234,Graph Neural Networks for Social Network Analysis - Study 132,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2019,29,4.142857142857143,synthetic_132,https://synthetic.paper/132,Low ROT
30,"NUIG-Shubhanker@Dravidian-CodeMix-FIRE2020: Sentiment Analysis of
  Code-Mixed Dravidian text using XLNet","Social media has penetrated into multilingual societies, however most of them
use English to be a preferred language for communication. So it looks natural
for them to mix their cultural language with English during conversations
resulting in abundance of multilingual data, call this code-mixed data,
available in todays' world.Downstream NLP tasks using such data is challenging
due to the semantic nature of it being spread across multiple languages.One
such Natural Language Processing task is sentiment analysis, for this we use an
auto-regressive XLNet model to perform sentiment analysis on code-mixed
Tamil-English and Malayalam-English datasets.",2020,25,4.166666666666667,,http://arxiv.org/abs/2010.07773v1,Low ROT
182,Quantum Machine Learning Algorithms - Study 80,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2015,46,4.181818181818182,synthetic_80,https://synthetic.paper/80,Low ROT
67,Czech News Dataset for Semantic Textual Similarity,"This paper describes a novel dataset consisting of sentences with semantic
similarity annotations. The data originate from the journalistic domain in the
Czech language. We describe the process of collecting and annotating the data
in detail. The dataset contains 138,556 human annotations divided into train
and test sets. In total, 485 journalism students participated in the creation
process. To increase the reliability of the test set, we compute the annotation
as an average of 9 individual annotations. We evaluate the quality of the
dataset by measuring inter and intra annotation annotators' agreements. Beside
agreement numbers, we provide detailed statistics of the collected dataset. We
conclude our paper with a baseline experiment of building a system for
predicting the semantic similarity of sentences. Due to the massive number of
training annotations (116 956), the model can perform significantly better than
an average annotator (0,92 versus 0,86 of Person's correlation coefficients).",2021,21,4.2,,http://arxiv.org/abs/2108.08708v3,Low ROT
271,Attention Mechanisms in Neural Networks - Study 169,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2016,42,4.2,synthetic_169,https://synthetic.paper/169,Low ROT
21,"Adversarial Attacks on Knowledge Graph Embeddings via Instance
  Attribution Methods","Despite the widespread use of Knowledge Graph Embeddings (KGE), little is
known about the security vulnerabilities that might disrupt their intended
behaviour. We study data poisoning attacks against KGE models for link
prediction. These attacks craft adversarial additions or deletions at training
time to cause model failure at test time. To select adversarial deletions, we
propose to use the model-agnostic instance attribution methods from
Interpretable Machine Learning, which identify the training instances that are
most influential to a neural model's predictions on test instances. We use
these influential triples as adversarial deletions. We further propose a
heuristic method to replace one of the two entities in each influential triple
to generate adversarial additions. Our experiments show that the proposed
strategies outperform the state-of-art data poisoning attacks on KGE models and
improve the MRR degradation due to the attacks by up to 62% over the baselines.",2021,21,4.2,,http://arxiv.org/abs/2111.03120v1,Low ROT
368,Reinforcement Learning for Autonomous Systems - Study 266,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2016,42,4.2,synthetic_266,https://synthetic.paper/266,Low ROT
310,Quantum Machine Learning Algorithms - Study 208,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2018,34,4.25,synthetic_208,https://synthetic.paper/208,Low ROT
394,Knowledge Distillation in Deep Learning - Study 292,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2015,47,4.2727272727272725,synthetic_292,https://synthetic.paper/292,Low ROT
101,Quasi-Recurrent Neural Networks,"Recurrent neural networks are a powerful tool for modeling sequential data,
but the dependence of each timestep's computation on the previous timestep's
output limits parallelism and makes RNNs unwieldy for very long sequences. We
introduce quasi-recurrent neural networks (QRNNs), an approach to neural
sequence modeling that alternates convolutional layers, which apply in parallel
across timesteps, and a minimalist recurrent pooling function that applies in
parallel across channels. Despite lacking trainable recurrent layers, stacked
QRNNs have better predictive accuracy than stacked LSTMs of the same hidden
size. Due to their increased parallelism, they are up to 16 times faster at
train and test time. Experiments on language modeling, sentiment
classification, and character-level neural machine translation demonstrate
these advantages and underline the viability of QRNNs as a basic building block
for a variety of sequence tasks.",2016,43,4.3,,http://arxiv.org/abs/1611.01576v2,Low ROT
198,Quantum Machine Learning Algorithms - Study 96,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2018,35,4.375,synthetic_96,https://synthetic.paper/96,Low ROT
382,Self-Supervised Learning in Computer Vision - Study 280,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2021,22,4.4,synthetic_280,https://synthetic.paper/280,Low ROT
142,Federated Learning for Privacy-Preserving AI - Study 40,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2016,44,4.4,synthetic_40,https://synthetic.paper/40,Low ROT
75,A Neural Comprehensive Ranker (NCR) for Open-Domain Question Answering,"This paper proposes a novel neural machine reading model for open-domain
question answering at scale. Existing machine comprehension models typically
assume that a short piece of relevant text containing answers is already
identified and given to the models, from which the models are designed to
extract answers. This assumption, however, is not realistic for building a
large-scale open-domain question answering system which requires both deep text
understanding and identifying relevant text from corpus simultaneously.
  In this paper, we introduce Neural Comprehensive Ranker (NCR) that integrates
both passage ranking and answer extraction in one single framework. A Q&A
system based on this framework allows users to issue an open-domain question
without needing to provide a piece of text that must contain the answer.
Experiments show that the unified NCR model is able to outperform the
states-of-the-art in both retrieval of relevant text and answer extraction.",2017,40,4.444444444444445,,http://arxiv.org/abs/1709.10204v2,Low ROT
212,Self-Supervised Learning in Computer Vision - Study 110,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2015,49,4.454545454545454,synthetic_110,https://synthetic.paper/110,Low ROT
266,Deep Learning for Medical Image Analysis - Study 164,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2017,41,4.555555555555555,synthetic_164,https://synthetic.paper/164,Low ROT
255,Continual Learning in Neural Networks - Study 153,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2019,32,4.571428571428571,synthetic_153,https://synthetic.paper/153,Low ROT
6,Document Image Coding and Clustering for Script Discrimination,"The paper introduces a new method for discrimination of documents given in
different scripts. The document is mapped into a uniformly coded text of
numerical values. It is derived from the position of the letters in the text
line, based on their typographical characteristics. Each code is considered as
a gray level. Accordingly, the coded text determines a 1-D image, on which
texture analysis by run-length statistics and local binary pattern is
performed. It defines feature vectors representing the script content of the
document. A modified clustering approach employed on document feature vector
groups documents written in the same script. Experimentation performed on two
custom oriented databases of historical documents in old Cyrillic, angular and
round Glagolitic as well as Antiqua and Fraktur scripts demonstrates the
superiority of the proposed method with respect to well-known methods in the
state-of-the-art.",2016,46,4.6,,http://arxiv.org/abs/1609.06492v1,Low ROT
390,Optimization Algorithms in Machine Learning - Study 288,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2015,51,4.636363636363637,synthetic_288,https://synthetic.paper/288,Low ROT
188,Optimization Algorithms in Machine Learning - Study 86,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2020,28,4.666666666666667,synthetic_86,https://synthetic.paper/86,Low ROT
59,"Deep Reinforcement Learning with Distributional Semantic Rewards for
  Abstractive Summarization","Deep reinforcement learning (RL) has been a commonly-used strategy for the
abstractive summarization task to address both the exposure bias and
non-differentiable task issues. However, the conventional reward Rouge-L simply
looks for exact n-grams matches between candidates and annotated references,
which inevitably makes the generated sentences repetitive and incoherent. In
this paper, instead of Rouge-L, we explore the practicability of utilizing the
distributional semantics to measure the matching degrees. With distributional
semantics, sentence-level evaluation can be obtained, and semantically-correct
phrases can also be generated without being limited to the surface form of the
reference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets
show that our proposed distributional semantics reward (DSR) has distinct
superiority in capturing the lexical and compositional diversity of natural
language.",2019,33,4.714285714285714,,http://arxiv.org/abs/1909.00141v2,Low ROT
392,Quantum Machine Learning Algorithms - Study 290,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2019,33,4.714285714285714,synthetic_290,https://synthetic.paper/290,Low ROT
106,Neural Architecture Search for Automated ML - Study 4,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2018,38,4.75,synthetic_4,https://synthetic.paper/4,Low ROT
241,Adversarial Training for Robust Neural Networks - Study 139,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2017,43,4.777777777777778,synthetic_139,https://synthetic.paper/139,Low ROT
222,Knowledge Distillation in Deep Learning - Study 120,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2017,43,4.777777777777778,synthetic_120,https://synthetic.paper/120,Low ROT
274,Natural Language Processing with Transformer Models - Study 172,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2021,24,4.8,synthetic_172,https://synthetic.paper/172,Low ROT
207,Self-Supervised Learning in Computer Vision - Study 105,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2015,53,4.818181818181818,synthetic_105,https://synthetic.paper/105,Low ROT
326,Optimization Algorithms in Machine Learning - Study 224,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2019,34,4.857142857142857,synthetic_224,https://synthetic.paper/224,Low ROT
192,Graph Neural Networks for Social Network Analysis - Study 90,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2019,34,4.857142857142857,synthetic_90,https://synthetic.paper/90,Low ROT
240,Generative Adversarial Networks for Image Synthesis - Study 138,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2016,49,4.9,synthetic_138,https://synthetic.paper/138,Low ROT
20,"Generating Factoid Questions With Recurrent Neural Networks: The 30M
  Factoid Question-Answer Corpus","Over the past decade, large-scale supervised learning corpora have enabled
machine learning researchers to make substantial advances. However, to this
date, there are no large-scale question-answer corpora available. In this paper
we present the 30M Factoid Question-Answer Corpus, an enormous question answer
pair corpus produced by applying a novel neural network architecture on the
knowledge base Freebase to transduce facts into natural language questions. The
produced question answer pairs are evaluated both by human evaluators and using
automatic evaluation metrics, including well-established machine translation
and sentence similarity metrics. Across all evaluation criteria the
question-generation model outperforms the competing template-based baseline.
Furthermore, when presented to human evaluators, the generated questions appear
comparable in quality to real human-generated questions.",2016,49,4.9,,http://arxiv.org/abs/1603.06807v2,Low ROT
357,Knowledge Distillation in Deep Learning - Study 255,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2015,54,4.909090909090909,synthetic_255,https://synthetic.paper/255,Low ROT
164,Federated Learning for Privacy-Preserving AI - Study 62,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2015,54,4.909090909090909,synthetic_62,https://synthetic.paper/62,Low ROT
300,Deep Learning for Medical Image Analysis - Study 198,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2016,50,5.0,synthetic_198,https://synthetic.paper/198,Low ROT
367,Attention Mechanisms in Neural Networks - Study 265,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2017,45,5.0,synthetic_265,https://synthetic.paper/265,Low ROT
345,Continual Learning in Neural Networks - Study 243,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2016,50,5.0,synthetic_243,https://synthetic.paper/243,Low ROT
276,Reinforcement Learning for Autonomous Systems - Study 174,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2018,40,5.0,synthetic_174,https://synthetic.paper/174,Low ROT
252,Continual Learning in Neural Networks - Study 150,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2018,41,5.125,synthetic_150,https://synthetic.paper/150,Low ROT
389,Quantum Machine Learning Algorithms - Study 287,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2016,52,5.2,synthetic_287,https://synthetic.paper/287,Low ROT
386,Deep Learning for Medical Image Analysis - Study 284,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2021,26,5.2,synthetic_284,https://synthetic.paper/284,Low ROT
159,Federated Learning for Privacy-Preserving AI - Study 57,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2021,26,5.2,synthetic_57,https://synthetic.paper/57,Low ROT
318,Explainable AI for Trustworthy Machine Learning - Study 216,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2021,26,5.2,synthetic_216,https://synthetic.paper/216,Low ROT
265,Deep Learning for Medical Image Analysis - Study 163,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2017,48,5.333333333333333,synthetic_163,https://synthetic.paper/163,Low ROT
2,pix2code: Generating Code from a Graphical User Interface Screenshot,"Transforming a graphical user interface screenshot created by a designer into
computer code is a typical task conducted by a developer in order to build
customized software, websites, and mobile applications. In this paper, we show
that deep learning methods can be leveraged to train a model end-to-end to
automatically generate code from a single input image with over 77% of accuracy
for three different platforms (i.e. iOS, Android and web-based technologies).",2017,48,5.333333333333333,,http://arxiv.org/abs/1705.07962v2,Low ROT
284,Multi-Modal Learning with Neural Networks - Study 182,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2017,48,5.333333333333333,synthetic_182,https://synthetic.paper/182,Low ROT
102,"Input Switched Affine Networks: An RNN Architecture Designed for
  Interpretability","There exist many problem domains where the interpretability of neural network
models is essential for deployment. Here we introduce a recurrent architecture
composed of input-switched affine transformations - in other words an RNN
without any explicit nonlinearities, but with input-dependent recurrent
weights. This simple form allows the RNN to be analyzed via straightforward
linear methods: we can exactly characterize the linear contribution of each
input to the model predictions; we can use a change-of-basis to disentangle
input, output, and computational hidden unit subspaces; we can fully
reverse-engineer the architecture's solution to a simple task. Despite this
ease of interpretation, the input switched affine network achieves reasonable
performance on a text modeling tasks, and allows greater computational
efficiency than networks with standard nonlinearities.",2016,54,5.4,,http://arxiv.org/abs/1611.09434v2,Low ROT
145,Neural Architecture Search for Automated ML - Study 43,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2019,38,5.428571428571429,synthetic_43,https://synthetic.paper/43,Low ROT
134,Multi-Modal Learning with Neural Networks - Study 32,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2017,49,5.444444444444445,synthetic_32,https://synthetic.paper/32,Low ROT
369,Adversarial Training for Robust Neural Networks - Study 267,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2020,33,5.5,synthetic_267,https://synthetic.paper/267,Low ROT
305,Optimization Algorithms in Machine Learning - Study 203,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2016,55,5.5,synthetic_203,https://synthetic.paper/203,Low ROT
259,Graph Neural Networks for Social Network Analysis - Study 157,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2017,50,5.555555555555555,synthetic_157,https://synthetic.paper/157,Low ROT
28,A Joint Model for Question Answering and Question Generation,"We propose a generative machine comprehension model that learns jointly to
ask and answer questions based on documents. The proposed model uses a
sequence-to-sequence framework that encodes the document and generates a
question (answer) given an answer (question). Significant improvement in model
performance is observed empirically on the SQuAD corpus, confirming our
hypothesis that the model benefits from jointly learning to perform both tasks.
We believe the joint model's novelty offers a new perspective on machine
comprehension beyond architectural engineering, and serves as a first step
towards autonomous information seeking.",2017,50,5.555555555555555,,http://arxiv.org/abs/1706.01450v1,Low ROT
105,Federated Learning for Privacy-Preserving AI - Study 3,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2021,28,5.6,synthetic_3,https://synthetic.paper/3,Low ROT
370,Generative Adversarial Networks for Image Synthesis - Study 268,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2021,28,5.6,synthetic_268,https://synthetic.paper/268,Low ROT
267,Optimization Algorithms in Machine Learning - Study 165,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2021,28,5.6,synthetic_165,https://synthetic.paper/165,Low ROT
329,Natural Language Processing with Transformer Models - Study 227,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2016,56,5.6,synthetic_227,https://synthetic.paper/227,Low ROT
302,Quantum Machine Learning Algorithms - Study 200,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2018,45,5.625,synthetic_200,https://synthetic.paper/200,Low ROT
116,Self-Supervised Learning in Computer Vision - Study 14,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2019,40,5.714285714285714,synthetic_14,https://synthetic.paper/14,Low ROT
309,Generative Adversarial Networks for Image Synthesis - Study 207,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2019,40,5.714285714285714,synthetic_207,https://synthetic.paper/207,Low ROT
199,Reinforcement Learning for Autonomous Systems - Study 97,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2018,46,5.75,synthetic_97,https://synthetic.paper/97,Low ROT
23,Continual Learning of Natural Language Processing Tasks: A Survey,"Continual learning (CL) is a learning paradigm that emulates the human
capability of learning and accumulating knowledge continually without
forgetting the previously learned knowledge and also transferring the learned
knowledge to help learn new tasks better. This survey presents a comprehensive
review and analysis of the recent progress of CL in NLP, which has significant
differences from CL in computer vision and machine learning. It covers (1) all
CL settings with a taxonomy of existing techniques; (2) catastrophic forgetting
(CF) prevention, (3) knowledge transfer (KT), which is particularly important
for NLP tasks; and (4) some theory and the hidden challenge of inter-task class
separation (ICS). (1), (3) and (4) have not been included in the existing
survey. Finally, a list of future directions is discussed.",2022,23,5.75,,http://arxiv.org/abs/2211.12701v2,Low ROT
99,"MOHONE: Modeling Higher Order Network Effects in KnowledgeGraphs via
  Network Infused Embeddings","Many knowledge graph embedding methods operate on triples and are therefore
implicitly limited by a very local view of the entire knowledge graph. We
present a new framework MOHONE to effectively model higher order network
effects in knowledge-graphs, thus enabling one to capture varying degrees of
network connectivity (from the local to the global). Our framework is generic,
explicitly models the network scale, and captures two different aspects of
similarity in networks: (a) shared local neighborhood and (b) structural
role-based similarity. First, we introduce methods that learn network
representations of entities in the knowledge graph capturing these varied
aspects of similarity. We then propose a fast, efficient method to incorporate
the information captured by these network representations into existing
knowledge graph embeddings. We show that our method consistently and
significantly improves the performance on link prediction of several different
knowledge-graph embedding methods including TRANSE, TRANSD, DISTMULT, and
COMPLEX(by at least 4 points or 17% in some cases).",2018,46,5.75,,http://arxiv.org/abs/1811.00198v1,Low ROT
39,Sentence Pair Scoring: Towards Unified Framework for Text Comprehension,"We review the task of Sentence Pair Scoring, popular in the literature in
various forms - viewed as Answer Sentence Selection, Semantic Text Scoring,
Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a
component of Memory Networks.
  We argue that all such tasks are similar from the model perspective and
propose new baselines by comparing the performance of common IR metrics and
popular convolutional, recurrent and attention-based neural models across many
Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating
randomized models, propose a statistically grounded methodology, and attempt to
improve comparisons by releasing new datasets that are much harder than some of
the currently used well explored benchmarks. We introduce a unified open source
software framework with easily pluggable models and tasks, which enables us to
experiment with multi-task reusability of trained sentence model. We set a new
state-of-art in performance on the Ubuntu Dialogue dataset.",2016,58,5.8,,http://arxiv.org/abs/1603.06127v4,Low ROT
194,Machine Learning Applications in Computer Vision - Study 92,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2016,58,5.8,synthetic_92,https://synthetic.paper/92,Low ROT
251,Multi-Modal Learning with Neural Networks - Study 149,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2016,58,5.8,synthetic_149,https://synthetic.paper/149,Low ROT
301,Optimization Algorithms in Machine Learning - Study 199,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2019,41,5.857142857142857,synthetic_199,https://synthetic.paper/199,Low ROT
5,Learning Visual Question Answering by Bootstrapping Hard Attention,"Attention mechanisms in biological perception are thought to select subsets
of perceptual information for more sophisticated processing which would be
prohibitive to perform on all sensory inputs. In computer vision, however,
there has been relatively little exploration of hard attention, where some
information is selectively ignored, in spite of the success of soft attention,
where information is re-weighted and aggregated, but never filtered out. Here,
we introduce a new approach for hard attention and find it achieves very
competitive performance on a recently-released visual question answering
datasets, equalling and in some cases surpassing similar soft attention
architectures while entirely ignoring some features. Even though the hard
attention mechanism is thought to be non-differentiable, we found that the
feature magnitudes correlate with semantic relevance, and provide a useful
signal for our mechanism's attentional selection criterion. Because hard
attention selects important features of the input information, it can also be
more efficient than analogous soft attention mechanisms. This is especially
important for recent approaches that use non-local pairwise operations, whereby
computational and memory costs are quadratic in the size of the set of
features.",2018,47,5.875,,http://arxiv.org/abs/1808.00300v1,Low ROT
173,Neural Architecture Search for Automated ML - Study 71,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2017,53,5.888888888888889,synthetic_71,https://synthetic.paper/71,Low ROT
372,Machine Learning Applications in Computer Vision - Study 270,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2017,53,5.888888888888889,synthetic_270,https://synthetic.paper/270,Low ROT
155,Reinforcement Learning for Autonomous Systems - Study 53,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2019,42,6.0,synthetic_53,https://synthetic.paper/53,Low ROT
282,Quantum Machine Learning Algorithms - Study 180,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2017,54,6.0,synthetic_180,https://synthetic.paper/180,Low ROT
56,Attentive Tensor Product Learning,"This paper proposes a new architecture - Attentive Tensor Product Learning
(ATPL) - to represent grammatical structures in deep learning models. ATPL is a
new architecture to bridge this gap by exploiting Tensor Product
Representations (TPR), a structured neural-symbolic model developed in
cognitive science, aiming to integrate deep learning with explicit language
structures and rules. The key ideas of ATPL are: 1) unsupervised learning of
role-unbinding vectors of words via TPR-based deep neural network; 2) employing
attention modules to compute TPR; and 3) integration of TPR with typical deep
learning architectures including Long Short-Term Memory (LSTM) and Feedforward
Neural Network (FFNN). The novelty of our approach lies in its ability to
extract the grammatical structure of a sentence by using role-unbinding
vectors, which are obtained in an unsupervised manner. This ATPL approach is
applied to 1) image captioning, 2) part of speech (POS) tagging, and 3)
constituency parsing of a sentence. Experimental results demonstrate the
effectiveness of the proposed approach.",2018,49,6.125,,http://arxiv.org/abs/1802.07089v2,Low ROT
344,Graph Neural Networks for Social Network Analysis - Study 242,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2018,49,6.125,synthetic_242,https://synthetic.paper/242,Low ROT
104,Federated Learning for Privacy-Preserving AI - Study 2,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2018,49,6.125,synthetic_2,https://synthetic.paper/2,Low ROT
355,Neural Architecture Search for Automated ML - Study 253,We present a comprehensive study of attention mechanisms and their impact on neural network performance.,2020,37,6.166666666666667,synthetic_253,https://synthetic.paper/253,Low ROT
64,"CLASSIC: Continual and Contrastive Learning of Aspect Sentiment
  Classification Tasks","This paper studies continual learning (CL) of a sequence of aspect sentiment
classification(ASC) tasks in a particular CL setting called domain incremental
learning (DIL). Each task is from a different domain or product. The DIL
setting is particularly suited to ASC because in testing the system needs not
know the task/domain to which the test data belongs. To our knowledge, this
setting has not been studied before for ASC. This paper proposes a novel model
called CLASSIC. The key novelty is a contrastive continual learning method that
enables both knowledge transfer across tasks and knowledge distillation from
old tasks to the new task, which eliminates the need for task ids in testing.
Experimental results show the high effectiveness of CLASSIC.",2021,31,6.2,,http://arxiv.org/abs/2112.02714v1,Low ROT
248,Graph Neural Networks for Social Network Analysis - Study 146,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2021,31,6.2,synthetic_146,https://synthetic.paper/146,Low ROT
128,Optimization Algorithms in Machine Learning - Study 26,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2017,56,6.222222222222222,synthetic_26,https://synthetic.paper/26,Low ROT
323,Graph Neural Networks for Social Network Analysis - Study 221,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2022,25,6.25,synthetic_221,https://synthetic.paper/221,Low ROT
210,Neural Network Compression Techniques - Study 108,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2019,44,6.285714285714286,synthetic_108,https://synthetic.paper/108,Low ROT
307,Quantum Machine Learning Algorithms - Study 205,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2020,38,6.333333333333333,synthetic_205,https://synthetic.paper/205,Low ROT
378,Reinforcement Learning for Autonomous Systems - Study 276,Our approach utilizes graph neural networks to analyze complex social network structures and relationships.,2020,38,6.333333333333333,synthetic_276,https://synthetic.paper/276,Low ROT
17,"Towards Explainable NLP: A Generative Explanation Framework for Text
  Classification","Building explainable systems is a critical problem in the field of Natural
Language Processing (NLP), since most machine learning models provide no
explanations for the predictions. Existing approaches for explainable machine
learning systems tend to focus on interpreting the outputs or the connections
between inputs and outputs. However, the fine-grained information is often
ignored, and the systems do not explicitly generate the human-readable
explanations. To better alleviate this problem, we propose a novel generative
explanation framework that learns to make classification decisions and generate
fine-grained explanations at the same time. More specifically, we introduce the
explainable factor and the minimum risk training approach that learn to
generate more reasonable explanations. We construct two new datasets that
contain summaries, rating scores, and fine-grained reasons. We conduct
experiments on both datasets, comparing with several strong neural network
baseline systems. Experimental results show that our method surpasses all
baselines on both datasets, and is able to generate concise explanations at the
same time.",2018,51,6.375,,http://arxiv.org/abs/1811.00196v2,Low ROT
340,Deep Learning for Medical Image Analysis - Study 238,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2019,45,6.428571428571429,synthetic_238,https://synthetic.paper/238,Low ROT
108,Quantum Machine Learning Algorithms - Study 6,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2017,58,6.444444444444445,synthetic_6,https://synthetic.paper/6,Low ROT
22,Transformer Quality in Linear Time,"We revisit the design choices in Transformers, and propose methods to address
their weaknesses in handling long sequences. First, we propose a simple layer
named gated attention unit, which allows the use of a weaker single-head
attention with minimal quality loss. We then propose a linear approximation
method complementary to this new layer, which is accelerator-friendly and
highly competitive in quality. The resulting model, named FLASH, matches the
perplexity of improved Transformers over both short (512) and long (8K) context
lengths, achieving training speedups of up to 4.9$\times$ on Wiki-40B and
12.1$\times$ on PG-19 for auto-regressive language modeling, and 4.8$\times$ on
C4 for masked language modeling.",2022,26,6.5,,http://arxiv.org/abs/2202.10447v2,Low ROT
281,Neural Network Compression Techniques - Study 179,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2018,52,6.5,synthetic_179,https://synthetic.paper/179,Low ROT
283,Neural Network Compression Techniques - Study 181,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2018,54,6.75,synthetic_181,https://synthetic.paper/181,Low ROT
319,Self-Supervised Learning in Computer Vision - Study 217,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2022,27,6.75,synthetic_217,https://synthetic.paper/217,Low ROT
122,Neural Network Compression Techniques - Study 20,We propose a deep learning framework specifically designed for medical image analysis and diagnosis.,2017,61,6.777777777777778,synthetic_20,https://synthetic.paper/20,Low ROT
40,Poisoning Knowledge Graph Embeddings via Relation Inference Patterns,"We study the problem of generating data poisoning attacks against Knowledge
Graph Embedding (KGE) models for the task of link prediction in knowledge
graphs. To poison KGE models, we propose to exploit their inductive abilities
which are captured through the relationship patterns like symmetry, inversion
and composition in the knowledge graph. Specifically, to degrade the model's
prediction confidence on target facts, we propose to improve the model's
prediction confidence on a set of decoy facts. Thus, we craft adversarial
additions that can improve the model's prediction confidence on decoy facts
through different inference patterns. Our experiments demonstrate that the
proposed poisoning attacks outperform state-of-art baselines on four KGE models
for two publicly available datasets. We also find that the symmetry pattern
based attacks generalize across all model-dataset combinations which indicates
the sensitivity of KGE models to this pattern.",2021,34,6.8,,http://arxiv.org/abs/2111.06345v1,Low ROT
180,Advanced Neural Network Architectures for Deep Learning - Study 78,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2020,41,6.833333333333333,synthetic_78,https://synthetic.paper/78,Low ROT
209,Natural Language Processing with Transformer Models - Study 107,This research focuses on federated learning techniques that preserve privacy while enabling collaborative AI development.,2017,62,6.888888888888889,synthetic_107,https://synthetic.paper/107,Low ROT
42,Collaborative Storytelling with Large-scale Neural Language Models,"Storytelling plays a central role in human socializing and entertainment.
However, much of the research on automatic storytelling generation assumes that
stories will be generated by an agent without any human interaction. In this
paper, we introduce the task of collaborative storytelling, where an artificial
intelligence agent and a person collaborate to create a unique story by taking
turns adding to it. We present a collaborative storytelling system which works
with a human storyteller to create a story by generating new utterances based
on the story so far. We constructed the storytelling system by tuning a
publicly-available large scale language model on a dataset of writing prompts
and their accompanying fictional works. We identify generating sufficiently
human-like utterances to be an important technical issue and propose a
sample-and-rank approach to improve utterance quality. Quantitative evaluation
shows that our approach outperforms a baseline, and we present qualitative
evaluation of our system's capabilities.",2020,42,7.0,,http://arxiv.org/abs/2011.10208v1,Low ROT
168,Explainable AI for Trustworthy Machine Learning - Study 66,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2020,42,7.0,synthetic_66,https://synthetic.paper/66,Low ROT
19,"The Next Big Thing(s) in Unsupervised Machine Learning: Five Lessons
  from Infant Learning","After a surge in popularity of supervised Deep Learning, the desire to reduce
the dependence on curated, labelled data sets and to leverage the vast
quantities of unlabelled data available recently triggered renewed interest in
unsupervised learning algorithms. Despite a significantly improved performance
due to approaches such as the identification of disentangled latent
representations, contrastive learning, and clustering optimisations, the
performance of unsupervised machine learning still falls short of its
hypothesised potential. Machine learning has previously taken inspiration from
neuroscience and cognitive science with great success. However, this has mostly
been based on adult learners with access to labels and a vast amount of prior
knowledge. In order to push unsupervised machine learning forward, we argue
that developmental science of infant cognition might hold the key to unlocking
the next generation of unsupervised learning approaches. Conceptually, human
infant learning is the closest biological parallel to artificial unsupervised
learning, as infants too must learn useful representations from unlabelled
data. In contrast to machine learning, these new representations are learned
rapidly and from relatively few examples. Moreover, infants learn robust
representations that can be used flexibly and efficiently in a number of
different tasks and contexts. We identify five crucial factors enabling
infants' quality and speed of learning, assess the extent to which these have
already been exploited in machine learning, and propose how further adoption of
these factors can give rise to previously unseen performance levels in
unsupervised learning.",2020,42,7.0,,http://arxiv.org/abs/2009.08497v1,Low ROT
244,Optimization Algorithms in Machine Learning - Study 142,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2019,49,7.0,synthetic_142,https://synthetic.paper/142,Low ROT
218,Optimization Algorithms in Machine Learning - Study 116,Our research demonstrates the effectiveness of transformer-based models in natural language processing applications.,2018,59,7.375,synthetic_116,https://synthetic.paper/116,Low ROT
130,Continual Learning in Neural Networks - Study 28,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2021,37,7.4,synthetic_28,https://synthetic.paper/28,Low ROT
293,Generative Adversarial Networks for Image Synthesis - Study 191,This work explores reinforcement learning strategies for developing autonomous systems with improved decision-making capabilities.,2021,37,7.4,synthetic_191,https://synthetic.paper/191,Low ROT
76,Budgeted Policy Learning for Task-Oriented Dialogue Systems,"This paper presents a new approach that extends Deep Dyna-Q (DDQ) by
incorporating a Budget-Conscious Scheduling (BCS) to best utilize a fixed,
small amount of user interactions (budget) for learning task-oriented dialogue
agents. BCS consists of (1) a Poisson-based global scheduler to allocate budget
over different stages of training; (2) a controller to decide at each training
step whether the agent is trained using real or simulated experiences; (3) a
user goal sampling module to generate the experiences that are most effective
for policy learning. Experiments on a movie-ticket booking task with simulated
and real users show that our approach leads to significant improvements in
success rate over the state-of-the-art baselines given the fixed budget.",2019,52,7.428571428571429,,http://arxiv.org/abs/1906.00499v1,Low ROT
149,Advanced Neural Network Architectures for Deep Learning - Study 47,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2019,52,7.428571428571429,synthetic_47,https://synthetic.paper/47,Low ROT
233,Natural Language Processing with Transformer Models - Study 131,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2021,38,7.6,synthetic_131,https://synthetic.paper/131,Low ROT
237,Optimization Algorithms in Machine Learning - Study 135,This paper presents a novel approach to neural network architecture design that improves performance across multiple domains.,2019,54,7.714285714285714,synthetic_135,https://synthetic.paper/135,Low ROT
239,Graph Neural Networks for Social Network Analysis - Study 137,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2018,62,7.75,synthetic_137,https://synthetic.paper/137,Low ROT
321,Multi-Modal Learning with Neural Networks - Study 219,This paper investigates optimization algorithms that enhance the training efficiency of machine learning models.,2018,62,7.75,synthetic_219,https://synthetic.paper/219,Low ROT
129,Federated Learning for Privacy-Preserving AI - Study 27,We explore meta-learning strategies that enable rapid adaptation to new tasks with limited training data.,2019,55,7.857142857142857,synthetic_27,https://synthetic.paper/27,Low ROT
201,Generative Adversarial Networks for Image Synthesis - Study 99,We introduce a new methodology for applying machine learning techniques to complex computer vision tasks.,2020,48,8.0,synthetic_99,https://synthetic.paper/99,Low ROT
31,Formal Algorithms for Transformers,"This document aims to be a self-contained, mathematically precise overview of
transformer architectures and algorithms (*not* results). It covers what
transformers are, how they are trained, what they are used for, their key
architectural components, and a preview of the most prominent models. The
reader is assumed to be familiar with basic ML terminology and simpler neural
network architectures such as MLPs.",2022,32,8.0,,http://arxiv.org/abs/2207.09238v1,Low ROT
