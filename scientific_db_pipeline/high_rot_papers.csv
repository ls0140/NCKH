paper_id,title,abstract,publication_year,citation_count,rot_score,doi,source_url,rot_group
26,Feature Weight Tuning for Recursive Neural Networks,"This paper addresses how a recursive neural network model can automatically
leave out useless information and emphasize important evidence, in other words,
to perform ""weight tuning"" for higher-level representation acquisition. We
propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural
Network (BENN), which automatically control how much one specific unit
contributes to the higher-level representation. The proposed model can be
viewed as incorporating a more powerful compositional function for embedding
acquisition in recursive neural networks. Experimental results demonstrate the
significant improvement over standard neural models.",2014,886,73.83333333333333,,http://arxiv.org/abs/1412.3714v2,High ROT
4,Tutorial on Answering Questions about Images with Deep Learning,"Together with the development of more accurate methods in Computer Vision and
Natural Language Understanding, holistic architectures that answer on questions
about the content of real-world images have emerged. In this tutorial, we build
a neural-based approach to answer questions about images. We base our tutorial
on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the
models that we present here can achieve a competitive performance on both
datasets, in fact, they are among the best methods that use a combination of
LSTM with a global, full frame CNN representation of an image. We hope that
after reading this tutorial, the reader will be able to use Deep Learning
frameworks, such as Keras and introduced Kraino, to build various architectures
that will lead to a further performance improvement on this challenging task.",2016,757,75.7,,http://arxiv.org/abs/1610.01076v1,High ROT
41,Robust Document Representations using Latent Topics and Metadata,"Task specific fine-tuning of a pre-trained neural language model using a
custom softmax output layer is the de facto approach of late when dealing with
document classification problems. This technique is not adequate when labeled
examples are not available at training time and when the metadata artifacts in
a document must be exploited. We address these challenges by generating
document representations that capture both text and metadata artifacts in a
task agnostic manner. Instead of traditional auto-regressive or auto-encoding
based training, our novel self-supervised approach learns a soft-partition of
the input space when generating text embeddings. Specifically, we employ a
pre-learned topic model distribution as surrogate labels and construct a loss
function based on KL divergence. Our solution also incorporates metadata
explicitly rather than just augmenting them with text. The generated document
embeddings exhibit compositional characteristics and are directly used by
downstream classification tasks to create decision boundaries from a small
number of labeled examples, thereby eschewing complicated recognition methods.
We demonstrate through extensive evaluation that our proposed cross-model
fusion solution outperforms several competitive baselines on multiple datasets.",2020,490,81.66666666666667,,http://arxiv.org/abs/2010.12681v1,High ROT
37,"Beyond Scale: The Diversity Coefficient as a Data Quality Metric for
  Variability in Natural Language Data","Current trends in pre-training Large Language Models (LLMs) primarily focus
on the scaling of model and dataset size. While the quality of pre-training
data is considered an important factor for training powerful LLMs, it remains a
nebulous concept that has not been rigorously characterized. To this end, we
propose a formalization of one key aspect of data quality -- measuring the
variability of natural language data -- specifically via a measure we call the
diversity coefficient. Our empirical analysis shows that the proposed diversity
coefficient aligns with the intuitive properties of diversity and variability,
e.g., it increases as the number of latent concepts increases. Then, we measure
the diversity coefficient of publicly available pre-training datasets and
demonstrate that their formal diversity is high compared to theoretical lower
and upper bounds. Finally, we conduct a comprehensive set of controlled
interventional experiments with GPT-2 and LLaMAv2 that demonstrate the
diversity coefficient of pre-training data characterizes useful aspects of
downstream model evaluation performance -- totaling 44 models of various sizes
(51M to 7B parameters). We conclude that our formal notion of diversity is an
important aspect of data quality that captures variability and causally leads
to improved evaluation performance.",2023,252,84.0,,http://arxiv.org/abs/2306.13840v3,High ROT
33,EvoMerge: Neuroevolution for Large Language Models,"Extensive fine-tuning on Large Language Models does not always yield better
results. Oftentimes, models tend to get better at imitating one form of data
without gaining greater reasoning ability and may even end up losing some
intelligence. Here I introduce EvoMerge, a systematic approach to large
language model training and merging. Leveraging model merging for weight
crossover and fine-tuning for weight mutation, EvoMerge establishes an
evolutionary process aimed at pushing models beyond the limits of conventional
fine-tuning.",2024,176,88.0,,http://arxiv.org/abs/2402.00070v1,High ROT
18,DopeLearning: A Computational Approach to Rap Lyrics Generation,"Writing rap lyrics requires both creativity to construct a meaningful,
interesting story and lyrical skills to produce complex rhyme patterns, which
form the cornerstone of good flow. We present a rap lyrics generation method
that captures both of these aspects. First, we develop a prediction model to
identify the next line of existing lyrics from a set of candidate next lines.
This model is based on two machine-learning techniques: the RankSVM algorithm
and a deep neural network model with a novel structure. Results show that the
prediction model can identify the true next line among 299 randomly selected
lines with an accuracy of 17%, i.e., over 50 times more likely than by random.
Second, we employ the prediction model to combine lines from existing songs,
producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics
shows that in terms of quantitative rhyme density, the method outperforms the
best human rappers by 21%. The rap lyrics generator has been deployed as an
online tool called DeepBeat, and the performance of the tool has been assessed
by analyzing its usage logs. This analysis shows that machine-learned rankings
correlate with user preferences.",2015,1017,92.45454545454545,,http://arxiv.org/abs/1505.04771v2,High ROT
17,"Towards Explainable NLP: A Generative Explanation Framework for Text
  Classification","Building explainable systems is a critical problem in the field of Natural
Language Processing (NLP), since most machine learning models provide no
explanations for the predictions. Existing approaches for explainable machine
learning systems tend to focus on interpreting the outputs or the connections
between inputs and outputs. However, the fine-grained information is often
ignored, and the systems do not explicitly generate the human-readable
explanations. To better alleviate this problem, we propose a novel generative
explanation framework that learns to make classification decisions and generate
fine-grained explanations at the same time. More specifically, we introduce the
explainable factor and the minimum risk training approach that learn to
generate more reasonable explanations. We construct two new datasets that
contain summaries, rating scores, and fine-grained reasons. We conduct
experiments on both datasets, comparing with several strong neural network
baseline systems. Experimental results show that our method surpasses all
baselines on both datasets, and is able to generate concise explanations at the
same time.",2018,755,94.375,,http://arxiv.org/abs/1811.00196v2,High ROT
29,Quantifying Uncertainties in Natural Language Processing Tasks,"Reliable uncertainty quantification is a first step towards building
explainable, transparent, and accountable artificial intelligent systems.
Recent progress in Bayesian deep learning has made such quantification
realizable. In this paper, we propose novel methods to study the benefits of
characterizing model and data uncertainties for natural language processing
(NLP) tasks. With empirical experiments on sentiment analysis, named entity
recognition, and language modeling using convolutional and recurrent neural
network models, we show that explicitly modeling uncertainties is not only
necessary to measure output confidence levels, but also useful at enhancing
model performances in various NLP tasks.",2018,755,94.375,,http://arxiv.org/abs/1811.07253v1,High ROT
39,Sentence Pair Scoring: Towards Unified Framework for Text Comprehension,"We review the task of Sentence Pair Scoring, popular in the literature in
various forms - viewed as Answer Sentence Selection, Semantic Text Scoring,
Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a
component of Memory Networks.
  We argue that all such tasks are similar from the model perspective and
propose new baselines by comparing the performance of common IR metrics and
popular convolutional, recurrent and attention-based neural models across many
Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating
randomized models, propose a statistically grounded methodology, and attempt to
improve comparisons by releasing new datasets that are much harder than some of
the currently used well explored benchmarks. We introduce a unified open source
software framework with easily pluggable models and tasks, which enables us to
experiment with multi-task reusability of trained sentence model. We set a new
state-of-art in performance on the Ubuntu Dialogue dataset.",2016,1004,100.4,,http://arxiv.org/abs/1603.06127v4,High ROT
44,"Recurrent Neural Networks with External Memory for Language
  Understanding","Recurrent Neural Networks (RNNs) have become increasingly popular for the
task of language understanding. In this task, a semantic tagger is deployed to
associate a semantic label to each word in an input sequence. The success of
RNN may be attributed to its ability to memorize long-term dependence that
relates the current-time semantic label prediction to the observations many
time instances away. However, the memory capacity of simple RNNs is limited
because of the gradient vanishing and exploding problem. We propose to use an
external memory to improve memorization capability of RNNs. We conducted
experiments on the ATIS dataset, and observed that the proposed model was able
to achieve the state-of-the-art results. We compare our proposed model with
alternative models and report analysis results that may provide insights for
future research.",2015,1157,105.18181818181819,,http://arxiv.org/abs/1506.00195v1,High ROT
22,Transformer Quality in Linear Time,"We revisit the design choices in Transformers, and propose methods to address
their weaknesses in handling long sequences. First, we propose a simple layer
named gated attention unit, which allows the use of a weaker single-head
attention with minimal quality loss. We then propose a linear approximation
method complementary to this new layer, which is accelerator-friendly and
highly competitive in quality. The resulting model, named FLASH, matches the
perplexity of improved Transformers over both short (512) and long (8K) context
lengths, achieving training speedups of up to 4.9$\times$ on Wiki-40B and
12.1$\times$ on PG-19 for auto-regressive language modeling, and 4.8$\times$ on
C4 for masked language modeling.",2022,425,106.25,,http://arxiv.org/abs/2202.10447v2,High ROT
2,pix2code: Generating Code from a Graphical User Interface Screenshot,"Transforming a graphical user interface screenshot created by a designer into
computer code is a typical task conducted by a developer in order to build
customized software, websites, and mobile applications. In this paper, we show
that deep learning methods can be leveraged to train a model end-to-end to
automatically generate code from a single input image with over 77% of accuracy
for three different platforms (i.e. iOS, Android and web-based technologies).",2017,975,108.33333333333333,,http://arxiv.org/abs/1705.07962v2,High ROT
48,End-to-End Attention-based Large Vocabulary Speech Recognition,"Many of the current state-of-the-art Large Vocabulary Continuous Speech
Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov
Models (HMMs). Most of these systems contain separate components that deal with
the acoustic modelling, language modelling and sequence decoding. We
investigate a more direct approach in which the HMM is replaced with a
Recurrent Neural Network (RNN) that performs sequence prediction directly at
the character level. Alignment between the input features and the desired
character sequence is learned automatically by an attention mechanism built
into the RNN. For each predicted character, the attention mechanism scans the
input sequence and chooses relevant frames. We propose two methods to speed up
this operation: limiting the scan to a subset of most promising frames and
pooling over time the information contained in neighboring frames, thereby
reducing source sequence length. Integrating an n-gram language model into the
decoding process yields recognition accuracies similar to other HMM-free
RNN-based approaches.",2015,1280,116.36363636363636,,http://arxiv.org/abs/1508.04395v2,High ROT
51,Incorporating Copying Mechanism in Sequence-to-Sequence Learning,"We address an important problem in sequence-to-sequence (Seq2Seq) learning
referred to as copying, in which certain segments in the input sequence are
selectively replicated in the output sequence. A similar phenomenon is
observable in human language communication. For example, humans tend to repeat
entity names or even long phrases in conversation. The challenge with regard to
copying in Seq2Seq is that new machinery is needed to decide when to perform
the operation. In this paper, we incorporate copying into neural network-based
Seq2Seq learning and propose a new model called CopyNet with encoder-decoder
structure. CopyNet can nicely integrate the regular way of word generation in
the decoder with the new copying mechanism which can choose sub-sequences in
the input sequence and put them at proper places in the output sequence. Our
empirical study on both synthetic data sets and real world data sets
demonstrates the efficacy of CopyNet. For example, CopyNet can outperform
regular RNN-based model with remarkable margins on text summarization tasks.",2016,1257,125.7,,http://arxiv.org/abs/1603.06393v3,High ROT
49,Towards Neural Network-based Reasoning,"We propose Neural Reasoner, a framework for neural network-based reasoning
over natural language sentences. Given a question, Neural Reasoner can infer
over multiple supporting facts and find an answer to the question in specific
forms. Neural Reasoner has 1) a specific interaction-pooling mechanism,
allowing it to examine multiple facts, and 2) a deep architecture, allowing it
to model the complicated logical relations in reasoning tasks. Assuming no
particular structure exists in the question and facts, Neural Reasoner is able
to accommodate different types of reasoning and different forms of language
expressions. Despite the model complexity, Neural Reasoner can still be trained
effectively in an end-to-end manner. Our empirical studies show that Neural
Reasoner can outperform existing neural reasoning systems with remarkable
margins on two difficult artificial tasks (Positional Reasoning and Path
Finding) proposed in [8]. For example, it improves the accuracy on Path
Finding(10K) from 33.4% [6] to over 98%.",2015,1420,129.0909090909091,,http://arxiv.org/abs/1508.05508v1,High ROT
50,"What to talk about and how? Selective Generation using LSTMs with
  Coarse-to-Fine Alignment","We propose an end-to-end, domain-independent neural encoder-aligner-decoder
model for selective generation, i.e., the joint task of content selection and
surface realization. Our model first encodes a full set of over-determined
database event records via an LSTM-based recurrent neural network, then
utilizes a novel coarse-to-fine aligner to identify the small subset of salient
records to talk about, and finally employs a decoder to generate free-form
descriptions of the aligned, selected records. Our model achieves the best
selection and generation results reported to-date (with 59% relative
improvement in generation) on the benchmark WeatherGov dataset, despite using
no specialized features or linguistic resources. Using an improved k-nearest
neighbor beam filter helps further. We also perform a series of ablations and
visualizations to elucidate the contributions of our key model components.
Lastly, we evaluate the generalizability of our model on the RoboCup dataset,
and get results that are competitive with or better than the state-of-the-art,
despite being severely data-starved.",2015,1450,131.8181818181818,,http://arxiv.org/abs/1509.00838v2,High ROT
45,"A Neural Network Approach to Context-Sensitive Generation of
  Conversational Responses","We present a novel response generation system that can be trained end to end
on large quantities of unstructured Twitter conversations. A neural network
architecture is used to address sparsity issues that arise when integrating
contextual information into classic statistical models, allowing the system to
take into account previous dialog utterances. Our dynamic-context generative
models show consistent gains over both context-sensitive and
non-context-sensitive Machine Translation and Information Retrieval baselines.",2015,1456,132.36363636363637,,http://arxiv.org/abs/1506.06714v1,High ROT
25,Dirichlet Variational Autoencoder for Text Modeling,"We introduce an improved variational autoencoder (VAE) for text modeling with
topic information explicitly modeled as a Dirichlet latent variable. By
providing the proposed model topic awareness, it is more superior at
reconstructing input texts. Furthermore, due to the inherent interactions
between the newly introduced Dirichlet variable and the conventional
multivariate Gaussian variable, the model is less prone to KL divergence
vanishing. We derive the variational lower bound for the new model and conduct
experiments on four different data sets. The results show that the proposed
model is superior at text reconstruction across the latent space and
classifications on learned representations have higher test accuracies.",2018,1089,136.125,,http://arxiv.org/abs/1811.00135v1,High ROT
6,Document Image Coding and Clustering for Script Discrimination,"The paper introduces a new method for discrimination of documents given in
different scripts. The document is mapped into a uniformly coded text of
numerical values. It is derived from the position of the letters in the text
line, based on their typographical characteristics. Each code is considered as
a gray level. Accordingly, the coded text determines a 1-D image, on which
texture analysis by run-length statistics and local binary pattern is
performed. It defines feature vectors representing the script content of the
document. A modified clustering approach employed on document feature vector
groups documents written in the same script. Experimentation performed on two
custom oriented databases of historical documents in old Cyrillic, angular and
round Glagolitic as well as Antiqua and Fraktur scripts demonstrates the
superiority of the proposed method with respect to well-known methods in the
state-of-the-art.",2016,1530,153.0,,http://arxiv.org/abs/1609.06492v1,High ROT
5,Learning Visual Question Answering by Bootstrapping Hard Attention,"Attention mechanisms in biological perception are thought to select subsets
of perceptual information for more sophisticated processing which would be
prohibitive to perform on all sensory inputs. In computer vision, however,
there has been relatively little exploration of hard attention, where some
information is selectively ignored, in spite of the success of soft attention,
where information is re-weighted and aggregated, but never filtered out. Here,
we introduce a new approach for hard attention and find it achieves very
competitive performance on a recently-released visual question answering
datasets, equalling and in some cases surpassing similar soft attention
architectures while entirely ignoring some features. Even though the hard
attention mechanism is thought to be non-differentiable, we found that the
feature magnitudes correlate with semantic relevance, and provide a useful
signal for our mechanism's attentional selection criterion. Because hard
attention selects important features of the input information, it can also be
more efficient than analogous soft attention mechanisms. This is especially
important for recent approaches that use non-local pairwise operations, whereby
computational and memory costs are quadratic in the size of the set of
features.",2018,1889,236.125,,http://arxiv.org/abs/1808.00300v1,High ROT
19,"The Next Big Thing(s) in Unsupervised Machine Learning: Five Lessons
  from Infant Learning","After a surge in popularity of supervised Deep Learning, the desire to reduce
the dependence on curated, labelled data sets and to leverage the vast
quantities of unlabelled data available recently triggered renewed interest in
unsupervised learning algorithms. Despite a significantly improved performance
due to approaches such as the identification of disentangled latent
representations, contrastive learning, and clustering optimisations, the
performance of unsupervised machine learning still falls short of its
hypothesised potential. Machine learning has previously taken inspiration from
neuroscience and cognitive science with great success. However, this has mostly
been based on adult learners with access to labels and a vast amount of prior
knowledge. In order to push unsupervised machine learning forward, we argue
that developmental science of infant cognition might hold the key to unlocking
the next generation of unsupervised learning approaches. Conceptually, human
infant learning is the closest biological parallel to artificial unsupervised
learning, as infants too must learn useful representations from unlabelled
data. In contrast to machine learning, these new representations are learned
rapidly and from relatively few examples. Moreover, infants learn robust
representations that can be used flexibly and efficiently in a number of
different tasks and contexts. We identify five crucial factors enabling
infants' quality and speed of learning, assess the extent to which these have
already been exploited in machine learning, and propose how further adoption of
these factors can give rise to previously unseen performance levels in
unsupervised learning.",2020,1672,278.6666666666667,,http://arxiv.org/abs/2009.08497v1,High ROT
42,Collaborative Storytelling with Large-scale Neural Language Models,"Storytelling plays a central role in human socializing and entertainment.
However, much of the research on automatic storytelling generation assumes that
stories will be generated by an agent without any human interaction. In this
paper, we introduce the task of collaborative storytelling, where an artificial
intelligence agent and a person collaborate to create a unique story by taking
turns adding to it. We present a collaborative storytelling system which works
with a human storyteller to create a story by generating new utterances based
on the story so far. We constructed the storytelling system by tuning a
publicly-available large scale language model on a dataset of writing prompts
and their accompanying fictional works. We identify generating sufficiently
human-like utterances to be an important technical issue and propose a
sample-and-rank approach to improve utterance quality. Quantitative evaluation
shows that our approach outperforms a baseline, and we present qualitative
evaluation of our system's capabilities.",2020,1688,281.3333333333333,,http://arxiv.org/abs/2011.10208v1,High ROT
30,"NUIG-Shubhanker@Dravidian-CodeMix-FIRE2020: Sentiment Analysis of
  Code-Mixed Dravidian text using XLNet","Social media has penetrated into multilingual societies, however most of them
use English to be a preferred language for communication. So it looks natural
for them to mix their cultural language with English during conversations
resulting in abundance of multilingual data, call this code-mixed data,
available in todays' world.Downstream NLP tasks using such data is challenging
due to the semantic nature of it being spread across multiple languages.One
such Natural Language Processing task is sentiment analysis, for this we use an
auto-regressive XLNet model to perform sentiment analysis on code-mixed
Tamil-English and Malayalam-English datasets.",2020,1799,299.8333333333333,,http://arxiv.org/abs/2010.07773v1,High ROT
7,"hyper-sinh: An Accurate and Reliable Function from Shallow to Deep
  Learning in TensorFlow and Keras","This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation
function suitable for Deep Learning (DL)-based algorithms for supervised
learning, such as Convolutional Neural Networks (CNN). hyper-sinh, developed in
the open source Python libraries TensorFlow and Keras, is thus described and
validated as an accurate and reliable activation function for both shallow and
deep neural networks. Improvements in accuracy and reliability in image and
text classification tasks on five (N = 5) benchmark data sets available from
Keras are discussed. Experimental results demonstrate the overall competitive
classification performance of both shallow and deep neural networks, obtained
via this novel function. This function is evaluated with respect to gold
standard activation functions, demonstrating its overall competitive accuracy
and reliability for both image and text classification.",2020,1908,318.0,,http://arxiv.org/abs/2011.07661v1,High ROT
20,"Generating Factoid Questions With Recurrent Neural Networks: The 30M
  Factoid Question-Answer Corpus","Over the past decade, large-scale supervised learning corpora have enabled
machine learning researchers to make substantial advances. However, to this
date, there are no large-scale question-answer corpora available. In this paper
we present the 30M Factoid Question-Answer Corpus, an enormous question answer
pair corpus produced by applying a novel neural network architecture on the
knowledge base Freebase to transduce facts into natural language questions. The
produced question answer pairs are evaluated both by human evaluators and using
automatic evaluation metrics, including well-established machine translation
and sentence similarity metrics. Across all evaluation criteria the
question-generation model outperforms the competing template-based baseline.
Furthermore, when presented to human evaluators, the generated questions appear
comparable in quality to real human-generated questions.",2016,4804,480.4,,http://arxiv.org/abs/1603.06807v2,High ROT
